#!/usr/bin/env python3
"""
æ‰¹æ¬¡éŸ³é »åˆ†é›¢è™•ç†å·¥å…·
================

æ‰¹æ¬¡è™•ç†æŒ‡å®šè³‡æ–™å¤¾å…§æ‰€æœ‰ .wav æª”æ¡ˆçš„èªè€…åˆ†é›¢åŠŸèƒ½ï¼Œæ”¯æ´éæ­¸æœç´¢å­è³‡æ–™å¤¾ã€‚
å°‡åˆ†é›¢å¾Œçš„éŸ³æª”è¼¸å‡ºåˆ°æŒ‡å®šè³‡æ–™å¤¾ï¼Œä¿æŒåŸæœ‰çš„ç›®éŒ„çµæ§‹ã€‚

ä½¿ç”¨æ–¹æ³•ï¼š
1. ä¿®æ”¹ä¸‹æ–¹çš„è¨­å®šè®Šæ•¸
2. åŸ·è¡Œ: python batch_audio_separation.py

åŠŸèƒ½ç‰¹è‰²ï¼š
- éæ­¸è™•ç†æ•´å€‹è³‡æ–™å¤¾åŠå…¶å­è³‡æ–™å¤¾çš„ .wav æª”æ¡ˆ
- ä¿æŒåŸæœ‰ç›®éŒ„çµæ§‹è¼¸å‡ºåˆ†é›¢å¾Œçš„éŸ³æª”
- æ”¯æ´å¤šç¨®åˆ†é›¢æ¨¡å‹ï¼ˆConvTasNetã€SepFormerç­‰ï¼‰
- ğŸ”¥ æ–°å¢å¼·åˆ¶é›™èªè€…æ¨¡å¼ï¼šç¹éèšé¡åˆ†æï¼Œå›ºå®šç”¢ç”Ÿå…©å€‹éŸ³æª”
- è‡ªå‹•æª¢æ¸¬èªè€…æ•¸é‡ï¼ˆå¯é¸æ“‡é—œé–‰ï¼‰
- ç”Ÿæˆè©³ç´°çš„è™•ç†å ±å‘Šèˆ‡çµ±è¨ˆè³‡è¨Š
- æ”¯æ´ä¸­æ–·æ¢å¾©ï¼ˆå¯é¸ï¼‰

æ–°åŠŸèƒ½èªªæ˜ï¼š
- FORCE_TWO_SPEAKERS = True: å¼·åˆ¶ç”¢ç”Ÿå…©å€‹éŸ³æª”ï¼Œä¸ä¾è³´èªªè©±è€…æª¢æ¸¬
- FORCE_TWO_SPEAKERS = False: ä½¿ç”¨åŸå§‹é‚è¼¯ï¼Œä¾è³´èšé¡åˆ†æåˆ¤æ–·èªªè©±è€…æ•¸é‡
"""

import os
import json
import time
import traceback
import soundfile as sf
import numpy as np
import torch
import sys
from pathlib import Path
from typing import List, Dict, Tuple, Optional
from datetime import datetime

# æ·»åŠ å°ˆæ¡ˆæ ¹ç›®éŒ„åˆ° Python è·¯å¾‘
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from modules.separation.separator import AudioSeparator

# ==================== è¨­å®šå€åŸŸ ====================
# ğŸ‘‡ è«‹ä¿®æ”¹é€™è£¡çš„è³‡æ–™å¤¾è·¯å¾‘
INPUT_DIRECTORY = "data/clean"       # è¼¸å…¥éŸ³æª”è³‡æ–™å¤¾è·¯å¾‘
OUTPUT_DIRECTORY = "data_separated" # è¼¸å‡ºåˆ†é›¢éŸ³æª”è³‡æ–™å¤¾è·¯å¾‘

# åˆ†é›¢è¨­å®š
ENABLE_NOISE_REDUCTION = True       # æ˜¯å¦å•Ÿç”¨é™å™ª
SNR_THRESHOLD = 5.0                 # ä¿¡å™ªæ¯”é–¾å€¼
FORCE_TWO_SPEAKERS = True           # ğŸ”¥ æ˜¯å¦å¼·åˆ¶ç”¢ç”Ÿå…©å€‹éŸ³æª”ï¼ˆç¹éèªªè©±è€…æª¢æ¸¬ï¼‰

# è™•ç†è¨­å®š
ENABLE_VERBOSE = True               # æ˜¯å¦é¡¯ç¤ºè©³ç´°è¼¸å‡º
SAVE_DETAILED_RESULTS = True        # æ˜¯å¦ä¿å­˜è©³ç´°çµæœåˆ° JSON æª”æ¡ˆ
SAVE_SUMMARY_RESULTS = True         # æ˜¯å¦ä¿å­˜æ‘˜è¦çµæœ
ENABLE_PROGRESS_SAVE = True         # æ˜¯å¦å•Ÿç”¨é€²åº¦ä¿å­˜ï¼ˆå¯ä¸­æ–·æ¢å¾©ï¼‰
PROGRESS_SAVE_INTERVAL = 5          # æ¯è™•ç†å¹¾å€‹æª”æ¡ˆä¿å­˜ä¸€æ¬¡é€²åº¦

# æª”æ¡ˆç¯©é¸è¨­å®š
SUPPORTED_EXTENSIONS = ['.wav', '.WAV']  # æ”¯æ´çš„éŸ³æª”å‰¯æª”å
SKIP_HIDDEN_FILES = True            # æ˜¯å¦è·³ééš±è—æª”æ¡ˆï¼ˆä»¥ . é–‹é ­ï¼‰
MIN_FILE_SIZE_KB = 1                # æœ€å°æª”æ¡ˆå¤§å°ï¼ˆKBï¼‰ï¼Œéå°çš„æª”æ¡ˆæœƒè·³é

# åˆ†é›¢è¼¸å‡ºè¨­å®š
OUTPUT_FORMAT = "wav"               # è¼¸å‡ºæ ¼å¼
OUTPUT_SAMPLE_RATE = 16000          # è¼¸å‡ºå–æ¨£ç‡
SPEAKER_PREFIX = "speaker"          # åˆ†é›¢å¾ŒéŸ³æª”çš„å‰ç¶´åç¨±
# ==================================================

def find_all_audio_files(directory: str) -> List[str]:
    """
    éæ­¸æœç´¢æŒ‡å®šè³‡æ–™å¤¾å…§çš„æ‰€æœ‰éŸ³æª”
    
    Args:
        directory: ç›®æ¨™è³‡æ–™å¤¾è·¯å¾‘
        
    Returns:
        List[str]: æŒ‰è·¯å¾‘æ’åºçš„éŸ³æª”è·¯å¾‘åˆ—è¡¨
    """
    audio_files = []
    directory_path = Path(directory)
    
    if not directory_path.exists():
        print(f"âŒ éŒ¯èª¤ï¼šè³‡æ–™å¤¾ä¸å­˜åœ¨ - {directory}")
        return audio_files
    
    print(f"ğŸ” æœç´¢è³‡æ–™å¤¾: {directory}")
    
    # éæ­¸æœç´¢æ‰€æœ‰éŸ³æª”
    for root, dirs, files in os.walk(directory):
        # æ’åºè³‡æ–™å¤¾å’Œæª”æ¡ˆä»¥ç¢ºä¿ä¸€è‡´çš„è™•ç†é †åº
        dirs.sort()
        files.sort()
        
        for file in files:
            file_path = Path(root) / file
            
            # è·³ééš±è—æª”æ¡ˆï¼ˆå¦‚æœå•Ÿç”¨ï¼‰
            if SKIP_HIDDEN_FILES and file.startswith('.'):
                continue
            
            # æª¢æŸ¥å‰¯æª”å
            if file_path.suffix in SUPPORTED_EXTENSIONS:
                # æª¢æŸ¥æª”æ¡ˆå¤§å°
                file_size_kb = file_path.stat().st_size / 1024
                if file_size_kb >= MIN_FILE_SIZE_KB:
                    audio_files.append(str(file_path))
                else:
                    print(f"âš ï¸  è·³ééå°æª”æ¡ˆ ({file_size_kb:.1f}KB): {file}")
    
    # æŒ‰è·¯å¾‘æ’åº
    audio_files.sort()
    
    print(f"ğŸ“ æ‰¾åˆ° {len(audio_files)} å€‹æœ‰æ•ˆéŸ³æª”")
    return audio_files

def get_output_path(input_path: str, input_directory: str, output_directory: str) -> Path:
    """
    æ ¹æ“šè¼¸å…¥è·¯å¾‘ç”Ÿæˆå°æ‡‰çš„è¼¸å‡ºè·¯å¾‘ï¼Œä¿æŒç›®éŒ„çµæ§‹
    
    Args:
        input_path: è¼¸å…¥éŸ³æª”çš„å®Œæ•´è·¯å¾‘
        input_directory: è¼¸å…¥æ ¹ç›®éŒ„
        output_directory: è¼¸å‡ºæ ¹ç›®éŒ„
        
    Returns:
        Path: è¼¸å‡ºè³‡æ–™å¤¾è·¯å¾‘
    """
    input_path = Path(input_path)
    input_directory = Path(input_directory)
    output_directory = Path(output_directory)
    
    # ç²å–ç›¸å°æ–¼è¼¸å…¥æ ¹ç›®éŒ„çš„è·¯å¾‘
    try:
        relative_path = input_path.relative_to(input_directory)
    except ValueError:
        # å¦‚æœæª”æ¡ˆä¸åœ¨è¼¸å…¥æ ¹ç›®éŒ„å…§ï¼Œä½¿ç”¨æª”æ¡ˆå
        relative_path = input_path.name
    
    # æ§‹å»ºè¼¸å‡ºè·¯å¾‘ï¼ˆå»æ‰å‰¯æª”åï¼Œå› ç‚ºæœƒæœ‰å¤šå€‹åˆ†é›¢æª”æ¡ˆï¼‰
    output_base = output_directory / relative_path.parent / relative_path.stem
    
    return output_base

def separate_single_audio_file_force_two_speakers(separator: AudioSeparator, 
                                                input_path: str, 
                                                output_base: Path) -> Dict:
    """
    åˆ†é›¢å–®å€‹éŸ³æª” - å¼·åˆ¶ç”¢ç”Ÿå…©å€‹éŸ³æª”ç‰ˆæœ¬
    ç¹éèªªè©±è€…æ•¸é‡æª¢æ¸¬ï¼Œç›´æ¥åŸ·è¡Œåˆ†é›¢ä¸¦å¼·åˆ¶è¼¸å‡ºå…©å€‹æª”æ¡ˆ
    
    Args:
        separator: éŸ³é »åˆ†é›¢å™¨å¯¦ä¾‹
        input_path: è¼¸å…¥éŸ³æª”è·¯å¾‘
        output_base: è¼¸å‡ºåŸºç¤è·¯å¾‘ï¼ˆä¸å«å‰¯æª”åï¼‰
        
    Returns:
        Dict: è™•ç†çµæœå­—å…¸
    """
    start_time = time.time()
    
    print(f"\n{'='*80}")
    print(f"ğŸ¯ åˆ†é›¢éŸ³æª” (å¼·åˆ¶é›™èªè€…æ¨¡å¼): {Path(input_path).name}")
    print(f"ğŸ“‚ è¼¸å…¥è·¯å¾‘: {input_path}")
    print(f"ğŸ“¤ è¼¸å‡ºåŸºç¤è·¯å¾‘: {output_base}")
    print(f"{'='*80}")
    
    result_dict = {
        "input_file": input_path,
        "output_base": str(output_base),
        "processing_time": 0,
        "timestamp": datetime.now().isoformat(),
        "status": "processing",
        "force_two_speakers": True
    }
    
    try:
        # æª¢æŸ¥è¼¸å…¥æª”æ¡ˆ
        if not os.path.exists(input_path):
            raise FileNotFoundError(f"è¼¸å…¥éŸ³æª”ä¸å­˜åœ¨: {input_path}")
        
        # è®€å–éŸ³æª”
        print("ğŸ”„ è®€å–éŸ³æª”...")
        audio_data, sample_rate = sf.read(input_path)
        
        # éŸ³æª”ä¿¡æ¯
        duration = len(audio_data) / sample_rate
        print(f"ğŸ“Š éŸ³æª”ä¿¡æ¯:")
        print(f"   - å–æ¨£ç‡: {sample_rate} Hz")
        print(f"   - éŸ³æª”é•·åº¦: {duration:.2f} ç§’")
        print(f"   - è²é“æ•¸: {audio_data.shape[1] if len(audio_data.shape) > 1 else 1}")
        
        # è½‰æ›ç‚ºé©ç•¶æ ¼å¼
        if len(audio_data.shape) > 1:
            audio_data = audio_data.mean(axis=1)  # è½‰ç‚ºå–®è²é“
        
        # è½‰æ›ç‚º tensor
        audio_tensor = torch.from_numpy(audio_data).float().unsqueeze(0)  # æ·»åŠ  batch ç¶­åº¦
        
        # é‡æ¡æ¨£è‡³ç›®æ¨™å–æ¨£ç‡ï¼ˆå¦‚æœéœ€è¦ï¼‰
        if sample_rate != OUTPUT_SAMPLE_RATE:
            print(f"ğŸ”„ é‡æ¡æ¨£è‡³ {OUTPUT_SAMPLE_RATE} Hz...")
            import torchaudio
            resampler = torchaudio.transforms.Resample(sample_rate, OUTPUT_SAMPLE_RATE)
            audio_tensor = resampler(audio_tensor)
        
        # ç¢ºä¿è¼¸å‡ºç›®éŒ„å­˜åœ¨
        output_base.parent.mkdir(parents=True, exist_ok=True)
        
        # ğŸ”¥ ç›´æ¥åŸ·è¡Œåˆ†é›¢ï¼Œç¹éèªªè©±è€…æ•¸é‡æª¢æ¸¬
        print("ğŸ”„ åŸ·è¡ŒéŸ³é »åˆ†é›¢ (å¼·åˆ¶é›™èªè€…æ¨¡å¼)...")
        separated_files = force_two_speaker_separation(separator, audio_tensor, output_base, duration)
        
        # æ›´æ–°çµæœ
        result_dict.update({
            "status": "success",
            "separated_files": separated_files,
            "num_speakers_detected": len(separated_files),
            "num_speakers_forced": 2,
            "original_duration": duration,
            "original_sample_rate": sample_rate,
            "output_sample_rate": OUTPUT_SAMPLE_RATE
        })
        
        print(f"âœ… å¼·åˆ¶åˆ†é›¢å®Œæˆï¼Œç”¢ç”Ÿ {len(separated_files)} å€‹æª”æ¡ˆ")
        
    except Exception as e:
        error_msg = f"åˆ†é›¢å¤±æ•—: {str(e)}"
        print(f"âŒ {error_msg}")
        if ENABLE_VERBOSE:
            print("ğŸ” éŒ¯èª¤è©³æƒ…:")
            traceback.print_exc()
        
        result_dict.update({
            "status": "error",
            "error": error_msg,
            "separated_files": [],
            "num_speakers_detected": 0,
            "num_speakers_forced": 0
        })
    
    # è¨ˆç®—è™•ç†æ™‚é–“
    processing_time = time.time() - start_time
    result_dict["processing_time"] = round(processing_time, 3)
    print(f"â±ï¸  è™•ç†æ™‚é–“: {processing_time:.3f} ç§’")
    
    return result_dict

def force_two_speaker_separation(separator: AudioSeparator, audio_tensor: torch.Tensor, 
                               output_base: Path, duration: float) -> List[Dict]:
    """
    å¼·åˆ¶åŸ·è¡Œé›™èªè€…åˆ†é›¢ï¼Œç¹éèªªè©±è€…æ•¸é‡æª¢æ¸¬
    
    Args:
        separator: éŸ³é »åˆ†é›¢å™¨å¯¦ä¾‹
        audio_tensor: éŸ³é »å¼µé‡
        output_base: è¼¸å‡ºåŸºç¤è·¯å¾‘
        duration: éŸ³é »æ™‚é•·
        
    Returns:
        List[Dict]: åˆ†é›¢æª”æ¡ˆä¿¡æ¯åˆ—è¡¨
    """
    separated_files = []
    
    try:
        with torch.no_grad():
            # ç›´æ¥åŸ·è¡Œåˆ†é›¢æ¨¡å‹ï¼Œä¸é€²è¡Œèªªè©±è€…æ•¸é‡æª¢æ¸¬

            # ç¢ºä¿è¼¸å…¥æ˜¯ [batch, samples] æ ¼å¼
            if len(audio_tensor.shape) == 3:
                if audio_tensor.shape[1] == 1:
                    audio_tensor = audio_tensor.squeeze(1)
            separated = separator.model.separate_batch(audio_tensor)

            # æ‡‰ç”¨é™å™ªï¼ˆå¦‚æœå•Ÿç”¨ï¼‰
            if separator.enable_noise_reduction:
                enhanced_separated = separator.enhance_separation(separated)
            else:
                enhanced_separated = separated
            
            # æ¸…ç†è¨˜æ†¶é«”
            del separated
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
            
            # è™•ç†è¼¸å‡ºæ ¼å¼ä¸¦å¼·åˆ¶ç”¢ç”Ÿå…©å€‹æª”æ¡ˆ

            # SpeechBrain æ¨¡å‹è¼¸å‡ºè™•ç†
            if len(enhanced_separated.shape) == 3:
                num_speakers = min(enhanced_separated.shape[2], 2)  # å¼·åˆ¶é™åˆ¶ç‚º2
                speaker_dim = 2
            else:
                num_speakers = 1
                speaker_dim = 0

            
            # å¼·åˆ¶ç”¢ç”Ÿå…©å€‹éŸ³æª”
            target_speakers = 2
            
            for i in range(target_speakers):
                try:
                    if i < num_speakers:
                        # æ­£å¸¸åˆ†é›¢çš„éŸ³æª”
                        if speaker_dim == 1:
                            speaker_audio = enhanced_separated[0, i, :].cpu()
                        elif speaker_dim == 2:
                            speaker_audio = enhanced_separated[0, :, i].cpu()
                        else:
                            # å¦‚æœåªæœ‰ä¸€å€‹è¼¸å‡ºï¼Œç¬¬ä¸€å€‹éŸ³æª”ä½¿ç”¨åŸå§‹è¼¸å‡º
                            if i == 0:
                                speaker_audio = enhanced_separated.cpu().squeeze()
                            else:
                                # ç¬¬äºŒå€‹éŸ³æª”ä½¿ç”¨åå‘éŸ³æª”æˆ–é™ä½éŸ³é‡çš„ç‰ˆæœ¬
                                speaker_audio = enhanced_separated.cpu().squeeze() * 0.3
                    else:
                        # å¦‚æœæ¨¡å‹è¼¸å‡ºä¸è¶³å…©å€‹ï¼Œå‰µå»ºç¬¬äºŒå€‹éŸ³æª”
                        # ä½¿ç”¨ç¬¬ä¸€å€‹éŸ³æª”çš„é™ä½éŸ³é‡ç‰ˆæœ¬ä½œç‚ºç¬¬äºŒå€‹éŸ³æª”
                        if speaker_dim == 1:
                            speaker_audio = enhanced_separated[0, 0, :].cpu() * 0.2
                        elif speaker_dim == 2:
                            speaker_audio = enhanced_separated[0, :, 0].cpu() * 0.2
                        else:
                            speaker_audio = enhanced_separated.cpu().squeeze() * 0.2
                    
                    # éŸ³é »è™•ç† - æ¡ç”¨èˆ‡ç³»çµ±ä¸»åˆ†é›¢æ¨¡çµ„ç›¸åŒçš„ç­–ç•¥
                    # éŸ³é »è™•ç† - æ¡ç”¨èˆ‡ç³»çµ±ä¸»åˆ†é›¢æ¨¡çµ„ç›¸åŒçš„ç­–ç•¥
                    if len(speaker_audio.shape) > 1:
                        speaker_audio = speaker_audio.squeeze()
                    
                    # æª¢æŸ¥éŸ³è¨Šæœ‰æ•ˆæ€§ - ä½¿ç”¨èˆ‡ä¸»ç³»çµ±ç›¸åŒçš„é–¾å€¼
                    rms = torch.sqrt(torch.mean(speaker_audio ** 2))
                    min_rms_threshold = 0.005
                    
                    if rms > min_rms_threshold:
                        # æ¡ç”¨æº«å’Œçš„æ­£è¦åŒ–è™•ç†ï¼Œèˆ‡ä¸»ç³»çµ±ä¸€è‡´
                        max_val = torch.max(torch.abs(speaker_audio))
                        if max_val > 0.95:  # åªåœ¨çœŸæ­£éœ€è¦æ™‚é€²è¡Œæ­£è¦åŒ–
                            # ä½¿ç”¨æº«å’Œçš„ç¸®æ”¾ï¼Œé¿å…æ”¹è®ŠéŸ³è³ªç‰¹å¾µ
                            scale_factor = 0.9 / max_val
                            speaker_audio = speaker_audio * scale_factor
                    else:
                        print(f"   - è­¦å‘Šï¼šèªè€… {i+1} èƒ½é‡å¤ªä½ (RMS={rms:.6f})ï¼Œå¯èƒ½æ˜¯ç„¡æ•ˆéŸ³æª”")
                        # å°æ–¼å¼·åˆ¶ç”¢ç”Ÿçš„ç„¡æ•ˆéŸ³æª”ï¼Œç¹¼çºŒè™•ç†ä½†æ¨™è¨˜ç‚ºä½å“è³ª
                        if i >= num_speakers:
                            speaker_audio = speaker_audio * 0.1  # é€²ä¸€æ­¥é™ä½éŸ³é‡
                    # æª¢æŸ¥éŸ³è¨Šæœ‰æ•ˆæ€§ - ä½¿ç”¨èˆ‡ä¸»ç³»çµ±ç›¸åŒçš„é–¾å€¼
                    rms = torch.sqrt(torch.mean(speaker_audio ** 2))
                    min_rms_threshold = 0.005
                    
                    if rms > min_rms_threshold:
                        # æ¡ç”¨æº«å’Œçš„æ­£è¦åŒ–è™•ç†ï¼Œèˆ‡ä¸»ç³»çµ±ä¸€è‡´
                        max_val = torch.max(torch.abs(speaker_audio))
                        if max_val > 0.95:  # åªåœ¨çœŸæ­£éœ€è¦æ™‚é€²è¡Œæ­£è¦åŒ–
                            # ä½¿ç”¨æº«å’Œçš„ç¸®æ”¾ï¼Œé¿å…æ”¹è®ŠéŸ³è³ªç‰¹å¾µ
                            scale_factor = 0.9 / max_val
                            speaker_audio = speaker_audio * scale_factor
                    else:
                        print(f"   - è­¦å‘Šï¼šèªè€… {i+1} èƒ½é‡å¤ªä½ (RMS={rms:.6f})ï¼Œå¯èƒ½æ˜¯ç„¡æ•ˆéŸ³æª”")
                        # å°æ–¼å¼·åˆ¶ç”¢ç”Ÿçš„ç„¡æ•ˆéŸ³æª”ï¼Œç¹¼çºŒè™•ç†ä½†æ¨™è¨˜ç‚ºä½å“è³ª
                        if i >= num_speakers:
                            speaker_audio = speaker_audio * 0.1  # é€²ä¸€æ­¥é™ä½éŸ³é‡
                    
                    final_tensor = speaker_audio.unsqueeze(0)
                    
                    # ç”Ÿæˆè¼¸å‡ºæª”æ¡ˆåç¨±
                    output_file = output_base.parent / f"{output_base.name}_{SPEAKER_PREFIX}{i+1}.{OUTPUT_FORMAT}"
                    
                    # ä¿å­˜éŸ³æª” - ä½¿ç”¨èˆ‡ç³»çµ±ä¸»åˆ†é›¢æ¨¡çµ„ç›¸åŒçš„å“è³ªè¨­å®š
                    # ä¿å­˜éŸ³æª” - ä½¿ç”¨èˆ‡ç³»çµ±ä¸»åˆ†é›¢æ¨¡çµ„ç›¸åŒçš„å“è³ªè¨­å®š
                    import torchaudio
                    torchaudio.save(
                        str(output_file),
                        final_tensor,
                        OUTPUT_SAMPLE_RATE,
                        bits_per_sample=16  # æŒ‡å®š16ä½å…ƒç¢ºä¿éŸ³è³ªï¼Œèˆ‡ä¸»ç³»çµ±ä¸€è‡´
                        OUTPUT_SAMPLE_RATE,
                        bits_per_sample=16  # æŒ‡å®š16ä½å…ƒç¢ºä¿éŸ³è³ªï¼Œèˆ‡ä¸»ç³»çµ±ä¸€è‡´
                    )
                    
                    separated_files.append({
                        "speaker": i + 1,
                        "file_path": str(output_file),
                        "start_time": 0.0,
                        "end_time": duration,
                        "duration": duration,
                        "forced_generation": i >= num_speakers  # æ¨™è¨˜æ˜¯å¦ç‚ºå¼·åˆ¶ç”¢ç”Ÿ
                    })
                    
                    print(f"   - {SPEAKER_PREFIX}{i+1}: {output_file.name} {'(å¼·åˆ¶ç”¢ç”Ÿ)' if i >= num_speakers else ''}")
                    
                except Exception as e:
                    print(f"âš ï¸  ç”¢ç”Ÿèªè€… {i+1} éŸ³æª”å¤±æ•—: {e}")
            
    except Exception as e:
        raise RuntimeError(f"å¼·åˆ¶åˆ†é›¢éç¨‹å¤±æ•—: {e}")
    
    return separated_files

# ä¿ç•™åŸå§‹å‡½æ•¸ä½œç‚ºå‚™é¸
def separate_single_audio_file(separator: AudioSeparator, 
                             input_path: str, 
                             output_base: Path) -> Dict:
    """
    åˆ†é›¢å–®å€‹éŸ³æª” (åŸå§‹ç‰ˆæœ¬ï¼Œä¾è³´èªªè©±è€…æª¢æ¸¬)
    
    Args:
        separator: éŸ³é »åˆ†é›¢å™¨å¯¦ä¾‹
        input_path: è¼¸å…¥éŸ³æª”è·¯å¾‘
        output_base: è¼¸å‡ºåŸºç¤è·¯å¾‘ï¼ˆä¸å«å‰¯æª”åï¼‰
        
    Returns:
        Dict: è™•ç†çµæœå­—å…¸
    """
    start_time = time.time()
    
    print(f"\n{'='*80}")
    print(f"ğŸ¯ åˆ†é›¢éŸ³æª”: {Path(input_path).name}")
    print(f"ğŸ“‚ è¼¸å…¥è·¯å¾‘: {input_path}")
    print(f"ğŸ“¤ è¼¸å‡ºåŸºç¤è·¯å¾‘: {output_base}")
    print(f"{'='*80}")
    
    result_dict = {
        "input_file": input_path,
        "output_base": str(output_base),
        "processing_time": 0,
        "timestamp": datetime.now().isoformat(),
        "status": "processing"
    }
    
    try:
        # æª¢æŸ¥è¼¸å…¥æª”æ¡ˆ
        if not os.path.exists(input_path):
            raise FileNotFoundError(f"è¼¸å…¥éŸ³æª”ä¸å­˜åœ¨: {input_path}")
        
        # è®€å–éŸ³æª”
        print("ğŸ”„ è®€å–éŸ³æª”...")
        audio_data, sample_rate = sf.read(input_path)
        
        # éŸ³æª”ä¿¡æ¯
        duration = len(audio_data) / sample_rate
        print(f"ğŸ“Š éŸ³æª”ä¿¡æ¯:")
        print(f"   - å–æ¨£ç‡: {sample_rate} Hz")
        print(f"   - éŸ³æª”é•·åº¦: {duration:.2f} ç§’")
        print(f"   - è²é“æ•¸: {audio_data.shape[1] if len(audio_data.shape) > 1 else 1}")
        
        # è½‰æ›ç‚ºé©ç•¶æ ¼å¼
        if len(audio_data.shape) > 1:
            audio_data = audio_data.mean(axis=1)  # è½‰ç‚ºå–®è²é“
        
        # è½‰æ›ç‚º tensor
        audio_tensor = torch.from_numpy(audio_data).float().unsqueeze(0)  # æ·»åŠ  batch ç¶­åº¦
        
        # é‡æ¡æ¨£è‡³ç›®æ¨™å–æ¨£ç‡ï¼ˆå¦‚æœéœ€è¦ï¼‰
        if sample_rate != OUTPUT_SAMPLE_RATE:
            print(f"ğŸ”„ é‡æ¡æ¨£è‡³ {OUTPUT_SAMPLE_RATE} Hz...")
            import torchaudio
            resampler = torchaudio.transforms.Resample(sample_rate, OUTPUT_SAMPLE_RATE)
            audio_tensor = resampler(audio_tensor)
        
        # ç¢ºä¿è¼¸å‡ºç›®éŒ„å­˜åœ¨
        output_base.parent.mkdir(parents=True, exist_ok=True)
        
        # åŸ·è¡Œåˆ†é›¢
        print("ğŸ”„ åŸ·è¡ŒéŸ³é »åˆ†é›¢...")
        separated_results = separator.separate_and_save(
            audio_tensor, 
            str(output_base.parent), 
            segment_index=0
        )
    
        # è™•ç†åˆ†é›¢çµæœ
        separated_files = []
        if separated_results:
            print(f"âœ… åˆ†é›¢æˆåŠŸï¼Œç”¢ç”Ÿ {len(separated_results)} å€‹æª”æ¡ˆ:")
            
            for i, (file_path, start_time_seg, end_time_seg, timestamp) in enumerate(separated_results):
                # é‡æ–°å‘½åæª”æ¡ˆä»¥ç¬¦åˆè¦æ±‚
                old_path = Path(file_path)
                new_filename = f"{output_base.name}_{SPEAKER_PREFIX}{i+1}.{OUTPUT_FORMAT}"
                new_path = output_base.parent / new_filename
                
                # ç§»å‹•ä¸¦é‡å‘½åæª”æ¡ˆ
                if old_path.exists():
                    old_path.rename(new_path)
                    separated_files.append({
                        "speaker": i + 1,
                        "file_path": str(new_path),
                        "start_time": start_time_seg,
                        "end_time": end_time_seg,
                        "duration": end_time_seg - start_time_seg
                    })
                    print(f"   - {SPEAKER_PREFIX}{i+1}: {new_path.name}")
        else:
            print("âš ï¸  åˆ†é›¢çµæœç‚ºç©ºï¼Œå¯èƒ½æ²’æœ‰æª¢æ¸¬åˆ°å¤šå€‹èªè€…")
        
        # æ›´æ–°çµæœ
        result_dict.update({
            "status": "success",
            "separated_files": separated_files,
            "num_speakers_detected": len(separated_files),
            "original_duration": duration,
            "original_sample_rate": sample_rate,
            "output_sample_rate": OUTPUT_SAMPLE_RATE
        })
        
    except Exception as e:
        error_msg = f"åˆ†é›¢å¤±æ•—: {str(e)}"
        print(f"âŒ {error_msg}")
        if ENABLE_VERBOSE:
            print("ğŸ” éŒ¯èª¤è©³æƒ…:")
            traceback.print_exc()
        
        result_dict.update({
            "status": "error",
            "error": error_msg,
            "separated_files": [],
            "num_speakers_detected": 0
        })
    
    # è¨ˆç®—è™•ç†æ™‚é–“
    processing_time = time.time() - start_time
    result_dict["processing_time"] = round(processing_time, 3)
    print(f"â±ï¸  è™•ç†æ™‚é–“: {processing_time:.3f} ç§’")
    
    return result_dict

def save_progress(results: List[Dict], progress_file: str):
    """ä¿å­˜è™•ç†é€²åº¦"""
    try:
        with open(progress_file, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2)
        print(f"ğŸ’¾ é€²åº¦å·²ä¿å­˜è‡³: {progress_file}")
    except Exception as e:
        print(f"âš ï¸  ä¿å­˜é€²åº¦å¤±æ•—: {e}")

def load_progress(progress_file: str) -> Tuple[List[Dict], int]:
    """
    è¼‰å…¥è™•ç†é€²åº¦
    
    Returns:
        Tuple[List[Dict], int]: (å·²è™•ç†çµæœåˆ—è¡¨, å·²è™•ç†æª”æ¡ˆæ•¸é‡)
    """
    if not os.path.exists(progress_file):
        return [], 0
    
    try:
        with open(progress_file, 'r', encoding='utf-8') as f:
            results = json.load(f)
        processed_count = len(results)
        print(f"ğŸ“¥ è¼‰å…¥é€²åº¦: å·²è™•ç† {processed_count} å€‹æª”æ¡ˆ")
        return results, processed_count
    except Exception as e:
        print(f"âš ï¸  è¼‰å…¥é€²åº¦å¤±æ•—: {e}")
        return [], 0

def generate_summary_report(results: List[Dict], total_time: float) -> Dict:
    """ç”Ÿæˆæ‘˜è¦å ±å‘Š"""
    total_files = len(results)
    successful_files = len([r for r in results if r.get("status") == "success"])
    failed_files = total_files - successful_files
    
    # çµ±è¨ˆå„ç¨®æƒ…æ³
    total_speakers_detected = sum(r.get("num_speakers_detected", 0) for r in results)
    files_with_speakers = len([r for r in results if r.get("num_speakers_detected", 0) > 0])
    processing_times = []
    total_audio_duration = 0
    
    for result in results:
        if "processing_time" in result:
            processing_times.append(result["processing_time"])
        if "original_duration" in result:
            total_audio_duration += result["original_duration"]
    
    # è¨ˆç®—å¹³å‡è™•ç†æ™‚é–“
    avg_processing_time = sum(processing_times) / len(processing_times) if processing_times else 0
    
    summary = {
        "ç¸½è™•ç†æ™‚é–“": f"{total_time:.2f} ç§’",
        "ç¸½æª”æ¡ˆæ•¸": total_files,
        "æˆåŠŸè™•ç†": successful_files,
        "å¤±æ•—æª”æ¡ˆ": failed_files,
        "æˆåŠŸç‡": f"{(successful_files/total_files*100):.1f}%" if total_files > 0 else "0%",
        "å¹³å‡å–®æª”è™•ç†æ™‚é–“": f"{avg_processing_time:.3f} ç§’",
        "ç¸½éŸ³é »æ™‚é•·": f"{total_audio_duration:.2f} ç§’",
        "æª¢æ¸¬åˆ°èªè€…çš„æª”æ¡ˆæ•¸": files_with_speakers,
        "ç¸½æª¢æ¸¬èªè€…æ•¸": total_speakers_detected,
        "å¹³å‡æ¯æª”èªè€…æ•¸": f"{(total_speakers_detected/files_with_speakers):.1f}" if files_with_speakers > 0 else "0",
        "è™•ç†æ™‚é–“ç¯„åœ": {
            "æœ€å¿«": f"{min(processing_times):.3f} ç§’" if processing_times else "N/A",
            "æœ€æ…¢": f"{max(processing_times):.3f} ç§’" if processing_times else "N/A"
        }
    }
    
    return summary

def batch_audio_separation(input_directory: str, output_directory: str):
    """
    æ‰¹æ¬¡éŸ³é »åˆ†é›¢åŠŸèƒ½
    
    Args:
        input_directory: è¼¸å…¥è³‡æ–™å¤¾è·¯å¾‘
        output_directory: è¼¸å‡ºè³‡æ–™å¤¾è·¯å¾‘
    """
    print("ğŸš€ æ‰¹æ¬¡éŸ³é »åˆ†é›¢è™•ç†å·¥å…·")
    print(f"è¼¸å…¥è³‡æ–™å¤¾: {input_directory}")
    print(f"è¼¸å‡ºè³‡æ–™å¤¾: {output_directory}")
    print(f"é™å™ªåŠŸèƒ½: {'å•Ÿç”¨' if ENABLE_NOISE_REDUCTION else 'åœç”¨'}")
    print(f"å¼·åˆ¶é›™èªè€…æ¨¡å¼: {'å•Ÿç”¨' if FORCE_TWO_SPEAKERS else 'åœç”¨'}")
    print("=" * 80)
    
    # æª¢æŸ¥è¼¸å…¥è³‡æ–™å¤¾
    if not os.path.exists(input_directory):
        print(f"âŒ éŒ¯èª¤ï¼šè¼¸å…¥è³‡æ–™å¤¾ä¸å­˜åœ¨ - {input_directory}")
        return
    
    # å‰µå»ºè¼¸å‡ºè³‡æ–™å¤¾
    os.makedirs(output_directory, exist_ok=True)
    
    # æœç´¢æ‰€æœ‰éŸ³æª”
    audio_files = find_all_audio_files(input_directory)
    if not audio_files:
        print("âŒ éŒ¯èª¤ï¼šæ‰¾ä¸åˆ°ä»»ä½•æ”¯æ´çš„éŸ³æª”")
        return
    
    # è¨­å®šè¼¸å‡ºæª”æ¡ˆåç¨±
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    base_name = f"batch_separation_results_{timestamp}"
    
    detailed_results_file = f"{base_name}_detailed.json"
    summary_results_file = f"{base_name}_summary.json"
    progress_file = f"{base_name}_progress.json"
    
    # è¼‰å…¥é€²åº¦ï¼ˆå¦‚æœå•Ÿç”¨ï¼‰
    results = []
    start_index = 0
    if ENABLE_PROGRESS_SAVE:
        results, start_index = load_progress(progress_file)
        if start_index > 0:
            print(f"ğŸ”„ ç¹¼çºŒå¾ç¬¬ {start_index + 1} å€‹æª”æ¡ˆé–‹å§‹è™•ç†...")
    
    try:
        print("ğŸ”„ åˆå§‹åŒ–éŸ³é »åˆ†é›¢å™¨...")
        separator = AudioSeparator(
            enable_noise_reduction=ENABLE_NOISE_REDUCTION,
            snr_threshold=SNR_THRESHOLD
        )
        print("âœ… éŸ³é »åˆ†é›¢å™¨åˆå§‹åŒ–å®Œæˆ")
        
        # é–‹å§‹æ‰¹æ¬¡è™•ç†
        total_start_time = time.time()
        
        for i, audio_file in enumerate(audio_files[start_index:], start_index):
            print(f"\né€²åº¦: {i + 1}/{len(audio_files)}")
            
            # ç”Ÿæˆè¼¸å‡ºè·¯å¾‘
            output_base = get_output_path(audio_file, input_directory, output_directory)
            
            # è™•ç†å–®å€‹æª”æ¡ˆ - æ ¹æ“šè¨­å®šé¸æ“‡è™•ç†æ–¹å¼
            if FORCE_TWO_SPEAKERS:
                result = separate_single_audio_file_force_two_speakers(separator, audio_file, output_base)
            else:
                result = separate_single_audio_file(separator, audio_file, output_base)
            results.append(result)
            
            # å®šæœŸä¿å­˜é€²åº¦
            if ENABLE_PROGRESS_SAVE and (i + 1) % PROGRESS_SAVE_INTERVAL == 0:
                save_progress(results, progress_file)
        
        # è¨ˆç®—ç¸½è™•ç†æ™‚é–“
        total_processing_time = time.time() - total_start_time
        
        print(f"\nğŸ‰ æ‰¹æ¬¡è™•ç†å®Œæˆï¼")
        print(f"â±ï¸  ç¸½è™•ç†æ™‚é–“: {total_processing_time:.2f} ç§’")
        
        # ç”Ÿæˆæ‘˜è¦å ±å‘Š
        summary = generate_summary_report(results, total_processing_time)
        
        # é¡¯ç¤ºæ‘˜è¦
        print("\n" + "=" * 80)
        print("ğŸ“‹ è™•ç†æ‘˜è¦")
        print("=" * 80)
        for key, value in summary.items():
            if isinstance(value, dict):
                print(f"{key}:")
                for sub_key, sub_value in value.items():
                    print(f"  {sub_key}: {sub_value}")
            else:
                print(f"{key}: {value}")
        
        # ä¿å­˜çµæœ
        if SAVE_DETAILED_RESULTS:
            with open(detailed_results_file, 'w', encoding='utf-8') as f:
                json.dump(results, f, ensure_ascii=False, indent=2)
            print(f"ğŸ’¾ è©³ç´°çµæœå·²ä¿å­˜è‡³: {detailed_results_file}")
        
        if SAVE_SUMMARY_RESULTS:
            with open(summary_results_file, 'w', encoding='utf-8') as f:
                json.dump(summary, f, ensure_ascii=False, indent=2)
            print(f"ğŸ’¾ æ‘˜è¦çµæœå·²ä¿å­˜è‡³: {summary_results_file}")
        
        # æ¸…ç†é€²åº¦æª”æ¡ˆ
        if ENABLE_PROGRESS_SAVE and os.path.exists(progress_file):
            os.remove(progress_file)
            print(f"ğŸ—‘ï¸  é€²åº¦æª”æ¡ˆå·²æ¸…ç†")
            
    except KeyboardInterrupt:
        print(f"\nâš ï¸  ä½¿ç”¨è€…ä¸­æ–·è™•ç†")
        if ENABLE_PROGRESS_SAVE:
            save_progress(results, progress_file)
            print(f"ğŸ’¾ é€²åº¦å·²ä¿å­˜ï¼Œä¸‹æ¬¡å¯ç¹¼çºŒè™•ç†")
    except Exception as e:
        print(f"\nâŒ æ‰¹æ¬¡è™•ç†ç™¼ç”ŸéŒ¯èª¤: {e}")
        if ENABLE_VERBOSE:
            traceback.print_exc()
        if ENABLE_PROGRESS_SAVE:
            save_progress(results, progress_file)

def main():
    """ä¸»å‡½æ•¸"""
    print("ğŸµ æ‰¹æ¬¡éŸ³é »åˆ†é›¢è™•ç†å·¥å…·")
    print("=" * 80)
    
    # æç¤ºç”¨æˆ¶å¯ä»¥ä¿®æ”¹è¨­å®š
    print(f"ğŸ“‚ è¼¸å…¥è³‡æ–™å¤¾: {INPUT_DIRECTORY}")
    print(f"ğŸ“¤ è¼¸å‡ºè³‡æ–™å¤¾: {OUTPUT_DIRECTORY}")
    print(f"ğŸ”§ é™å™ªåŠŸèƒ½: {'å•Ÿç”¨' if ENABLE_NOISE_REDUCTION else 'åœç”¨'}")
    print(f"ğŸ”§ å¼·åˆ¶é›™èªè€…: {'å•Ÿç”¨' if FORCE_TWO_SPEAKERS else 'åœç”¨'}")
    print(f"ğŸ”§ è©³ç´°è¼¸å‡º: {'é–‹å•Ÿ' if ENABLE_VERBOSE else 'é—œé–‰'}")
    print(f"ğŸ’¾ ä¿å­˜çµæœ: {'é–‹å•Ÿ' if SAVE_DETAILED_RESULTS else 'é—œé–‰'}")
    print(f"ğŸ”„ é€²åº¦ä¿å­˜: {'é–‹å•Ÿ' if ENABLE_PROGRESS_SAVE else 'é—œé–‰'}")
    print()
    
    if not os.path.exists(INPUT_DIRECTORY):
        print("ğŸ’¡ æç¤ºï¼šæ‰¾ä¸åˆ°é è¨­è¼¸å…¥è³‡æ–™å¤¾")
        print("è«‹ç·¨è¼¯æ­¤æª”æ¡ˆçš„ INPUT_DIRECTORY è®Šæ•¸ï¼ŒæŒ‡å‘æ‚¨æƒ³æ‰¹æ¬¡è™•ç†çš„è³‡æ–™å¤¾")
        print()
        return
    
    batch_audio_separation(INPUT_DIRECTORY, OUTPUT_DIRECTORY)

if __name__ == "__main__":
    main()
