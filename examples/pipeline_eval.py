import argparse
import csv
from dataclasses import dataclass
from pathlib import Path
from typing import Dict, List, Tuple
import time
import json
import gc

import numpy as np
import torch
import torchaudio
import tempfile
import soundfile as sf

import sys
from pathlib import Path

# å°‡å°ˆæ¡ˆæ ¹ç›®éŒ„åŠ å…¥ Python è·¯å¾‘
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from pipelines.orchestrator import init_pipeline_modules,run_pipeline_file
from utils.logger import get_logger
from modules.separation.separator import AudioSeparator
from modules.asr.text_utils import compute_cer, normalize_zh
# from modules.identification.VID_identify_v5 import SpeakerIdentifier
# from modules.asr.whisper_asr import WhisperASR
from scipy import stats
from sklearn.metrics import roc_curve, auc

# å‰µå»º logger å¯¦ä¾‹
logger = get_logger(__name__)

try:
    import matplotlib.pyplot as plt
    PLOTTING_AVAILABLE = True
except ImportError:
    PLOTTING_AVAILABLE = False
    logger.warning("matplotlib ä¸å¯ç”¨ï¼Œå°‡è·³éåœ–è¡¨ç”Ÿæˆ")


@dataclass
class SourceInfo:
    path: Path
    transcript: str | None
    mix_sdr: float | None = None
    sep_sdr: float | None = None
    delta_sdr: float | None = None
    clean_wave: torch.Tensor | None = None
    
    
def load_truth_map(path: Path) -> Dict[str, str]:
    mapping: Dict[str, str] = {}
    with path.open("r", encoding="utf-8") as f:
        next(f)  # skip header
        for line in f:
            line = line.strip()
            if not line:
                continue
            fname, txt = line.split(",", 1)
            mapping[fname] = txt
    return mapping


def load_mixture_map(path: Path) -> List[Tuple[str, str, str, str]]:
    rows: List[Tuple[str, str, str, str]] = []
    with path.open("r", encoding="utf-8") as f:
        reader = csv.DictReader(f)
        for row in reader:
            # è·¨å¹³å°è·¯å¾‘è™•ç†ï¼šæ‰‹å‹•å°‡ Windows é¢¨æ ¼çš„ \ è½‰æ›ç‚º / 
            # é€™æ¨£åœ¨ macOS/Linux/Windows éƒ½èƒ½æ­£ç¢ºè™•ç†
            src1_normalized = row["src1"].replace("\\", "/")
            src2_normalized = row["src2"].replace("\\", "/")
            rows.append((row["mix_id"], row["mix_file"], src1_normalized, src2_normalized))
    return rows

#è¨ˆç®—SI-SDR å¯åŒæ™‚è¨ˆç®—åˆ†é›¢å‰å’Œåˆ†é›¢å¾Œçš„SDR
def si_sdr(est: torch.Tensor, ref: torch.Tensor) -> float:
    """Scale-Invariant SDR for single-channel signals."""
    min_len = min(len(est), len(ref))
    est = est[:min_len]
    ref = ref[:min_len]

    est = est - est.mean()
    ref = ref - ref.mean()
    s_target = torch.dot(est, ref) * ref / torch.dot(ref, ref)
    e_noise = est - s_target
    return 10 * torch.log10((s_target.pow(2).sum()) / (e_noise.pow(2).sum())).item()


def calculate_topk_accuracy(segment_info: List[Dict], true_speakers: List[str]) -> Dict[str, float]:
    """è¨ˆç®— Top-K æº–ç¢ºç‡"""
    if not segment_info:
        return {}
    
    # ç°¡åŒ–ç‰ˆï¼šç›®å‰åªè¨ˆç®— Top-1ï¼ˆå› ç‚ºæˆ‘å€‘åªæœ‰æœ€ä½³é æ¸¬ï¼‰
    correct = sum(1 for info in segment_info if info['speaker'] in true_speakers)
    total = len(segment_info)
    
    return {
        'top1_accuracy': round(correct / total if total > 0 else 0, 4),
        'speaker_coverage': round(len(set(info['speaker'] for info in segment_info if info['speaker'] in true_speakers)) / len(true_speakers) if true_speakers else 0, 4)
    }


def calculate_per_speaker_metrics(segment_info: List[Dict], true_speakers: List[str]) -> Dict[str, Dict]:
    """è¨ˆç®—æ¯å€‹èªè€…çš„æ€§èƒ½æŒ‡æ¨™"""
    speaker_stats = {}
    
    for info in segment_info:
        speaker = info['speaker']
        if speaker not in speaker_stats:
            speaker_stats[speaker] = {
                'count': 0,
                'correct': 0,
                'distances': [],
                'avg_distance': 0,
                'accuracy': 0
            }
        
        speaker_stats[speaker]['count'] += 1
        speaker_stats[speaker]['distances'].append(info['distance'])
        
        if speaker in true_speakers:
            speaker_stats[speaker]['correct'] += 1
    
    # è¨ˆç®—å¹³å‡å€¼
    for speaker, stats in speaker_stats.items():
        stats['avg_distance'] = round(np.mean(stats['distances']), 4)
        stats['accuracy'] = round(stats['correct'] / stats['count'], 4)
    
    return {'per_speaker_stats': speaker_stats}


def calculate_confusion_data(pred_speakers: List[str], true_speakers: List[str]) -> Dict:
    """è¨ˆç®—æ··æ·†çŸ©é™£ç›¸é—œæ•¸æ“š"""
    all_speakers = sorted(set(pred_speakers + true_speakers))
    
    confusion_pairs = []
    for i, pred in enumerate(pred_speakers):
        true = true_speakers[i] if i < len(true_speakers) else 'unknown'
        confusion_pairs.append({
            'predicted': pred,
            'actual': true,
            'correct': pred == true
        })
    
    return {
        'confusion_pairs': confusion_pairs,
        'unique_speakers': all_speakers,
        'total_predictions': len(pred_speakers)
    }
#å–æª”æ¡ˆä¸¦çµ±è¨ˆSI-SDR
def compute_baseline_sisdr(
    mix_path: Path,
    src1_path: Path,
    src2_path: Path,
    separator: AudioSeparator,
    sep_paths: List[Path] | None = None,
) -> Dict[str, float]:
    """
    è¨ˆç®— baseline èˆ‡åˆ†é›¢å¾Œçš„ SI-SDRï¼Œä»¥åŠ Î”SI-SDRã€‚

    mix_path  : æ··éŸ³æª” (.wav)
    src1_path : ä¾†æº 1 (ä¹¾æ·¨)
    src2_path : ä¾†æº 2 (ä¹¾æ·¨)
    separator : ä½ åˆå§‹åŒ–å¥½çš„ AudioSeparator å¯¦ä¾‹
    sep_paths : ã€å¯é¸ã€‘å·²å­˜åœ¨çš„åˆ†é›¢å¾Œ wav è·¯å¾‘æ¸…å–®
                è‹¥ Noneï¼Œå‡½å¼æœƒè‡¨æ™‚å†è·‘ä¸€æ¬¡ separator å–å¾—åˆ†é›¢éŸ³
    return    : dictï¼Œå« 4 å€‹æ¬„ä½
    """
    # --- è®€å– waveforms --------------------------------------------------
    def _load(wav: Path) -> torch.Tensor:
        wav_tensor, _ = torchaudio.load(wav)
        return wav_tensor.squeeze(0)          # [1, T] -> [T]

    mix_wave  = _load(mix_path)
    src1_wave = _load(src1_path)
    src2_wave = _load(src2_path)

    # --- baselineï¼šæ··éŸ³ vs å…©å€‹ä¹¾æ·¨æº -----------------------------------
    sdr_mix_src1 = si_sdr(mix_wave, src1_wave)
    sdr_mix_src2 = si_sdr(mix_wave, src2_wave)

    # --- å–å¾—åˆ†é›¢å¾ŒéŸ³æª” ---------------------------------------------------
    if sep_paths is None:
        # å¦‚æœå‘¼å«ç«¯æ²’çµ¦ï¼Œå°±è‡¨æ™‚å†åˆ†ä¸€æ¬¡ï¼ˆçœäº‹ä½†è¼ƒè€—æ™‚ï¼‰
        tmp_dir = Path(tempfile.mkdtemp(prefix="eval_sep_"))
        # æ³¨æ„ï¼šseparator éœ€è¦ tensor åœ¨æ­£ç¢º device
        mix_wave_device = mix_wave.to(separator.device).unsqueeze(0)  # [T] â†’ [1, T]
        sep_paths = [
            Path(p) for p, *_ in
            separator.separate_and_save(mix_wave_device, tmp_dir.as_posix(), segment_index=0)
        ]

    # --- åˆ†é›¢å¾Œ vs ä¹¾æ·¨æºï¼šå–ã€Œæœ€ä½³å°æ‡‰ã€å³å¯ -----------------------------
    def _best_sdr(ref_wave: torch.Tensor) -> float:
        best = -1e9
        for p in sep_paths:
            est_wave = _load(p)
            best = max(best, si_sdr(est_wave, ref_wave))
        return best

    sdr_sep_src1 = _best_sdr(src1_wave)
    sdr_sep_src2 = _best_sdr(src2_wave)

    # --- Î”SI-SDR ---------------------------------------------------------
    delta1 = sdr_sep_src1 - sdr_mix_src1
    delta2 = sdr_sep_src2 - sdr_mix_src2

    return {
        "si_sdr_src1":       sdr_sep_src1,
        "si_sdr_src2":       sdr_sep_src2,
        "delta_si_sdr_src1": delta1,
        "delta_si_sdr_src2": delta2,
    }
#spkID
def compute_accuracy(pred_speakers: List[str], true_speakers: List[str]) -> float:
    """
    pred_speakers: pipeline åˆ†é›¢å¾Œæ‰€æœ‰æ®µè½çš„é æ¸¬ speaker id (e.g. ["spk3","spk10"])
    true_speakers: mixture_map å®šç¾©çš„å…©ä½åŸå§‹ speaker id (e.g. ["spk10","spk1"])
    å›å‚³å€¼: æ­£ç¢ºæ•¸ / 2 -> 0.0, 0.5, or 1.0
    """
    matched = len(set(pred_speakers) & set(true_speakers))
    return matched / len(true_speakers)

def compute_speaker_identification_metrics(bundle: List[dict], true_speakers: List[str], spk) -> dict:
    """
    è¨ˆç®—è©³ç´°çš„èªè€…è¾¨è­˜æŒ‡æ¨™
    
    Args:
        bundle: pipeline è¼¸å‡ºçš„éŸ³æ®µè³‡è¨Š
        true_speakers: çœŸå¯¦çš„èªè€… ID åˆ—è¡¨
        spk: èªè€…è¾¨è­˜å™¨å¯¦ä¾‹
        
    Returns:
        dict: åŒ…å«å„ç¨®æŒ‡æ¨™çš„å­—å…¸
    """
    metrics = {
        'accuracy': 0.0,
        'min_distance': "",
        'top1_top2_margin': "",
        'identification_scores': [],  # ç”¨æ–¼å¾ŒçºŒçµ±è¨ˆ
        'prediction_confidence': "",
        'distance_variance': "",
        'true_speaker1': '',
        'true_speaker2': '',
        'pred_speaker1': '',
        'pred_speaker2': '',
        'distance1': "",
        'distance2': ""
    }
    
    # è¨­å®šçœŸå¯¦èªè€…
    if len(true_speakers) >= 2:
        metrics['true_speaker1'] = true_speakers[0]
        metrics['true_speaker2'] = true_speakers[1]
    elif len(true_speakers) == 1:
        metrics['true_speaker1'] = true_speakers[0]
        metrics['true_speaker2'] = ''
    
    # åŸºæœ¬æº–ç¢ºç‡è¨ˆç®—
    pred_speakers = [seg.get("speaker") for seg in bundle if seg.get("speaker")]
    metrics['accuracy'] = compute_accuracy(pred_speakers, true_speakers)
    
    # æ”¶é›†æ‰€æœ‰éŸ³æ®µçš„è·é›¢è³‡è¨Š
    distances = []
    margins = []
    segment_info = []  # æ”¶é›†æ¯å€‹æ®µè½çš„è©³ç´°è³‡è¨Š
    
    for seg in bundle:
        speaker = seg.get('speaker', '')
        distance = seg.get('distance', None)
        
        if 'speaker_distances' in seg and seg['speaker_distances']:
            # æœ‰è©³ç´°çš„å€™é¸çµæœ - ä½¿ç”¨å®Œæ•´çš„è·é›¢è³‡è¨Š
            seg_distances = seg['speaker_distances']
            if len(seg_distances) >= 1:
                # æŒ‰è·é›¢æ’åºï¼ˆè·é›¢è¶Šå°è¶Šç›¸ä¼¼ï¼‰
                sorted_distances = sorted(seg_distances, key=lambda x: x[2])
                top1_dist = sorted_distances[0][2]
                top1_speaker = sorted_distances[0][1]
                
                # æ”¶é›†æ®µè½è³‡è¨Š
                segment_info.append({
                    'speaker': top1_speaker,
                    'distance': top1_dist,
                    'is_correct': top1_speaker in true_speakers
                })
                
                distances.append(top1_dist)
                
                # å¦‚æœæœ‰ç¬¬äºŒå€‹å€™é¸è€…ï¼Œè¨ˆç®— margin
                top2_dist = None
                if len(seg_distances) >= 2:
                    top2_dist = sorted_distances[1][2]
                    margins.append(top2_dist - top1_dist)  # margin è¶Šå¤§è¡¨ç¤ºä¿¡å¿ƒåº¦è¶Šé«˜
                
                # è¨˜éŒ„è­˜åˆ¥çµæœç”¨æ–¼ EER è¨ˆç®—
                is_correct = top1_speaker in true_speakers
                metrics['identification_scores'].append({
                    'distance': top1_dist,
                    'margin': (top2_dist - top1_dist) if top2_dist is not None else 0.0,
                    'is_correct': is_correct,
                    'predicted_speaker': top1_speaker
                })
        elif speaker and distance is not None:
            # æ²’æœ‰è©³ç´°è·é›¢è³‡è¨Šï¼Œä½†æœ‰åŸºæœ¬çš„èªè€…å’Œè·é›¢ - ä½¿ç”¨ API å›å‚³çš„è·é›¢
            segment_info.append({
                'speaker': speaker,
                'distance': distance,
                'is_correct': speaker in true_speakers
            })
            
            distances.append(distance)
            
            # å°æ–¼æ–°èªè€…æˆ–åªæœ‰åŸºæœ¬è³‡è¨Šçš„æƒ…æ³ï¼Œç„¡æ³•è¨ˆç®— margin
            metrics['identification_scores'].append({
                'distance': distance,
                'margin': 0.0,  # ç„¡æ³•è¨ˆç®— margin
                'is_correct': speaker in true_speakers,
                'predicted_speaker': speaker
            })
    
    # è¨­å®šé æ¸¬èªè€…å’Œå°æ‡‰è·é›¢
    if segment_info:
        # å–å‰å…©å€‹æ®µè½çš„é æ¸¬çµæœ
        if len(segment_info) >= 1:
            metrics['pred_speaker1'] = segment_info[0]['speaker']
            metrics['distance1'] = round(segment_info[0]['distance'], 4)
            
        if len(segment_info) >= 2:
            metrics['pred_speaker2'] = segment_info[1]['speaker'] 
            metrics['distance2'] = round(segment_info[1]['distance'], 4)
        else:
            # å¦‚æœåªæœ‰ä¸€å€‹æ®µè½ï¼Œç¬¬äºŒå€‹è¨­ç‚ºç©º
            metrics['pred_speaker2'] = ''
            metrics['distance2'] = ""
    
    # è¨ˆç®—çµ±è¨ˆæŒ‡æ¨™
    if distances:
        metrics['min_distance'] = round(min(distances), 4)
        metrics['distance_variance'] = round(np.var(distances), 4)
        
        # è¨ˆç®—é¡å¤–çš„èªè€…è¾¨è­˜æŒ‡æ¨™
        pred_speakers = [info['speaker'] for info in segment_info]
        pred_distances = [info['distance'] for info in segment_info]
        
        # Top-K æº–ç¢ºç‡åˆ†æ
        metrics.update(calculate_topk_accuracy(segment_info, true_speakers))
        
        # æŒ‰èªè€…æ€§èƒ½åˆ†æ
        metrics.update(calculate_per_speaker_metrics(segment_info, true_speakers))
        
        # æ··æ·†çŸ©é™£æ•¸æ“š
        metrics['confusion_data'] = calculate_confusion_data(pred_speakers, true_speakers)
    else:
        metrics['min_distance'] = ""
        metrics['distance_variance'] = ""
        
    if margins:
        metrics['top1_top2_margin'] = round(np.mean(margins), 4)
        metrics['prediction_confidence'] = round(np.mean(margins), 4)
    else:
        metrics['top1_top2_margin'] = ""
        metrics['prediction_confidence'] = ""
    
    return metrics

def calculate_batch_speaker_metrics(all_results: List[dict]) -> dict:
    """
    è¨ˆç®—æ•´å€‹æ‰¹æ¬¡çš„èªè€…è¾¨è­˜ç¶œåˆçµ±è¨ˆ
    
    Args:
        all_results: æ‰€æœ‰æ¸¬è©¦çµæœçš„åˆ—è¡¨
        
    Returns:
        dict: æ‰¹æ¬¡çµ±è¨ˆæŒ‡æ¨™
    """
    batch_metrics = {
        'total_samples': len(all_results),
        'overall_accuracy': 0.0,
        'eer': 0.0,
        'min_dcf': 0.0,
        'cllr': 0.0,
        'distance_distribution': {
            'mean': 0.0,
            'std': 0.0,
            'min': float('inf'),
            'max': float('-inf')
        },
        'margin_distribution': {
            'mean': 0.0,
            'std': 0.0,
            'min': float('inf'),
            'max': float('-inf')
        },
        'confidence_stability': 0.0
    }
    
    # æ”¶é›†æ‰€æœ‰è­˜åˆ¥çµæœ
    all_scores = []
    all_distances = []
    all_margins = []
    correct_predictions = 0
    total_predictions = 0
    
    for result in all_results:
        if 'identification_scores' in result and result['identification_scores']:
            for score_info in result['identification_scores']:
                all_scores.append(score_info)
                all_distances.append(score_info['distance'])
                all_margins.append(score_info['margin'])
                
                if score_info['is_correct']:
                    correct_predictions += 1
                total_predictions += 1
    
    # æ•´é«”æº–ç¢ºç‡
    if total_predictions > 0:
        batch_metrics['overall_accuracy'] = correct_predictions / total_predictions
    
    # è·é›¢åˆ†å¸ƒçµ±è¨ˆ
    if all_distances:
        batch_metrics['distance_distribution'] = {
            'mean': np.mean(all_distances),
            'std': np.std(all_distances),
            'min': np.min(all_distances),
            'max': np.max(all_distances)
        }
    
    # Margin åˆ†å¸ƒçµ±è¨ˆ
    if all_margins:
        batch_metrics['margin_distribution'] = {
            'mean': np.mean(all_margins),
            'std': np.std(all_margins),
            'min': np.min(all_margins),
            'max': np.max(all_margins)
        }
        
        # ä¿¡å¿ƒåº¦ç©©å®šæ€§ï¼ˆmargin çš„è®Šç•°ä¿‚æ•¸ï¼‰
        if np.mean(all_margins) > 0:
            batch_metrics['confidence_stability'] = np.std(all_margins) / np.mean(all_margins)
    
    # è¨ˆç®— EER å’Œ minDCFï¼ˆå¦‚æœæœ‰è¶³å¤ çš„æ­£è² æ¨£æœ¬ï¼‰
    if len(all_scores) > 10:  # éœ€è¦è¶³å¤ çš„æ¨£æœ¬
        try:
            # æº–å‚™ EER è¨ˆç®—çš„è³‡æ–™
            y_true = [1 if score['is_correct'] else 0 for score in all_scores]
            # è·é›¢è¶Šå°è¡¨ç¤ºè¶Šç›¸ä¼¼ï¼Œæ‰€ä»¥ç”¨è² è·é›¢ä½œç‚ºåˆ†æ•¸
            y_scores = [-score['distance'] for score in all_scores]
            
            if len(set(y_true)) > 1:  # ç¢ºä¿æœ‰æ­£è² æ¨£æœ¬
                fpr, tpr, thresholds = roc_curve(y_true, y_scores)
                
                # è¨ˆç®— EER
                fnr = 1 - tpr
                eer_index = np.nanargmin(np.absolute(fnr - fpr))
                batch_metrics['eer'] = (fpr[eer_index] + fnr[eer_index]) / 2
                
                # ç°¡åŒ–çš„ minDCF è¨ˆç®—ï¼ˆå‡è¨­ Cmiss=Cfa=1, Ptarget=0.01ï¼‰
                Ptarget = 0.01
                Cmiss = 1
                Cfa = 1
                dcf_values = []
                
                for i in range(len(fpr)):
                    dcf = Cmiss * fnr[i] * Ptarget + Cfa * fpr[i] * (1 - Ptarget)
                    dcf_values.append(dcf)
                
                batch_metrics['min_dcf'] = min(dcf_values)
                
                # ç°¡åŒ–çš„ Cllr è¨ˆç®—
                # é€™æ˜¯ä¸€å€‹ç°¡åŒ–ç‰ˆæœ¬ï¼Œå¯¦éš› Cllr éœ€è¦æ›´è¤‡é›œçš„è¨ˆç®—
                if len(y_scores) > 0:
                    batch_metrics['cllr'] = -np.mean([
                        np.log2(score) if label == 1 else np.log2(1 - score)
                        for score, label in zip(y_scores, y_true)
                        if 0 < score < 1
                    ]) if any(0 < score < 1 for score in y_scores) else 0.0
                    
        except Exception as e:
            logger.warning(f"è¨ˆç®— EER/minDCF æ™‚å‡ºéŒ¯: {e}")
            batch_metrics['eer'] = 0.0
            batch_metrics['min_dcf'] = 0.0
            batch_metrics['cllr'] = 0.0
    
    return batch_metrics

def create_speaker_identification_plots(all_results: List[dict], output_dir: Path) -> None:
    """
    å‰µå»ºèªè€…è¾¨è­˜åˆ†æåœ–è¡¨
    
    Args:
        all_results: æ‰€æœ‰æ¸¬è©¦çµæœ
        output_dir: åœ–è¡¨è¼¸å‡ºç›®éŒ„
    """
    if not PLOTTING_AVAILABLE:
        logger.warning("matplotlib not available, skipping chart generation")
        return
        
    output_dir.mkdir(exist_ok=True)
    
    # æ”¶é›†æ‰€æœ‰è³‡æ–™
    correct_distances = []
    incorrect_distances = []
    all_margins = []
    all_distances = []
    
    for result in all_results:
        if 'identification_scores' in result and result['identification_scores']:
            for score_info in result['identification_scores']:
                distance = score_info['distance']
                margin = score_info['margin']
                is_correct = score_info['is_correct']
                
                all_distances.append(distance)
                all_margins.append(margin)
                
                if is_correct:
                    correct_distances.append(distance)
                else:
                    incorrect_distances.append(distance)
    
    # 1. è·é›¢åˆ†å¸ƒåœ–ï¼ˆæ­£ç¢º vs éŒ¯èª¤é æ¸¬ï¼‰
    plt.figure(figsize=(12, 8))
    
    plt.subplot(2, 2, 1)
    if correct_distances and incorrect_distances:
        bins = np.linspace(min(all_distances), max(all_distances), 30)
        plt.hist(correct_distances, bins=bins, alpha=0.7, label='Correct Predictions', color='green', density=True)
        plt.hist(incorrect_distances, bins=bins, alpha=0.7, label='Incorrect Predictions', color='red', density=True)
        plt.xlabel('Voice Distance')
        plt.ylabel('Density')
        plt.title('Speaker ID Distance Distribution (Correct vs Incorrect)')
        plt.legend()
        plt.grid(True, alpha=0.3)
    
    # 2. Margin åˆ†å¸ƒåœ–
    plt.subplot(2, 2, 2)
    if all_margins:
        plt.hist(all_margins, bins=30, alpha=0.7, color='blue', density=True)
        plt.xlabel('Top-1 vs Top-2 Margin')
        plt.ylabel('Density')
        plt.title('Prediction Confidence Distribution (Margin)')
        plt.grid(True, alpha=0.3)
        
        # æ·»åŠ çµ±è¨ˆç·š
        mean_margin = np.mean(all_margins)
        plt.axvline(mean_margin, color='red', linestyle='--', label=f'Mean: {mean_margin:.3f}')
        plt.legend()
    
    # 3. ROC æ›²ç·šï¼ˆå¦‚æœæœ‰è¶³å¤ è³‡æ–™ï¼‰
    plt.subplot(2, 2, 3)
    if len(correct_distances) > 5 and len(incorrect_distances) > 5:
        y_true = [1] * len(correct_distances) + [0] * len(incorrect_distances)
        y_scores = [-d for d in correct_distances] + [-d for d in incorrect_distances]  # è² è·é›¢ä½œç‚ºåˆ†æ•¸
        
        fpr, tpr, _ = roc_curve(y_true, y_scores)
        roc_auc = auc(fpr, tpr)
        
        plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC Curve (AUC = {roc_auc:.3f})')
        plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
        plt.xlim([0.0, 1.0])
        plt.ylim([0.0, 1.05])
        plt.xlabel('False Positive Rate')
        plt.ylabel('True Positive Rate')
        plt.title('ROC Curve')
        plt.legend(loc="lower right")
        plt.grid(True, alpha=0.3)
    
    # 4. è·é›¢ vs Margin æ•£é»åœ–
    plt.subplot(2, 2, 4)
    if all_distances and all_margins and len(all_distances) == len(all_margins):
        # æ ¹æ“šæ­£ç¢ºæ€§è‘—è‰²
        colors = []
        for result in all_results:
            if 'identification_scores' in result and result['identification_scores']:
                for score_info in result['identification_scores']:
                    colors.append('green' if score_info['is_correct'] else 'red')
        
        if len(colors) == len(all_distances):
            plt.scatter(all_distances, all_margins, c=colors, alpha=0.6)
            plt.xlabel('Voice Distance')
            plt.ylabel('Top-1 vs Top-2 Margin')
            plt.title('Distance vs Confidence')
            plt.grid(True, alpha=0.3)
            
            # æ·»åŠ åœ–ä¾‹
            from matplotlib.patches import Patch
            legend_elements = [Patch(facecolor='green', label='Correct Predictions'),
                             Patch(facecolor='red', label='Incorrect Predictions')]
            plt.legend(handles=legend_elements)
    
    plt.tight_layout()
    plt.savefig(output_dir / 'speaker_identification_analysis.png', dpi=300, bbox_inches='tight')
    plt.close()
    
    logger.info(f"Speaker identification analysis chart saved to: {output_dir / 'speaker_identification_analysis.png'}")
    
    # å‰µå»ºæ“´å±•åˆ†æåœ–è¡¨
    create_extended_analysis_plots(all_results, output_dir)


def create_extended_analysis_plots(results: List[Dict], output_dir: Path):
    """å‰µå»ºæ“´å±•çš„èªè€…è­˜åˆ¥åˆ†æåœ–è¡¨"""
    import matplotlib.pyplot as plt
    from collections import defaultdict, Counter
    
    # æ”¶é›†æ‰€æœ‰èªè€…çµ±è¨ˆæ•¸æ“š
    all_speaker_stats = defaultdict(lambda: {'correct': 0, 'total': 0, 'distances': []})
    confusion_pairs = []
    
    for result in results:
        if 'per_speaker_stats' in result:
            for speaker, stats in result['per_speaker_stats'].items():
                all_speaker_stats[speaker]['total'] += stats['count']
                all_speaker_stats[speaker]['correct'] += stats['correct']
                all_speaker_stats[speaker]['distances'].extend(stats['distances'])
        
        if 'confusion_data' in result:
            confusion_pairs.extend(result['confusion_data']['confusion_pairs'])
    
    # å‰µå»º 2x3 çš„å­åœ–ä½ˆå±€
    fig, axes = plt.subplots(2, 3, figsize=(18, 12))
    
    # 1. æŒ‰èªè€…æº–ç¢ºç‡åœ–
    if all_speaker_stats:
        speakers = list(all_speaker_stats.keys())
        accuracies = [all_speaker_stats[s]['correct'] / all_speaker_stats[s]['total'] 
                     for s in speakers]
        
        axes[0, 0].bar(range(len(speakers)), accuracies)
        axes[0, 0].set_xlabel('Speaker ID')
        axes[0, 0].set_ylabel('Accuracy')
        axes[0, 0].set_title('Per-Speaker Accuracy')
        axes[0, 0].set_xticks(range(len(speakers)))
        axes[0, 0].set_xticklabels(speakers, rotation=45)
        axes[0, 0].grid(True, alpha=0.3)
    
    # 2. æŒ‰èªè€…å¹³å‡è·é›¢åœ–
    if all_speaker_stats:
        avg_distances = [np.mean(all_speaker_stats[s]['distances']) if all_speaker_stats[s]['distances'] else 0
                        for s in speakers]
        
        axes[0, 1].bar(range(len(speakers)), avg_distances)
        axes[0, 1].set_xlabel('Speaker ID')
        axes[0, 1].set_ylabel('Average Distance')
        axes[0, 1].set_title('Per-Speaker Average Distance')
        axes[0, 1].set_xticks(range(len(speakers)))
        axes[0, 1].set_xticklabels(speakers, rotation=45)
        axes[0, 1].grid(True, alpha=0.3)
    
    # 3. æ–°èªè€… vs å·²çŸ¥èªè€…æ¯”ä¾‹
    new_speakers = sum(1 for s in speakers if s.startswith('n'))
    known_speakers = len(speakers) - new_speakers
    
    axes[0, 2].pie([new_speakers, known_speakers], 
                   labels=['New Speakers', 'Known Speakers'],
                   autopct='%1.1f%%',
                   colors=['orange', 'blue'])
    axes[0, 2].set_title('New vs Known Speaker Distribution')
    
    # 4. æ··æ·†çŸ©é™£ç†±åœ–ï¼ˆç°¡åŒ–ç‰ˆï¼‰
    if confusion_pairs:
        pred_counts = Counter(pair['predicted'] for pair in confusion_pairs)
        true_counts = Counter(pair['actual'] for pair in confusion_pairs)
        
        # é¡¯ç¤ºæœ€å¸¸è¦‹çš„é æ¸¬çµæœ
        top_preds = dict(pred_counts.most_common(10))
        
        axes[1, 0].bar(range(len(top_preds)), list(top_preds.values()))
        axes[1, 0].set_xlabel('Predicted Speaker')
        axes[1, 0].set_ylabel('Count')
        axes[1, 0].set_title('Top Predicted Speakers')
        axes[1, 0].set_xticks(range(len(top_preds)))
        axes[1, 0].set_xticklabels(list(top_preds.keys()), rotation=45)
        axes[1, 0].grid(True, alpha=0.3)
    
    # 5. è·é›¢åˆ†å¸ƒç®±ç·šåœ–ï¼ˆæŒ‰èªè€…ï¼‰
    if all_speaker_stats and len(speakers) <= 15:  # é™åˆ¶èªè€…æ•¸é‡ä»¥å…åœ–è¡¨éæ–¼æ“æ“ 
        distance_data = [all_speaker_stats[s]['distances'] for s in speakers[:15]]
        distance_data = [d for d in distance_data if d]  # ç§»é™¤ç©ºåˆ—è¡¨
        
        if distance_data:
            axes[1, 1].boxplot(distance_data, labels=speakers[:len(distance_data)])
            axes[1, 1].set_xlabel('Speaker ID')
            axes[1, 1].set_ylabel('Distance')
            axes[1, 1].set_title('Distance Distribution by Speaker')
            axes[1, 1].tick_params(axis='x', rotation=45)
            axes[1, 1].grid(True, alpha=0.3)
    
    # 6. é æ¸¬ä¿¡å¿ƒåº¦èˆ‡æ­£ç¢ºæ€§é—œä¿‚
    confidences = []
    correctness = []
    
    for result in results:
        if 'identification_scores' in result:
            for score in result['identification_scores']:
                confidences.append(score.get('margin', 0))
                correctness.append(1 if score['is_correct'] else 0)
    
    if confidences and correctness:
        # å°‡ä¿¡å¿ƒåº¦åˆ†çµ„ä¸¦è¨ˆç®—æ¯çµ„çš„æº–ç¢ºç‡
        conf_bins = np.linspace(min(confidences), max(confidences), 10)
        bin_indices = np.digitize(confidences, conf_bins)
        
        bin_accuracies = []
        bin_centers = []
        
        for i in range(1, len(conf_bins)):
            mask = bin_indices == i
            if np.sum(mask) > 0:
                bin_acc = np.mean([correctness[j] for j in range(len(correctness)) if mask[j]])
                bin_accuracies.append(bin_acc)
                bin_centers.append((conf_bins[i-1] + conf_bins[i]) / 2)
        
        if bin_accuracies:
            axes[1, 2].plot(bin_centers, bin_accuracies, 'o-')
            axes[1, 2].set_xlabel('Confidence (Margin)')
            axes[1, 2].set_ylabel('Accuracy')
            axes[1, 2].set_title('Confidence vs Accuracy')
            axes[1, 2].grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.savefig(output_dir / 'extended_speaker_analysis.png', dpi=300, bbox_inches='tight')
    plt.close()
    
    logger.info(f"Extended speaker analysis chart saved to: {output_dir / 'extended_speaker_analysis.png'}")

def generate_speaker_metrics_report(batch_metrics: dict, output_path: Path) -> None:
    """
    ç”Ÿæˆèªè€…è¾¨è­˜æŒ‡æ¨™å ±å‘Š
    
    Args:
        batch_metrics: æ‰¹æ¬¡çµ±è¨ˆæŒ‡æ¨™
        output_path: å ±å‘Šè¼¸å‡ºè·¯å¾‘
    """
    report = f"""
# èªè€…è¾¨è­˜è©•ä¼°å ±å‘Š

## æ•´é«”çµ±è¨ˆ
- ç¸½æ¨£æœ¬æ•¸: {batch_metrics['total_samples']}
- æ•´é«”æº–ç¢ºç‡: {batch_metrics['overall_accuracy']:.4f}

## éŒ¯èª¤ç‡æŒ‡æ¨™
- EER (Equal Error Rate): {batch_metrics['eer']:.4f}
- minDCF (Minimum Detection Cost Function): {batch_metrics['min_dcf']:.4f}
- Cllr (Calibration Loss): {batch_metrics['cllr']:.4f}

## è·é›¢åˆ†å¸ƒçµ±è¨ˆ
- å¹³å‡è·é›¢: {batch_metrics['distance_distribution']['mean']:.4f}
- è·é›¢æ¨™æº–å·®: {batch_metrics['distance_distribution']['std']:.4f}
- æœ€å°è·é›¢: {batch_metrics['distance_distribution']['min']:.4f}
- æœ€å¤§è·é›¢: {batch_metrics['distance_distribution']['max']:.4f}

## Top-1 vs Top-2 Margin åˆ†å¸ƒ
- å¹³å‡ Margin: {batch_metrics['margin_distribution']['mean']:.4f}
- Margin æ¨™æº–å·®: {batch_metrics['margin_distribution']['std']:.4f}
- æœ€å° Margin: {batch_metrics['margin_distribution']['min']:.4f}
- æœ€å¤§ Margin: {batch_metrics['margin_distribution']['max']:.4f}

## ç³»çµ±ç©©å®šæ€§
- ä¿¡å¿ƒåº¦ç©©å®šæ€§ (è®Šç•°ä¿‚æ•¸): {batch_metrics['confidence_stability']:.4f}
  - æ•¸å€¼è¶Šå°è¡¨ç¤ºç³»çµ±é æ¸¬ä¿¡å¿ƒåº¦è¶Šç©©å®š
  - < 0.3: éå¸¸ç©©å®š
  - 0.3-0.5: ç©©å®š
  - 0.5-1.0: ä¸­ç­‰ç©©å®š
  - > 1.0: ä¸ç©©å®š

## æŒ‡æ¨™è§£é‡‹

### EER (Equal Error Rate)
ç­‰éŒ¯èª¤ç‡ï¼Œç•¶ False Accept Rate ç­‰æ–¼ False Reject Rate æ™‚çš„éŒ¯èª¤ç‡ã€‚
æ•¸å€¼è¶Šå°è¶Šå¥½ï¼Œè¡¨ç¤ºç³»çµ±è­˜åˆ¥èƒ½åŠ›è¶Šå¼·ã€‚

### minDCF (Minimum Detection Cost Function)
æœ€å°æª¢æ¸¬ä»£åƒ¹å‡½æ•¸ï¼Œè€ƒæ…®ä¸åŒé¡å‹éŒ¯èª¤çš„ä»£åƒ¹ã€‚
æ•¸å€¼è¶Šå°è¶Šå¥½ï¼Œè¡¨ç¤ºç³»çµ±æ•´é«”æ€§èƒ½è¶Šå„ªã€‚

### Cllr (Calibration Loss)
æ ¡æº–æå¤±ï¼Œè¡¡é‡ç³»çµ±è¼¸å‡ºåˆ†æ•¸çš„æ ¡æº–æ€§ã€‚
æ•¸å€¼è¶Šå°è¶Šå¥½ï¼Œè¡¨ç¤ºç³»çµ±è¼¸å‡ºçš„ä¿¡å¿ƒåº¦èˆ‡å¯¦éš›æº–ç¢ºç‡è¶Šä¸€è‡´ã€‚

### Top-1 vs Top-2 Margin
æœ€ç›¸ä¼¼èªè€…èˆ‡ç¬¬äºŒç›¸ä¼¼èªè€…çš„è·é›¢å·®å€¼ã€‚
æ•¸å€¼è¶Šå¤§è¡¨ç¤ºç³»çµ±å°é æ¸¬çµæœè¶Šæœ‰ä¿¡å¿ƒã€‚

### è·é›¢åˆ†å¸ƒ
è²ç´‹æ¯”å°çš„é¤˜å¼¦è·é›¢åˆ†å¸ƒæƒ…æ³ã€‚
æ­£ç¢ºé æ¸¬çš„è·é›¢æ‡‰è©²æ˜é¡¯å°æ–¼éŒ¯èª¤é æ¸¬çš„è·é›¢ã€‚
"""
    
    with output_path.open('w', encoding='utf-8') as f:
        f.write(report)
    
    logger.info(f"Speaker identification evaluation report saved to: {output_path}")

NUM_MAP = {'0':'é›¶','1':'ä¸€','2':'äºŒ','3':'ä¸‰','4':'å››',
           '5':'äº”','6':'å…­','7':'ä¸ƒ','8':'å…«','9':'ä¹'}

def normalize_numbers_to_zh(text: str) -> str:
    return "".join(NUM_MAP.get(ch, ch) for ch in text)

def load_config_and_data() -> Tuple[argparse.Namespace, Path, Path, Dict[str, str], List[Tuple[str, str, str, str]]]:
    parser = argparse.ArgumentParser(description="Run speech pipeline and evaluate")
    parser.add_argument("--mix-dir", default="data/mix", help="mixè·¯å¾‘")
    parser.add_argument("--clean-dir", default="data/clean", help="cleanè·¯å¾‘")
    parser.add_argument("--truth-map", default="data/truth_map.csv")
    parser.add_argument("--test-list",  default="data/mix/mixture_map.csv",  help="batch æ¸¬è©¦æ¸…å–® CSV (mix_id,mix_file,src1,src2)")
    parser.add_argument("--out", default="work_output/pipeline_results.csv")
    args = parser.parse_args()

    mix_dir = Path(args.mix_dir)
    clean_dir = Path(args.clean_dir)
    truth_map = load_truth_map(Path(args.truth_map))
    mixture_rows = load_mixture_map(Path(args.test_list))
    Path(args.out).parent.mkdir(parents=True, exist_ok=True)

    return args, mix_dir, clean_dir, truth_map, mixture_rows

def prepare_results_structure() -> tuple[list[str], list[dict]]:
    columns = [
        "mix_id", "mix_file",
        "sep_time", "sid_time", "asr_time", "total_time",
        "si_sdr_src1", "si_sdr_src2",
        "delta_si_sdr_src1", "delta_si_sdr_src2",
        "accuracy", "min_distance", "top1_top2_margin", 
        "prediction_confidence", "distance_variance",
        "true_speaker1", "true_speaker2",
        "pred_speaker1", "pred_speaker2", 
        "distance1", "distance2",
        "cer1", "cer2", "avg_conf",
        "ref_text1", "pred_text1", "ref_text2", "pred_text2",
    ]
    return columns, []

def process_one_mixture(
    mix_path: str,
    mix_id: str = None,
    sep=None, spk=None, asr=None,
    clean_dir: Path = None,
    mixture_rows: List[Tuple[str, str, str, str]] = None,
    truth_map: Dict[str, str] = None,
) -> dict:
    """
    è™•ç†ä¸€çµ„æ··éŸ³éŸ³æª”ï¼Œè·‘å®Œæ•´çš„ pipelineï¼Œä¸¦å›å‚³æ‰€æœ‰å¯å–å¾—çš„æŒ‡æ¨™ã€‚
    
    mix_path: æ··éŸ³éŸ³æª”å®Œæ•´è·¯å¾‘
    mix_id: é¸å¡«çš„æª”æ¡ˆåç¨±ï¼ˆä¸å«å‰¯æª”åï¼‰ï¼Œé è¨­ç”¨æª”å
    return: ä¸€å€‹ dictï¼ŒåŒ…å« pipeline çš„æ‰€æœ‰å¯è§€å¯Ÿæ•¸æ“š
    """
    # ä½¿ç”¨ç›®å‰æ™‚é–“è¨ˆç®—ç¸½è€—æ™‚
    overall_start = time.perf_counter()
    mix_wav = Path(mix_path)
    
    if mix_id is None:
        mix_id = Path(mix_path).stem  # å–æª”åç•¶ä½œ id

    # ğŸŒ€ å‘¼å«ä¸»æµç¨‹è·‘å®Œæ•´ pipeline
    try:
        bundle, _ , stats = run_pipeline_file(mix_path,3, sep=sep, spk=spk, asr=asr)
        
        # ç‚ºæ¯å€‹æ®µè½æ·»åŠ è©³ç´°çš„èªè€…è¾¨è­˜è³‡è¨Š
        enhanced_bundle = []
        for seg in bundle:
            if 'path' in seg:
                # é‡æ–°é€²è¡Œèªè€…è¾¨è­˜ä»¥ç²å–è©³ç´°è³‡è¨Š
                try:
                    # è®€å–éŸ³è¨Š
                    import soundfile as sf
                    signal, sr = sf.read(seg['path'])
                    
                    # æå–åµŒå…¥å‘é‡
                    new_embedding = spk.audio_processor.extract_embedding_from_stream(signal, sr)
                    
                    # èˆ‡è³‡æ–™åº«æ¯”å°ä»¥ç²å–æ‰€æœ‰å€™é¸çµæœ
                    best_id, best_name, best_distance, all_distances = spk.database.compare_embedding(new_embedding)
                    
                    # æ·»åŠ è©³ç´°çš„èªè€…è¾¨è­˜è³‡è¨Š
                    seg['speaker_distances'] = all_distances  # æ‰€æœ‰å€™é¸çµæœ
                    seg['detailed_speaker_info'] = {
                        'best_id': best_id,
                        'best_name': best_name,
                        'best_distance': best_distance,
                        'all_candidates': all_distances
                    }
                    
                    # å¦‚æœ API è¿”å›çš„æ˜¯æ–°èªè€…ï¼Œæˆ‘å€‘ä»ç„¶ä¿ç•™èˆ‡ç¾æœ‰èªè€…çš„è·é›¢è³‡è¨Š
                    if seg.get('speaker', '').startswith('n') and all_distances:
                        # æ–°èªè€…çš„æƒ…æ³ï¼šæ›´æ–° distance ç‚ºèˆ‡æœ€è¿‘ç¾æœ‰èªè€…çš„è·é›¢
                        seg['distance'] = best_distance
                    
                except Exception as e:
                    logger.warning(f"ç„¡æ³•ç²å–æ®µè½ {seg.get('path', 'unknown')} çš„è©³ç´°èªè€…è³‡è¨Š: {e}")
                    seg['speaker_distances'] = []
                    seg['detailed_speaker_info'] = {}
                    
            enhanced_bundle.append(seg)
        
        bundle = enhanced_bundle
        
    except Exception as e:
        logger.error(f"è™•ç†æª”æ¡ˆ {mix_path} æ™‚å‡ºéŒ¯ï¼š{e}")
        return {
            "mix_id": mix_id,
            "mix_file": mix_path,
            "error": str(e)
        }

    # ğŸ“Š çµ±è¨ˆèˆ‡æ•´ç†å„éšæ®µè³‡è¨Š
    recog_texts = [s.get("text", "") for s in bundle if s.get("text")]
    confidences = [s.get("confidence", 0.0) for s in bundle if s.get("text")]

    full_text = " ".join(recog_texts)
    avg_conf = sum(confidences) / len(confidences) if confidences else 0.0

    result = {
        "mix_id": mix_id,
        "mix_file": mix_path,
        "seg_count": len(bundle),
        "sep_time": round(stats.get("separate", 0.0), 3),
        "sid_time": round(stats.get("speaker", 0.0), 3),
        "asr_time": round(stats.get("asr", 0.0), 3),
        "total_time": round(stats.get("total", 0.0), 3),
        "pipeline_time": round(stats.get("pipeline_total", 0.0), 3),
        "avg_conf": round(avg_conf, 4),
        "predicted_text": full_text,
        "segments": bundle,
        "overall_time": round(time.perf_counter() - overall_start, 3),
    }
    
    # â‘¢ åŠ ä¸Š baseline SI-SDR è¨ˆç®—    
    # è§£æ src1 / src2 æ¸…éŸ³è·¯å¾‘
    if clean_dir and mixture_rows:
        row = next((r for r in mixture_rows if r[0] == mix_id), None)
        if row:
            # è·¯å¾‘å·²åœ¨ load_mixture_map ä¸­æ­£è¦åŒ–ï¼Œç›´æ¥ä½¿ç”¨å³å¯
            src1 = clean_dir / row[2]
            src2 = clean_dir / row[3]
            sep_paths = [Path(s["path"]) for s in bundle if "path" in s]
            sisdr_metrics = compute_baseline_sisdr(mix_wav, src1, src2, sep, sep_paths)
            result.update(sisdr_metrics)
            
            # --- å–å‡º true speaker IDs from mixture_map row ---
            # è·¯å¾‘å·²æ­£è¦åŒ–ï¼Œç›´æ¥ä½¿ç”¨ Path è§£æ
            # row[2] = "speaker10/speaker10_17.wav" (å·²æ­£è¦åŒ–)
            # Path(row[2]).parent.name â†’ "speaker10"
            # .replace("speaker", "spk") â†’ "spk10"
            
            true_spk1 = Path(row[2]).parent.name.replace("speaker", "spk")
            true_spk2 = Path(row[3]).parent.name.replace("speaker", "spk")
            true_speakers = [true_spk1, true_spk2]
            # logger.debug(f"çœŸå¯¦èªè€…ï¼š{true_speakers}")
            
            # --- è¨ˆç®—è©³ç´°çš„èªè€…è¾¨è­˜æŒ‡æ¨™ ---
            speaker_metrics = compute_speaker_identification_metrics(bundle, true_speakers, spk)
            result.update(speaker_metrics)
            
                        # === CER & ref/pred text for each speaker === #
            # 1) åœ°å–æ¯å€‹ source çš„æ­£ç¢ºæ–‡å­—
            ref_fname1 = Path(src1).name                         # e.g. "speaker10_17.wav"
            ref_fname2 = Path(src2).name
            raw_ref1 = truth_map.get(ref_fname1, "")
            raw_ref2 = truth_map.get(ref_fname2, "")

            # 2) Normalize (å»æ¨™é»ã€ç©ºæ ¼ã€çµ±ä¸€ç¹é«”)
            ref_norm1 = normalize_zh(raw_ref1)
            ref_norm2 = normalize_zh(raw_ref2)
            # logger.debug(f"æ­£è¦åŒ–æ–‡å­—1ï¼š{ref_norm1}, æ­£è¦åŒ–æ–‡å­—2ï¼š{ref_norm2}")

            # 3) ç›´æ¥æŠ“ bundle å‰å…©æ®µæ–‡å­—ï¼Œä¸ç†æœƒ speaker id
            pred_texts = [seg.get("text", "") for seg in bundle if seg.get("text")]
            
            # ç¢ºä¿è‡³å°‘æœ‰ 2 å€‹å…ƒç´ ï¼Œä¸è¶³å‰‡è£œç©ºå­—ä¸²
            while len(pred_texts) < 2:
                pred_texts.append("")
            
            predA, predB = pred_texts[:2]                      # A = ç¬¬ä¸€æ®µ, B = ç¬¬äºŒæ®µ

            normA = normalize_numbers_to_zh(normalize_zh(predA))
            normB = normalize_numbers_to_zh(normalize_zh(predB))
            # logger.debug(f"é æ¸¬æ–‡å­—1ï¼š{normA}, é æ¸¬æ–‡å­—2ï¼š{normB}")
            # 4) äº¤å‰è¨ˆç®— 4 å€‹ CER
            cA1 = compute_cer(ref_norm1, normA) if ref_norm1 else None
            cB2 = compute_cer(ref_norm2, normB) if ref_norm2 else None
            cA2 = compute_cer(ref_norm2, normA) if ref_norm2 else None
            cB1 = compute_cer(ref_norm1, normB) if ref_norm1 else None
            # logger.debug(f"CER1ï¼š{cA1:.4f if cA1 else 'N/A'} CER2ï¼š{cB2:.4f if cB2 else 'N/A'}")
            
                        # 5) é¸ã€ŒåŠ ç¸½æœ€å°ã€çš„é…å°
            if (cA1 or 0) + (cB2 or 0) <= (cA2 or 0) + (cB1 or 0):
                final_pred1, final_pred2 = normA, normB
                cer1, cer2 = cA1, cB2
            else:
                final_pred1, final_pred2 = normB, normA
                cer1, cer2 = cB1, cA2
            # 5) å¯«å…¥çµæœ
            result.update({
                "ref_text1": ref_norm1,
                "pred_text1": final_pred1,
                "cer1": cer1,
                "ref_text2": ref_norm2,
                "pred_text2": final_pred2,
                "cer2": cer2,
            })
            torch.cuda.empty_cache()
            gc.collect()
        else:
            logger.warning(f"mix_id {mix_id} not found in mixture_map")
        
            

    return result

def run_and_evaluate_pipeline(
    mixture_csv: Path,
    mix_dir: Path,
    clean_dir: Path,
    truth_map: Dict[str, str],
    sep, spk, asr,
    output_csv: Path,
):
    test_rows = load_mixture_map(mixture_csv)               # â‘  è®€å–æ¸…å–®
    cols, _ = prepare_results_structure()
    
    # æ”¶é›†æ‰€æœ‰çµæœç”¨æ–¼æ‰¹æ¬¡çµ±è¨ˆ
    all_results = []

    with output_csv.open("w", newline="", encoding="utf-8") as fout:
        writer = csv.DictWriter(fout, fieldnames=cols)
        writer.writeheader()

        for mix_id, mix_file, *_ in test_rows:              # â‘¡ é€ç­†è·‘
            mix_path = mix_dir / mix_file
            logger.info(f"è™•ç† {mix_id} â†’ {mix_path}")

            res = process_one_mixture(
                str(mix_path), mix_id,
                sep=sep, spk=spk, asr=asr,
                clean_dir=clean_dir,
                mixture_rows=test_rows,
                truth_map=truth_map
            )

            # â‘¢ å¦‚æœ pipeline å‡ºéŒ¯å°±è·³éï¼Œä¸å¯«é€² CSV
            if "error" in res:
                logger.warning(f"è·³é {mix_id}ï¼ŒåŸå› ï¼š{res['error']}")
                continue

            # â‘£ åªç•™ä¸‹æŒ‡å®šæ¬„ä½ï¼Œä¸¦è™•ç†ç‰¹æ®Šæ•¸å€¼
            row = {}
            for k in cols:
                value = res.get(k, "")
                # è™•ç†ç„¡çª®å¤§å’Œ NaN å€¼
                if isinstance(value, float):
                    if value == float('inf'):
                        value = ""
                    elif value == float('-inf'):
                        value = ""
                    elif np.isnan(value):
                        value = ""
                row[k] = value
            writer.writerow(row)
            
            # â‘¤ æ”¶é›†çµæœç”¨æ–¼æ‰¹æ¬¡çµ±è¨ˆ
            all_results.append(res)

    # â‘¥ è¨ˆç®—æ‰¹æ¬¡çµ±è¨ˆä¸¦ä¿å­˜
    if all_results:
        batch_metrics = calculate_batch_speaker_metrics(all_results)
        
        # å‰µå»ºè¼¸å‡ºç›®éŒ„
        output_dir = output_csv.parent
        batch_stats_file = output_dir / f"{output_csv.stem}_batch_speaker_metrics.json"
        report_file = output_dir / f"{output_csv.stem}_speaker_analysis_report.md"
        
        # ä¿å­˜æ‰¹æ¬¡çµ±è¨ˆåˆ° JSON æ–‡ä»¶
        with batch_stats_file.open("w", encoding="utf-8") as f:
            json.dump(batch_metrics, f, indent=2, ensure_ascii=False)
        
        # ç”Ÿæˆåˆ†æåœ–è¡¨
        create_speaker_identification_plots(all_results, output_dir)
        
        # ç”Ÿæˆè©³ç´°å ±å‘Š
        generate_speaker_metrics_report(batch_metrics, report_file)
        
        logger.info(f"Batch speaker identification statistics saved to: {batch_stats_file}")
        
        # è¼¸å‡ºé—œéµçµ±è¨ˆè³‡è¨Š
        logger.info("ğŸ“Š Batch Speaker Identification Statistics Summary:")
        logger.info(f"   Total Samples: {batch_metrics['total_samples']}")
        logger.info(f"   Overall Accuracy: {batch_metrics['overall_accuracy']:.3f}")
        logger.info(f"   EER: {batch_metrics['eer']:.3f}")
        logger.info(f"   minDCF: {batch_metrics['min_dcf']:.3f}")
        logger.info(f"   Average Distance: {batch_metrics['distance_distribution']['mean']:.4f} Â± {batch_metrics['distance_distribution']['std']:.4f}")
        logger.info(f"   Average Margin: {batch_metrics['margin_distribution']['mean']:.4f} Â± {batch_metrics['margin_distribution']['std']:.4f}")
        logger.info(f"   Confidence Stability: {batch_metrics['confidence_stability']:.4f}")

    logger.info(f"All completed! Results written to {output_csv}")
    
def main():
    # 1ï¸âƒ£ æ¸…ç©º GPU å¿«å–
    torch.cuda.empty_cache()
    gc.collect()

    # 2ï¸âƒ£ è¼‰å…¥æ‰€æœ‰åƒæ•¸èˆ‡è³‡æ–™
    #    --mix-dirã€--clean-dirã€--truth-mapã€--test-listã€--out éƒ½å·²è¨­é è¨­
    args, mix_dir, clean_dir, truth_map, mixture_rows = load_config_and_data()

    # 3ï¸âƒ£ åˆå§‹åŒ–æ¨¡å‹
    sep, spk, asr, _ = init_pipeline_modules()

    run_and_evaluate_pipeline(
        mixture_csv=Path(args.test_list),
        mix_dir=mix_dir,
        clean_dir=clean_dir,
        truth_map=truth_map,
        sep=sep, spk=spk, asr=asr,
        output_csv=Path(args.out),
    )

    # # â‘£ æ¸¬è©¦ä¸€çµ„æ··éŸ³
    # test_mix_id = "m05"  # ä½ å¯ä»¥è‡ªè¨‚ä¸€å€‹ ID
    # test_mix_path = "data/mix/m05.wav"  # â† æ”¹æˆä½ å¯¦éš›æœ‰çš„éŸ³æª”è·¯å¾‘ï¼

    # logger.info(f"è™•ç†æ¸¬è©¦éŸ³æª”ï¼š{test_mix_path}")
    # result = process_one_mixture(
    #     test_mix_path,
    #     test_mix_id,
    #     sep=sep,
    #     spk=spk,
    #     asr=asr,
    #     clean_dir=clean_dir,
    #     mixture_rows=mixture_rows,
    #     truth_map=truth_map
    # )
    # #  å¯«å…¥ JSON æª”
    # with open(f"{test_mix_id}_result.json", "w", encoding="utf-8") as f:
    #     json.dump(result, f, indent=2, ensure_ascii=False)
    # logger.info(f"å·²å„²å­˜ JSON æª”è‡³ï¼š{test_mix_id}_result.json")
    
    
if __name__ == "__main__":
    main()
