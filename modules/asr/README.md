# ASR (èªéŸ³è¾¨è­˜) æ¨¡çµ„

**ç‰ˆæœ¬**: v0.4.0  
**ä½œè€…**: Gino  
**æœ€å¾Œæ›´æ–°è€…**: CYouuu  
**æœ€å¾Œæ›´æ–°**: 2025-07-21

ASR (Automatic Speech Recognition) æ¨¡çµ„è² è²¬å°‡éŸ³æª”è½‰æ›æˆæ–‡å­—ï¼Œä¸¦æä¾›é€è©æ™‚é–“æˆ³èˆ‡ä¿¡å¿ƒå€¼ã€‚æ¡ç”¨ Faster-Whisper æŠ€è¡“ï¼Œæ”¯æ´ GPU/CPU å‹•æ…‹åˆ‡æ›ã€‚

## ğŸ“ æ¨¡çµ„çµæ§‹

```
modules/asr/
â”œâ”€â”€ asr_model.py        # Faster-Whisper æ¨¡å‹è¼‰å…¥èˆ‡ç®¡ç†
â”œâ”€â”€ text_utils.py       # æ–‡å­—è™•ç†èˆ‡æ™‚é–“æˆ³åˆä½µå·¥å…·
â”œâ”€â”€ whisper_asr.py      # ASR ä¸»æµç¨‹ï¼Œå–®æª”èˆ‡æ‰¹æ¬¡è™•ç†
â””â”€â”€ README.md           # æœ¬æ–‡æª”
```

## ğŸš€ ä¸»è¦åŠŸèƒ½

- **èªéŸ³è½‰æ–‡å­—**: ä½¿ç”¨ Faster-Whisper é€²è¡Œé«˜ç²¾åº¦è½‰éŒ„
- **é€è©æ™‚é–“æˆ³**: æä¾›è©ç´šåˆ¥çš„æ™‚é–“æ¨™è¨˜
- **ä¿¡å¿ƒå€¼è©•ä¼°**: æ¯å€‹è©éƒ½æœ‰å°æ‡‰çš„ä¿¡å¿ƒåˆ†æ•¸
- **å¤šèªè¨€æ”¯æ´**: è‡ªå‹•èªè¨€åµæ¸¬æˆ–æŒ‡å®šèªè¨€
- **GPU/CPU åˆ‡æ›**: æ ¹æ“šç¡¬é«”ç’°å¢ƒå‹•æ…‹é¸æ“‡
- **æ‰¹æ¬¡è™•ç†**: æ”¯æ´ç›®éŒ„ç´šåˆ¥çš„æ‰¹æ¬¡è½‰éŒ„

## âš™ï¸ é…ç½®ç³»çµ± (V2)

### æ¨¡å‹é…ç½® (constants.py)
```python
# æ¨¡å‹åç¨±å¸¸æ•¸
DEFAULT_WHISPER_MODEL = "medium"           # é è¨­æ¨¡å‹
DEFAULT_WHISPER_BEAM_SIZE = 5             # é è¨­ beam size
WHISPER_MODEL_CACHE_DIR = "models/faster-whisper"  # æ¨¡å‹å¿«å–ç›®éŒ„

# æ¨¡å‹å°ç…§è¡¨
WHISPER_MODEL_MAP = {
    "tiny": "guillaumekln/faster-whisper-tiny",
    "base": "guillaumekln/faster-whisper-base", 
    "small": "guillaumekln/faster-whisper-small",
    "medium": "guillaumekln/faster-whisper-medium",
    "large-v2": "guillaumekln/faster-whisper-large-v2",
    "large-v3": "guillaumekln/faster-whisper-large-v3",
}
```

### ç’°å¢ƒè®Šæ•¸ (.env)
```bash
MODELS_BASE_DIR=./models          # æ¨¡å‹åŸºç¤ç›®éŒ„
FORCE_CPU=false                   # å¼·åˆ¶ä½¿ç”¨ CPU
```

## ğŸ’» ä½¿ç”¨æ–¹å¼

### åŸºæœ¬ä½¿ç”¨
```python
from modules.asr.whisper_asr import WhisperASR
from utils.constants import DEFAULT_WHISPER_MODEL

# åˆå§‹åŒ– ASR (ä½¿ç”¨é è¨­é…ç½®)
asr = WhisperASR(gpu=True)  # è‡ªå‹•ä½¿ç”¨ DEFAULT_WHISPER_MODEL

# æŒ‡å®šæ¨¡å‹
asr = WhisperASR(model_name="large-v2", gpu=True, beam=8)

# å–®æª”è½‰éŒ„
text, confidence, words = asr.transcribe("audio.wav")
print(f"æ–‡å­—: {text}")
print(f"å¹³å‡ä¿¡å¿ƒå€¼: {confidence:.3f}")
```

### æ‰¹æ¬¡è™•ç†
```python
# æ‰¹æ¬¡è½‰éŒ„ç›®éŒ„ä¸‹æ‰€æœ‰ wav æª”æ¡ˆ
json_path = asr.transcribe_dir("audio_folder", "output_id")
print(f"çµæœå„²å­˜æ–¼: {json_path}")
```

### è©ç´šæ™‚é–“æˆ³
```python
text, confidence, words = asr.transcribe("audio.wav")

for word_info in words:
    print(f"è©: {word_info['word']}")
    print(f"é–‹å§‹æ™‚é–“: {word_info['start']:.2f}s")
    print(f"çµæŸæ™‚é–“: {word_info['end']:.2f}s") 
    print(f"ä¿¡å¿ƒå€¼: {word_info['probability']:.3f}")
    print("-" * 20)
```

## ğŸ”§ æ¨¡å‹ç®¡ç† (asr_model.py)

### æ¨¡å‹è¼‰å…¥
```python
from modules.asr.asr_model import load_model
from utils.constants import WHISPER_MODEL_CACHE_DIR

# è¼‰å…¥æ¨¡å‹ï¼ˆä½¿ç”¨å¸¸æ•¸é…ç½®ï¼‰
model = load_model(
    model_name="medium",           # æˆ–ä½¿ç”¨ DEFAULT_WHISPER_MODEL
    gpu=True,
    cache=WHISPER_MODEL_CACHE_DIR  # ä½¿ç”¨å¸¸æ•¸å®šç¾©çš„å¿«å–ç›®éŒ„
)
```

### æ”¯æ´çš„æ¨¡å‹
- `tiny`: æœ€å¿«é€Ÿï¼Œç²¾åº¦è¼ƒä½
- `base`: å¹³è¡¡é€Ÿåº¦èˆ‡ç²¾åº¦
- `small`: è‰¯å¥½çš„ç²¾åº¦  
- `medium`: æ¨è–¦ä½¿ç”¨ï¼Œç²¾åº¦èˆ‡é€Ÿåº¦å¹³è¡¡
- `large-v2`: é«˜ç²¾åº¦ï¼Œéœ€è¦æ›´å¤šè¨ˆç®—è³‡æº
- `large-v3`: æœ€æ–°ç‰ˆæœ¬ï¼Œæœ€é«˜ç²¾åº¦

### è‡ªå‹•è¨­å‚™é¸æ“‡
```python
# ç³»çµ±æœƒè‡ªå‹•æ ¹æ“šç¡¬é«”é¸æ“‡æœ€ä½³è¨­å®šï¼š
# GPU å¯ç”¨: device="cuda", compute_type="float16"
# CPU æ¨¡å¼: device="cpu", compute_type="int8"
```

## ğŸ“ æ–‡å­—è™•ç† (text_utils.py)

### å­—å…ƒåˆ°è©çš„æ™‚é–“æˆ³åˆä½µ
Whisper é è¨­å›å‚³å­—å…ƒç´šæ™‚é–“æˆ³ï¼Œæœ¬æ¨¡çµ„ä½¿ç”¨çµå·´åˆ†è©å°‡å…¶åˆä½µç‚ºè©ç´šï¼š

```python
from modules.asr.text_utils import merge_char_to_word

# å­—å…ƒç´šæ™‚é–“æˆ³ (Whisper åŸå§‹è¼¸å‡º)
char_words = [
    {"start": 0.0, "end": 0.3, "word": "ä½ ", "probability": 0.95},
    {"start": 0.3, "end": 0.6, "word": "å¥½", "probability": 0.92}
]

# åˆä½µç‚ºè©ç´šæ™‚é–“æˆ³
word_level = merge_char_to_word("ä½ å¥½", char_words)
# çµæœ: [{"start": 0.0, "end": 0.6, "word": "ä½ å¥½", "probability": 0.935}]
```

### ä¾è³´å¥—ä»¶
```bash
# å„ªå…ˆä½¿ç”¨ jieba-fast (æ›´å¿«)
pip install jieba-fast

# å‚™ç”¨ jieba
pip install jieba
```

## ğŸ“Š è¼¸å‡ºæ ¼å¼

### å–®æª”è½‰éŒ„è¼¸å‡º
```python
# transcribe() å›å‚³æ ¼å¼
text = "é€™æ˜¯ä¸€æ®µæ¸¬è©¦èªéŸ³"                    # str: å®Œæ•´è½‰éŒ„æ–‡å­—
confidence = 0.852                          # float: å¹³å‡ä¿¡å¿ƒå€¼
words = [                                   # List[Dict]: è©ç´šæ™‚é–“æˆ³
    {
        "word": "é€™æ˜¯",
        "start": 0.0,
        "end": 0.4, 
        "probability": 0.89
    },
    {
        "word": "ä¸€æ®µ",
        "start": 0.4,
        "end": 0.8,
        "probability": 0.91
    },
    ...
]
```

### æ‰¹æ¬¡è™•ç†è¼¸å‡º JSON
```json
{
    "identity": "speaker_001",
    "timestamp": "2025-07-21 10:30:45",
    "results": {
        "audio1.wav": {
            "text": "è½‰éŒ„æ–‡å­—å…§å®¹",
            "confidence": 0.876,
            "words": [...]
        },
        "audio2.wav": {
            "text": "å¦ä¸€æ®µèªéŸ³",
            "confidence": 0.823,
            "words": [...]
        }
    },
    "summary": {
        "total_files": 2,
        "avg_confidence": 0.849,
        "processing_time": "12.34s"
    }
}
```

## ğŸ”„ èˆ‡å…¶ä»–æ¨¡çµ„æ•´åˆ

### èˆ‡ Separation æ¨¡çµ„
```python
# èªéŸ³åˆ†é›¢å¾Œçš„å¤šè»Œé“è½‰éŒ„
from modules.separation.separator import SeparationService
from modules.asr.whisper_asr import WhisperASR

separator = SeparationService()
asr = WhisperASR(model_name="medium", gpu=True)

# 1. åˆ†é›¢å¤šèªªè©±è€…èªéŸ³
separated_files = separator.separate("mixed_audio.wav", "output_dir")

# 2. åˆ†åˆ¥è½‰éŒ„æ¯å€‹è»Œé“
for track_file in separated_files:
    text, confidence, words = asr.transcribe(track_file)
    print(f"{track_file}: {text} (ä¿¡å¿ƒå€¼: {confidence:.3f})")
```

### èˆ‡ Database æ¨¡çµ„ (V2)
```python
# è½‰éŒ„çµæœå„²å­˜è‡³ Weaviate V2
from modules.database.database import DatabaseService
from modules.asr.whisper_asr import WhisperASR

db = DatabaseService()
asr = WhisperASR()

text, confidence, words = asr.transcribe("speaker.wav")

# V2 schema å„²å­˜èªéŸ³è½‰éŒ„è¨˜éŒ„
transcription_data = {
    "text": text,
    "confidence": confidence, 
    "word_timestamps": words,
    "audio_filename": "speaker.wav",
    "transcription_timestamp": datetime.now()
}
# å¯èˆ‡ Speaker è³‡æ–™å»ºç«‹é—œè¯
```

## ğŸ› ï¸ æ•ˆèƒ½èª¿æ•´

### æ¨¡å‹é¸æ“‡å»ºè­°
- **é–‹ç™¼/æ¸¬è©¦**: `tiny` æˆ– `base` (å¿«é€Ÿé©—è­‰)
- **ç”Ÿç”¢ç’°å¢ƒ**: `medium` (æ¨è–¦ï¼Œå¹³è¡¡ç²¾åº¦èˆ‡é€Ÿåº¦)
- **é«˜ç²¾åº¦éœ€æ±‚**: `large-v2` æˆ– `large-v3`

### GPU è¨˜æ†¶é«”å„ªåŒ–
```python
# å¤§æª”æ¡ˆè™•ç†æ™‚çš„è¨˜æ†¶é«”ç®¡ç†
asr = WhisperASR(model_name="medium", gpu=True)

# åˆ†æ®µè™•ç†é•·éŸ³æª” (ä¾‹å¦‚ >30åˆ†é˜)
def transcribe_long_audio(audio_path, chunk_duration=300):  # 5åˆ†é˜åˆ†æ®µ
    # å¯¦ä½œéŸ³æª”åˆ‡å‰²èˆ‡åˆä½µé‚è¼¯
    pass
```

### æ‰¹æ¬¡è™•ç†æœ€ä½³åŒ–
```python
# å¤§é‡æª”æ¡ˆè™•ç†æ™‚çš„è³‡æºç®¡ç†
import gc
import torch

def batch_transcribe_optimized(file_list):
    asr = WhisperASR(model_name="medium", gpu=True)
    
    for i, audio_file in enumerate(file_list):
        text, conf, words = asr.transcribe(audio_file)
        # è™•ç†çµæœ...
        
        # æ¯ 10 å€‹æª”æ¡ˆæ¸…ç†ä¸€æ¬¡è¨˜æ†¶é«”
        if i % 10 == 0:
            torch.cuda.empty_cache() if torch.cuda.is_available() else None
            gc.collect()
```

## ğŸ› å¸¸è¦‹å•é¡Œ

### Q: æ¨¡å‹ä¸‹è¼‰å¤±æ•—
```bash
# æ‰‹å‹•ä¸‹è¼‰æ¨¡å‹åˆ°å¿«å–ç›®éŒ„
mkdir -p models/faster-whisper
# æª¢æŸ¥ç¶²è·¯é€£ç·šï¼Œæˆ–ä½¿ç”¨ä»£ç†
```

### Q: GPU è¨˜æ†¶é«”ä¸è¶³
```python
# å¼·åˆ¶ä½¿ç”¨ CPU
asr = WhisperASR(model_name="medium", gpu=False)

# æˆ–è¨­å®šç’°å¢ƒè®Šæ•¸
export FORCE_CPU=true
```

### Q: ä¸­æ–‡åˆ†è©æ•ˆæœä¸ä½³
```python
# å˜—è©¦ä¸åŒçš„åˆ†è©æ¨¡å¼
# åœ¨ text_utils.py ä¸­å¯èª¿æ•´ jieba.cut() åƒæ•¸
```

## ğŸ“š ç›¸é—œæ–‡æª”

- [ä¸»å°ˆæ¡ˆ README](../../README.md)
- [Database V2 æ–‡æª”](../database/README.md)  
- [Separation æ¨¡çµ„](../separation/README.md)
- [API æ¥å£æ–‡æª”](../../api/README.md)
- [é…ç½®ç³»çµ±èªªæ˜](../../utils/README.md)

## ğŸ“ æ–‡å­—è™•ç† (text_utils.py)

### å­—å…ƒåˆ°è©çš„æ™‚é–“æˆ³åˆä½µ
Whisper é è¨­å›å‚³å­—å…ƒç´šæ™‚é–“æˆ³ï¼Œæœ¬æ¨¡çµ„ä½¿ç”¨çµå·´åˆ†è©å°‡å…¶åˆä½µç‚ºè©ç´šï¼š

```python
from modules.asr.text_utils import merge_char_to_word

# å­—å…ƒç´šæ™‚é–“æˆ³ (Whisper åŸå§‹è¼¸å‡º)
char_words = [
    {"start": 0.0, "end": 0.3, "word": "ä½ ", "probability": 0.95},
    {"start": 0.3, "end": 0.6, "word": "å¥½", "probability": 0.92}
]

# åˆä½µç‚ºè©ç´šæ™‚é–“æˆ³
word_level = merge_char_to_word("ä½ å¥½", char_words)
# çµæœ: [{"start": 0.0, "end": 0.6, "word": "ä½ å¥½", "probability": 0.935}]
```

### ä¾è³´å¥—ä»¶
```bash
# å„ªå…ˆä½¿ç”¨ jieba-fast (æ›´å¿«)
pip install jieba-fast

# å‚™ç”¨ jieba
pip install jieba
```

char_wordsï¼šWhisper è¼¸å‡ºçš„æ¯å€‹å­—è©æ™‚é–“æˆ³æ¸…å–®ï¼Œæ¯é …åŒ…å« startã€endã€wordã€probabilityã€‚

å›å‚³å€¼ï¼šè©ç´šåˆä½µçµæœï¼Œæ¯é …åŒ…å« wordã€startã€endã€probabilityã€‚

æœ¬æ¨¡çµ„å¯è®“è¼¸å‡ºçµæœæ›´é©åˆç”¨ä¾†åšå¾ŒçºŒçš„å­—å¹•å‘ˆç¾ã€èªæ„åˆ†æã€æˆ–è³‡æ–™åº«æ¯”å°ã€‚




ğŸ”¹ whisper_asr.py â€” ASR æ ¸å¿ƒå°è£èˆ‡æµç¨‹æ§åˆ¶
æ­¤æª”æ˜¯ asr_model èˆ‡ text_utils çš„æ•´åˆå°è£ï¼Œæä¾›å…©å¤§åŠŸèƒ½ï¼š

transcribeï¼šè™•ç†å–®ä¸€éŸ³æª”ï¼Œå›å‚³è½‰å¯«çµæœèˆ‡ä¿¡å¿ƒå€¼ã€‚

transcribe_dirï¼šæ‰¹æ¬¡è™•ç†è³‡æ–™å¤¾å…§æ‰€æœ‰ .wav æª”ï¼Œä¸¦è¼¸å‡º asr.jsonã€‚

å»ºç«‹æ™‚æœƒå‚³å…¥ model_nameã€gpu ç­‰åƒæ•¸ï¼Œå…§éƒ¨æœƒè‡ªå‹•å‘¼å« load_model å–å¾— Whisper æ¨¡å‹å¯¦ä¾‹ã€‚

é¡åˆ¥åˆå§‹åŒ–

WhisperASR(model_name="medium", gpu=False, beam=5, lang="auto")
model_nameï¼šWhisper æ¨¡å‹å¤§å°ã€‚

gpuï¼šæ˜¯å¦å•Ÿç”¨ GPUã€‚

beamï¼šBeam search å¤§å°ï¼Œæ•¸å€¼è¶Šå¤§çµæœè¶Šç©©å®šä½†é€Ÿåº¦è¼ƒæ…¢ã€‚

langï¼šèªè¨€ï¼Œå¯è¨­ç‚º "auto" æˆ–æŒ‡å®šèªç³»ï¼ˆå¦‚ "zh"ã€"en"ï¼‰ã€‚

ä¸»è¦æ–¹æ³•


def transcribe(wav_path: str) -> tuple[str, float, list[dict]]
å‚³å…¥å–®ä¸€ wav æª”è·¯å¾‘ï¼Œå›å‚³ï¼š

æ–‡å­—è½‰å¯«

å¹³å‡ä¿¡å¿ƒå€¼

è©ç´šæ™‚é–“æˆ³æ¸…å–®


def transcribe_dir(input_dir: str, output_id: str) -> str
è™•ç†æ•´å€‹è³‡æ–™å¤¾å…§æ‰€æœ‰ .wavï¼Œè½‰å¯«å¾Œè¼¸å‡ºç‚º data/{output_id}/asr.jsonï¼Œå›å‚³è¼¸å‡ºæª”æ¡ˆè·¯å¾‘ã€‚