"""
===============================================================================
å‹•æ…‹æ¨¡å‹ç®¡ç†å™¨ (Dynamic Model Manager)
===============================================================================

ç‰ˆæœ¬ï¼šv1.0.0
ä½œè€…ï¼šEvanLo62
æœ€å¾Œæ›´æ–°ï¼š2025-08-24

æ¨¡çµ„æ¦‚è¦ï¼š
-----------
æœ¬æ¨¡çµ„æä¾›å‹•æ…‹èªéŸ³åˆ†é›¢æ¨¡å‹ç®¡ç†åŠŸèƒ½ï¼Œæ ¹æ“šåµæ¸¬åˆ°çš„èªè€…æ•¸é‡è‡ªå‹•é¸æ“‡é©ç•¶çš„æ¨¡å‹ã€‚
æ”¯æ´æ¨¡å‹å¿«å–ã€è‡ªå‹•åˆ‡æ›å’Œè³‡æºç®¡ç†ï¼Œæä¾›é«˜æ•ˆèƒ½çš„æ¨¡å‹ç®¡ç†è§£æ±ºæ–¹æ¡ˆã€‚

ğŸ¯ æ ¸å¿ƒåŠŸèƒ½ï¼š
 â€¢ å‹•æ…‹æ¨¡å‹é¸æ“‡ï¼šæ ¹æ“šèªè€…æ•¸é‡è‡ªå‹•åˆ‡æ›æ¨¡å‹
 â€¢ æ¨¡å‹å¿«å–æ©Ÿåˆ¶ï¼šé¿å…é‡è¤‡è¼‰å…¥ç›¸åŒæ¨¡å‹
 â€¢ è³‡æºç®¡ç†ï¼šè‡ªå‹•æ¸…ç†GPUè¨˜æ†¶é«”
 â€¢ éŒ¯èª¤è™•ç†ï¼šå®Œæ•´çš„éŒ¯èª¤è™•ç†å’Œæ—¥èªŒè¨˜éŒ„

ğŸ”§ æŠ€è¡“æ¶æ§‹ï¼š
-----------
 æ¨¡å‹è¼‰å…¥    ï¼šSpeechBrain SepFormer
 è¨˜æ†¶é«”ç®¡ç†  ï¼šPyTorch CUDA è¨˜æ†¶é«”ç®¡ç†
 å¿«å–ç­–ç•¥    ï¼šåŸºæ–¼æ¨¡å‹é¡å‹çš„å¿«å–æ©Ÿåˆ¶

ğŸ“Š æ”¯æ´æ¨¡å‹ï¼š
-----------
 â€¢ SepFormer 2äººèªè€…åˆ†é›¢æ¨¡å‹ï¼ˆé è¨“ç·´ï¼‰
 â€¢ SepFormer 3äººèªè€…åˆ†é›¢æ¨¡å‹ï¼ˆè‡ªè¨“ç·´ï¼‰

===============================================================================
"""

import os
import torch
from speechbrain.inference import SepformerSeparation as separator
from enum import Enum

# å°å…¥æ—¥èªŒæ¨¡çµ„
from utils.logger import get_logger

# å°å…¥å¸¸æ•¸
from utils.constants import AUDIO_SAMPLE_RATE

# åˆå§‹åŒ–æ—¥èªŒ
logger = get_logger(__name__)

# æ¨¡å‹é¡å‹æšèˆ‰
class SeparationModel(Enum):
    SEPFORMER_2SPEAKER = "sepformer_2speaker"    # SepFormer 2äººèªè€…åˆ†é›¢æ¨¡å‹ï¼ˆé è¨“ç·´ï¼‰
    SEPFORMER_3SPEAKER = "sepformer_3speaker"    # SepFormer 3äººèªè€…åˆ†é›¢æ¨¡å‹ï¼ˆè‡ªè¨“ç·´ï¼‰

# æ¨¡å‹é…ç½®
MODEL_CONFIGS = {
    SeparationModel.SEPFORMER_2SPEAKER: {
        "model_name": "speechbrain/sepformer-whamr16k",
        "num_speakers": 2,
        "sample_rate": AUDIO_SAMPLE_RATE
    },
    SeparationModel.SEPFORMER_3SPEAKER: {
        "model_name": "AlvinLo62/sepformer-tcc300-3spks-16k-noisy",
        "num_speakers": 3,
        "sample_rate": AUDIO_SAMPLE_RATE
    }
}

class DynamicModelManager:
    """å‹•æ…‹æ¨¡å‹ç®¡ç†å™¨ - æ ¹æ“šèªè€…æ•¸é‡è‡ªå‹•é¸æ“‡é©ç•¶çš„æ¨¡å‹"""
    
    def __init__(self, device: str):
        """
        åˆå§‹åŒ–å‹•æ…‹æ¨¡å‹ç®¡ç†å™¨
        
        Args:
            device: è¨ˆç®—è¨­å‚™ (cuda:0, cpu ç­‰)
        """
        self.device = device
        self.loaded_models = {}  # å¿«å–å·²è¼‰å…¥çš„æ¨¡å‹
        self.current_model_type = None
        logger.info("å‹•æ…‹æ¨¡å‹ç®¡ç†å™¨åˆå§‹åŒ–å®Œæˆ")
    
    def get_model_for_speakers(self, num_speakers: int) -> tuple[separator, SeparationModel]:
        """
        æ ¹æ“šèªè€…æ•¸é‡å–å¾—é©ç•¶çš„æ¨¡å‹
        
        Args:
            num_speakers: åµæ¸¬åˆ°çš„èªè€…æ•¸é‡
            
        Returns:
            tuple: (æ¨¡å‹å¯¦ä¾‹, æ¨¡å‹é¡å‹)
        """
        # æ±ºå®šä½¿ç”¨å“ªå€‹æ¨¡å‹
        if num_speakers <= 2:
            target_model_type = SeparationModel.SEPFORMER_2SPEAKER
        else:
            target_model_type = SeparationModel.SEPFORMER_3SPEAKER
        
        # æª¢æŸ¥æ˜¯å¦éœ€è¦åˆ‡æ›æ¨¡å‹
        if target_model_type != self.current_model_type:
            logger.info(f"åˆ‡æ›æ¨¡å‹ï¼š{target_model_type.value} (èªè€…æ•¸é‡: {num_speakers})")
            
            # è¼‰å…¥æ–°æ¨¡å‹ï¼ˆå¦‚æœå°šæœªå¿«å–ï¼‰
            if target_model_type not in self.loaded_models:
                self._load_model(target_model_type)
            
            self.current_model_type = target_model_type
            
            # æ¸…ç† GPU è¨˜æ†¶é«”
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
        
        return self.loaded_models[target_model_type], target_model_type
    
    def _load_model(self, model_type: SeparationModel):
        """
        è¼‰å…¥æŒ‡å®šçš„æ¨¡å‹
        
        Args:
            model_type: è¦è¼‰å…¥çš„æ¨¡å‹é¡å‹
        """
        try:
            model_config = MODEL_CONFIGS[model_type]
            model_name = model_config["model_name"]
            
            logger.info(f"è¼‰å…¥æ¨¡å‹: {model_name}")
            
            # æª¢æŸ¥æœ¬åœ°æ¨¡å‹ç›®éŒ„
            local_model_path = os.path.abspath(f"models/{model_type.value}")
            if os.path.exists(local_model_path):
                # æª¢æŸ¥æ˜¯å¦æœ‰ç„¡æ•ˆçš„ç¬¦è™Ÿé€£çµ
                hyperparams_file = os.path.join(local_model_path, "hyperparams.yaml")
                if os.path.exists(hyperparams_file):
                    try:
                        with open(hyperparams_file, 'r', encoding='utf-8') as f:
                            content = f.read(100)
                    except (PermissionError, OSError, UnicodeDecodeError) as e:
                        logger.warning(f"æœ¬åœ°æ¨¡å‹æª”æ¡ˆç„¡æ³•è®€å–ï¼Œé‡æ–°ä¸‹è¼‰: {e}")
                        import shutil
                        shutil.rmtree(local_model_path, ignore_errors=True)
            
            # è¼‰å…¥æ¨¡å‹
            model = separator.from_hparams(
                source=model_name,
                savedir=local_model_path,
                run_opts={"device": self.device}
            )
            
            # æ¸¬è©¦æ¨¡å‹
            with torch.no_grad():
                test_audio = torch.randn(1, AUDIO_SAMPLE_RATE).to(self.device)
                _ = model.separate_batch(test_audio)
            
            self.loaded_models[model_type] = model
            logger.info(f"æ¨¡å‹ {model_type.value} è¼‰å…¥ä¸¦æ¸¬è©¦å®Œæˆ")
            
        except Exception as e:
            logger.error(f"è¼‰å…¥æ¨¡å‹ {model_type.value} å¤±æ•—: {e}")
            raise
    
    def get_model_config(self, model_type: SeparationModel) -> dict:
        """
        å–å¾—æ¨¡å‹é…ç½®è³‡è¨Š
        
        Args:
            model_type: æ¨¡å‹é¡å‹
            
        Returns:
            dict: æ¨¡å‹é…ç½®å­—å…¸
        """
        return MODEL_CONFIGS[model_type]
    
    def preload_model(self, model_type: SeparationModel):
        """
        é è¼‰å…¥æŒ‡å®šæ¨¡å‹
        
        Args:
            model_type: è¦é è¼‰å…¥çš„æ¨¡å‹é¡å‹
        """
        if model_type not in self.loaded_models:
            logger.info(f"é è¼‰å…¥æ¨¡å‹: {model_type.value}")
            self._load_model(model_type)
        else:
            logger.info(f"æ¨¡å‹ {model_type.value} å·²ç¶“è¼‰å…¥")
    
    def unload_model(self, model_type: SeparationModel):
        """
        å¸è¼‰æŒ‡å®šæ¨¡å‹
        
        Args:
            model_type: è¦å¸è¼‰çš„æ¨¡å‹é¡å‹
        """
        if model_type in self.loaded_models:
            del self.loaded_models[model_type]
            logger.info(f"å·²å¸è¼‰æ¨¡å‹: {model_type.value}")
            
            # å¦‚æœå¸è¼‰çš„æ˜¯ç•¶å‰æ¨¡å‹ï¼Œé‡ç½®ç•¶å‰æ¨¡å‹é¡å‹
            if self.current_model_type == model_type:
                self.current_model_type = None
            
            # æ¸…ç† GPU è¨˜æ†¶é«”
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
    
    def get_loaded_models(self) -> list[SeparationModel]:
        """
        å–å¾—å·²è¼‰å…¥çš„æ¨¡å‹åˆ—è¡¨
        
        Returns:
            list: å·²è¼‰å…¥çš„æ¨¡å‹é¡å‹åˆ—è¡¨
        """
        return list(self.loaded_models.keys())
    
    def cleanup(self):
        """æ¸…ç†æ‰€æœ‰è¼‰å…¥çš„æ¨¡å‹"""
        self.loaded_models.clear()
        self.current_model_type = None
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        logger.info("æ¨¡å‹ç®¡ç†å™¨å·²æ¸…ç†")

def create_dynamic_model_manager(device: str) -> DynamicModelManager:
    """
    å»ºç«‹å‹•æ…‹æ¨¡å‹ç®¡ç†å™¨çš„ä¾¿åˆ©å‡½å¼
    
    Args:
        device: è¨ˆç®—è¨­å‚™
        
    Returns:
        DynamicModelManager: å‹•æ…‹æ¨¡å‹ç®¡ç†å™¨å¯¦ä¾‹
    """
    return DynamicModelManager(device)

def get_available_models() -> dict[str, str]:
    """
    å–å¾—å¯ç”¨çš„æ¨¡å‹åˆ—è¡¨
    
    Returns:
        dict: æ¨¡å‹åç¨±åˆ°æè¿°çš„æ˜ å°„
    """
    return {
        "sepformer_2speaker": "SepFormer 2äººèªè€…åˆ†é›¢æ¨¡å‹ï¼ˆé è¨“ç·´ï¼‰",
        "sepformer_3speaker": "SepFormer 3äººèªè€…åˆ†é›¢æ¨¡å‹ï¼ˆè‡ªè¨“ç·´ï¼‰"
    }

def get_model_configs() -> dict:
    """
    å–å¾—æ‰€æœ‰æ¨¡å‹çš„é…ç½®è³‡è¨Š
    
    Returns:
        dict: æ¨¡å‹é…ç½®å­—å…¸
    """
    return MODEL_CONFIGS
