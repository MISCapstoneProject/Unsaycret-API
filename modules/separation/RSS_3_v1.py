import os
import pyaudio
import logging
import numpy as np
from datetime import datetime
from concurrent.futures import ThreadPoolExecutor

# å¾ utils æ¨¡çµ„è¼‰å…¥é…ç½®
from utils.constants import (
    AUDIO_CHUNK_SIZE, AUDIO_PYAUDIO_FORMAT_STR, AUDIO_CHANNELS, 
    AUDIO_RECORDING_RATE, AUDIO_TARGET_RATE, AUDIO_WINDOW_SIZE, 
    AUDIO_OVERLAP, AUDIO_MIN_ENERGY_THRESHOLD, NOISE_REDUCE_STRENGTH, 
    AUDIO_MAX_BUFFER_MINUTES, SNR_THRESHOLD, WIENER_FILTER_STRENGTH,
    HIGH_FREQ_CUTOFF, DYNAMIC_RANGE_COMPRESSION, CONVTASNET_MODEL_NAME,
    NUM_SPEAKERS_SEPARATION
)
from utils.env_config import FORCE_CPU, CUDA_DEVICE_INDEX

# è¨­å®šæ—¥èªŒ - æ”¹ç‚º INFO ç´šåˆ¥
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# ä¿®æ”¹æ¨¡å‹è¼‰å…¥æ–¹å¼
try:
    from asteroid.models import ConvTasNet
    USE_ASTEROID = True
    logger.info("ä½¿ç”¨ Asteroid å¥—ä»¶è¼‰å…¥ ConvTasNet")
except ImportError:
    from transformers import AutoModel
    USE_ASTEROID = False
    logger.info("ä½¿ç”¨ Transformers å¥—ä»¶è¼‰å…¥æ¨¡å‹")
import torch
import torchaudio
import noisereduce as nr
import threading
import time
from scipy import signal
from scipy.ndimage import uniform_filter1d

# è¨­å®šéŸ³è¨Šè£ç½®åƒæ•¸
DEVICE_INDEX = None


class AudioSeparator:
    def __init__(self, enable_noise_reduction=True, snr_threshold=SNR_THRESHOLD):
        # è¨­å‚™é¸æ“‡é‚è¼¯ï¼šå„ªå…ˆè€ƒæ…® FORCE_CPU è¨­å®š
        if FORCE_CPU:
            self.device = "cpu"
            logger.info("ğŸ”§ FORCE_CPU=trueï¼Œå¼·åˆ¶ä½¿ç”¨ CPU")
        else:
            if torch.cuda.is_available():
                # æª¢æŸ¥æŒ‡å®šçš„CUDAè¨­å‚™æ˜¯å¦å­˜åœ¨
                if CUDA_DEVICE_INDEX < torch.cuda.device_count():
                    self.device = f"cuda:{CUDA_DEVICE_INDEX}"
                    # ç¢ºä¿è¨­å®šæ­£ç¢ºçš„è¨­å‚™
                    torch.cuda.set_device(CUDA_DEVICE_INDEX)
                    if CUDA_DEVICE_INDEX != 0:
                        logger.info(f"ğŸ¯ ä½¿ç”¨ CUDA è¨­å‚™ç´¢å¼•: {CUDA_DEVICE_INDEX}")
                else:
                    logger.warning(f"âš ï¸  CUDA è¨­å‚™ç´¢å¼• {CUDA_DEVICE_INDEX} ä¸å­˜åœ¨ï¼Œä½¿ç”¨ cuda:0")
                    self.device = "cuda:0"
                    torch.cuda.set_device(0)
            else:
                self.device = "cpu"
                logger.info("ğŸ–¥ï¸  æœªåµæ¸¬åˆ° GPUï¼Œä½¿ç”¨ CPU")
        
        self.enable_noise_reduction = enable_noise_reduction
        self.snr_threshold = snr_threshold
        
        logger.info(f"ä½¿ç”¨è¨­å‚™: {self.device}")
        logger.info(f"æ¨¡å‹: {CONVTASNET_MODEL_NAME}")
        
        # è¨­è¨ˆæ›´æº«å’Œçš„ä½é€šæ¿¾æ³¢å™¨
        nyquist = AUDIO_TARGET_RATE // 2
        cutoff = min(HIGH_FREQ_CUTOFF, nyquist - 100)
        self.lowpass_filter = signal.butter(2, cutoff / nyquist, btype='low', output='sos')  # é™ä½æ¿¾æ³¢å™¨éšæ•¸
        
        try:
            logger.info("è¼‰å…¥æ¨¡å‹ä¸­...")
            self.model = self._load_model()
            logger.info("æ¨¡å‹è¼‰å…¥å®Œæˆ")
            self._test_model()
        except Exception as e:
            logger.error(f"æ¨¡å‹è¼‰å…¥å¤±æ•—: {e}")
            raise
        
        try:
            self.resampler = torchaudio.transforms.Resample(
                orig_freq=AUDIO_RECORDING_RATE,
                new_freq=AUDIO_TARGET_RATE
            ).to(self.device)
        except Exception as e:
            logger.error(f"é‡æ–°å–æ¨£å™¨åˆå§‹åŒ–å¤±æ•—: {e}")
            raise
        
        self.executor = ThreadPoolExecutor(max_workers=2)
        self.futures = []
        self.is_recording = False
        self.processing_stats = {
            'segments_processed': 0,
            'segments_skipped': 0,
            'errors': 0
        }
        
        self.max_buffer_size = int(AUDIO_RECORDING_RATE * AUDIO_MAX_BUFFER_MINUTES * 60 / AUDIO_CHUNK_SIZE)
        logger.info("AudioSeparator åˆå§‹åŒ–å®Œæˆ")

    def _load_model(self):
        """è¼‰å…¥ ConvTasNet æ¨¡å‹"""
        if USE_ASTEROID:
            try:
                model = ConvTasNet.from_pretrained(CONVTASNET_MODEL_NAME)
                model = model.to(self.device)
                model.eval()
                return model
            except Exception as e:
                logger.warning(f"Asteroid è¼‰å…¥å¤±æ•—ï¼Œå˜—è©¦å…¶ä»–æ–¹æ³•...")
        
        try:
            model = AutoModel.from_pretrained(
                CONVTASNET_MODEL_NAME,
                trust_remote_code=True
            )
            model = model.to(self.device)
            model.eval()
            return model
        except Exception as e:
            logger.info("å˜—è©¦ torch.hub è¼‰å…¥...")
            return self._manual_load_model()

    def _manual_load_model(self):
        """æ‰‹å‹•è¼‰å…¥æ¨¡å‹"""
        try:
            model_dir = "models/ConvTasNet"
            os.makedirs(model_dir, exist_ok=True)
            model = torch.hub.load('JorisCos/ConvTasNet', 'ConvTasNet_Libri3Mix_sepnoisy_16k', pretrained=True)
            model = model.to(self.device)
            model.eval()
            return model
        except Exception as e:
            logger.error(f"æ‰€æœ‰è¼‰å…¥æ–¹æ³•éƒ½å¤±æ•—: {e}")
            raise

    def _test_model(self):
        """æ¸¬è©¦æ¨¡å‹"""
        try:
            test_audio = torch.randn(1, 1, 16000).to(self.device)
            with torch.no_grad():
                output = self.model(test_audio)
                logger.info("æ¨¡å‹æ¸¬è©¦é€šé")
        except Exception as e:
            logger.error(f"æ¨¡å‹æ¸¬è©¦å¤±æ•—: {e}")
            raise

    def estimate_snr(self, signal):
        """ä¼°ç®—ä¿¡è™Ÿé›œè¨Šæ¯”"""
        try:
            signal_power = np.mean(signal ** 2)
            if len(signal) > 1000:
                noise_estimate = np.std(signal[-1000:]) ** 2
            else:
                noise_estimate = np.std(signal) ** 2 * 0.1
            noise_estimate = max(noise_estimate, 1e-10)
            snr = 10 * np.log10(signal_power / noise_estimate)
            return snr
        except:
            return 0

    def wiener_filter(self, audio_signal):
        """ç¶­ç´æ¿¾æ³¢å™¨ - æ›´æº«å’Œçš„è™•ç†"""
        try:
            f, t, stft = signal.stft(audio_signal, fs=AUDIO_TARGET_RATE, nperseg=512, noverlap=256)
            
            # ä½¿ç”¨æ›´æº«å’Œçš„é›œè¨Šä¼°è¨ˆ
            quiet_samples = min(int(AUDIO_TARGET_RATE * 0.05), len(audio_signal) // 8)
            noise_sample = audio_signal[:quiet_samples]
            _, _, noise_stft = signal.stft(noise_sample, fs=AUDIO_TARGET_RATE, nperseg=512, noverlap=256)
            noise_power = np.mean(np.abs(noise_stft) ** 2, axis=1, keepdims=True)
            
            signal_power = np.abs(stft) ** 2
            wiener_gain = signal_power / (signal_power + WIENER_FILTER_STRENGTH * noise_power)
            
            # é™åˆ¶å¢ç›Šç¯„åœä»¥é¿å…éåº¦è™•ç†
            wiener_gain = np.clip(wiener_gain, 0.1, 1.0)
            
            filtered_stft = stft * wiener_gain
            _, filtered_audio = signal.istft(filtered_stft, fs=AUDIO_TARGET_RATE)
            
            return filtered_audio[:len(audio_signal)]
        except:
            return audio_signal

    def smooth_audio(self, audio_signal):
        """éŸ³è¨Šå¹³æ»‘è™•ç†"""
        try:
            # ç§»é™¤çªç„¶çš„è·³èº
            diff = np.diff(audio_signal)
            threshold = np.std(diff) * 3  # æ›´å¯¬é¬†çš„é–¾å€¼
            artifact_indices = np.where(np.abs(diff) > threshold)[0]
            
            for idx in artifact_indices[:20]:  # é™åˆ¶è™•ç†æ•¸é‡
                if 0 < idx < len(audio_signal) - 1:
                    audio_signal[idx] = (audio_signal[idx-1] + audio_signal[idx+1]) / 2
            
            # è¼•å¾®å¹³æ»‘
            audio_signal = uniform_filter1d(audio_signal, size=3)
            
            # è¼•å¾®ä½é€šæ¿¾æ³¢
            audio_signal = signal.sosfilt(self.lowpass_filter, audio_signal)
            
            return audio_signal
        except:
            return audio_signal

    def dynamic_range_compression(self, audio_signal):
        """å‹•æ…‹ç¯„åœå£“ç¸®"""
        try:
            # è»Ÿé™åˆ¶å™¨
            threshold = 0.8
            ratio = DYNAMIC_RANGE_COMPRESSION
            
            # è¨ˆç®—çµ•å°å€¼
            abs_signal = np.abs(audio_signal)
            
            # å°è¶…éé–¾å€¼çš„éƒ¨åˆ†é€²è¡Œå£“ç¸®
            mask = abs_signal > threshold
            compressed = np.copy(audio_signal)
            
            if np.any(mask):
                over_threshold = abs_signal[mask]
                compressed_magnitude = threshold + (over_threshold - threshold) * ratio
                compressed[mask] = np.sign(audio_signal[mask]) * compressed_magnitude
            
            return compressed
        except:
            return audio_signal

    def spectral_gating(self, audio):
        """æ”¹è‰¯çš„é »è­œé–˜æ§é™å™ª"""
        try:
            noise_sample_length = max(int(AUDIO_TARGET_RATE * 0.05), 1)
            noise_sample = audio[:noise_sample_length]
            
            return nr.reduce_noise(
                y=audio,
                y_noise=noise_sample,
                sr=AUDIO_TARGET_RATE,
                prop_decrease=NOISE_REDUCE_STRENGTH,
                stationary=False,  # éç©©æ…‹é›œè¨Šè™•ç†
                n_jobs=1
            )
        except:
            return audio

    def enhance_separation(self, separated_signals):
        """å¢å¼·åˆ†é›¢æ•ˆæœ - æ”¹å–„éŸ³è³ª"""
        if not self.enable_noise_reduction:
            return separated_signals
        
        # è™•ç†å½¢ç‹€
        if len(separated_signals.shape) == 3:
            if separated_signals.shape[1] == NUM_SPEAKERS_SEPARATION:
                enhanced_signals = torch.zeros_like(separated_signals)
                speaker_dim = 1
                time_dim = 2
            elif separated_signals.shape[2] == NUM_SPEAKERS_SEPARATION:
                separated_signals = separated_signals.transpose(1, 2)
                enhanced_signals = torch.zeros_like(separated_signals)
                speaker_dim = 1
                time_dim = 2
            else:
                enhanced_signals = torch.zeros_like(separated_signals)
                speaker_dim = 2
                time_dim = 1
        else:
            if separated_signals.shape[0] == NUM_SPEAKERS_SEPARATION:
                separated_signals = separated_signals.unsqueeze(0)
                enhanced_signals = torch.zeros_like(separated_signals)
                speaker_dim = 1
                time_dim = 2
            else:
                separated_signals = separated_signals.unsqueeze(0).transpose(1, 2)
                enhanced_signals = torch.zeros_like(separated_signals)
                speaker_dim = 1
                time_dim = 2
        
        num_speakers = separated_signals.shape[speaker_dim]
        
        for i in range(num_speakers):
            if speaker_dim == 1:
                current_signal = separated_signals[0, i, :].cpu().numpy()
            else:
                current_signal = separated_signals[0, :, i].cpu().numpy()
            
            # å¤šéšæ®µéŸ³è³ªæ”¹å–„
            processed_signal = current_signal
            
            # 1. ç¶­ç´æ¿¾æ³¢
            signal_snr = self.estimate_snr(current_signal)
            if signal_snr < self.snr_threshold + 3:
                processed_signal = self.wiener_filter(processed_signal)
            
            # 2. å‚³çµ±é™å™ªï¼ˆåƒ…åœ¨å¿…è¦æ™‚ï¼‰
            if signal_snr < self.snr_threshold:
                processed_signal = self.spectral_gating(processed_signal)
            
            # 3. éŸ³è¨Šå¹³æ»‘å’Œä¿®å¾©
            processed_signal = self.smooth_audio(processed_signal)
            
            # 4. å‹•æ…‹ç¯„åœå£“ç¸®
            processed_signal = self.dynamic_range_compression(processed_signal)
            
            # 5. æœ€çµ‚æ­£è¦åŒ–
            max_val = np.max(np.abs(processed_signal))
            if max_val > 0:
                processed_signal = processed_signal / max_val * 0.95
            
            length = min(len(processed_signal), separated_signals.shape[time_dim])
            
            if speaker_dim == 1:
                enhanced_signals[0, i, :length] = torch.from_numpy(processed_signal[:length]).to(self.device)
            else:
                enhanced_signals[0, :length, i] = torch.from_numpy(processed_signal[:length]).to(self.device)
        
        return enhanced_signals

    def process_audio(self, audio_data):
        """è™•ç†éŸ³è¨Šæ ¼å¼"""
        try:
            # è½‰æ›ç‚º float32
            if AUDIO_PYAUDIO_FORMAT_STR == "paInt16":
                audio_float = audio_data.astype(np.float32) / 32768.0
            else:
                audio_float = audio_data.astype(np.float32)
            
            # èƒ½é‡æª¢æ¸¬ï¼šéä½å‰‡ç•¥é
            energy = np.mean(np.abs(audio_float))
            if energy < AUDIO_MIN_ENERGY_THRESHOLD:
                logger.debug(f"éŸ³è¨Šèƒ½é‡ ({energy:.6f}) ä½æ–¼é–¾å€¼ ({AUDIO_MIN_ENERGY_THRESHOLD})")
                return None
            
            # é‡å¡‘ç‚ºæ­£ç¢ºå½¢ç‹€
            if len(audio_float.shape) == 1:
                audio_float = audio_float.reshape(-1, AUDIO_CHANNELS)

            # èª¿æ•´å½¢ç‹€ä»¥ç¬¦åˆæ¨¡å‹è¼¸å…¥ï¼š[channels, time]
            audio_tensor = torch.from_numpy(audio_float).T.float()

            # å¦‚æœæ˜¯é›™è²é“è€Œæ¨¡å‹åªæ”¯æ´å–®è²é“å‰‡å–å¹³å‡
            if audio_tensor.shape[0] == 2:
                audio_tensor = torch.mean(audio_tensor, dim=0, keepdim=True)

            # ç§»è‡³ GPU ä¸¦é‡æ–°å–æ¨£è‡³ 16kHz
            audio_tensor = audio_tensor.to(self.device)
            resampled = self.resampler(audio_tensor)
            
            # ç¢ºä¿å½¢ç‹€æ­£ç¢º
            if len(resampled.shape) == 1:
                resampled = resampled.unsqueeze(0)
            
            return resampled
            
        except Exception as e:
            logger.error(f"éŸ³è¨Šè™•ç†éŒ¯èª¤ï¼š{e}")
            self.processing_stats['errors'] += 1
            return None

    def cleanup_futures(self):
        """æ¸…ç†å·²å®Œæˆçš„ä»»å‹™"""
        completed_futures = []
        for future in self.futures:
            if future.done():
                try:
                    future.result()  # ç²å–çµæœä»¥æ•ç²ä»»ä½•ç•°å¸¸
                except Exception as e:
                    logger.error(f"è™•ç†ä»»å‹™ç™¼ç”ŸéŒ¯èª¤ï¼š{e}")
                    self.processing_stats['errors'] += 1
                completed_futures.append(future)
        
        # ç§»é™¤å·²å®Œæˆçš„ä»»å‹™
        for future in completed_futures:
            self.futures.remove(future)

    def record_and_process(self, output_dir):
        """éŒ„éŸ³ä¸¦è™•ç†"""
        mixed_audio_buffer = []
        p = None
        stream = None
        
        try:
            p = pyaudio.PyAudio()
            
            # æª¢æŸ¥è¨­å‚™å¯ç”¨æ€§
            if DEVICE_INDEX is not None:
                device_info = p.get_device_info_by_index(DEVICE_INDEX)
                logger.info(f"ä½¿ç”¨éŸ³è¨Šè¨­å‚™: {device_info['name']}")
            
            stream = p.open(
                format=pyaudio.paFloat32,  # ä½¿ç”¨å¯¦éš›çš„ pyaudio å¸¸æ•¸
                channels=AUDIO_CHANNELS,
                rate=AUDIO_RECORDING_RATE,
                input=True,
                frames_per_buffer=AUDIO_CHUNK_SIZE,
                input_device_index=DEVICE_INDEX
            )
            
            logger.info("é–‹å§‹éŒ„éŸ³")
            
            # è¨ˆç®—ç·©è¡å€å¤§å°èˆ‡é‡ç–Šæ•¸æ“š
            samples_per_window = int(AUDIO_WINDOW_SIZE * AUDIO_RECORDING_RATE)
            window_frames = int(samples_per_window / AUDIO_CHUNK_SIZE)
            overlap_frames = int((AUDIO_OVERLAP * AUDIO_RECORDING_RATE) / AUDIO_CHUNK_SIZE)
            slide_frames = window_frames - overlap_frames
            
            buffer = []
            segment_index = 0
            self.is_recording = True
            last_stats_time = time.time()
            
            while self.is_recording:
                try:
                    data = stream.read(AUDIO_CHUNK_SIZE, exception_on_overflow=False)
                    frame = np.frombuffer(data, dtype=np.float32 if AUDIO_PYAUDIO_FORMAT_STR == "paFloat32" else np.int16)
                    
                    buffer.append(frame)
                    
                    # é™åˆ¶ mixed_audio_buffer å¤§å°ä»¥é˜²æ­¢è¨˜æ†¶é«”è€—ç›¡
                    mixed_audio_buffer.append(frame.copy())
                    if len(mixed_audio_buffer) > self.max_buffer_size:
                        mixed_audio_buffer.pop(0)
                        
                except IOError as e:
                    logger.warning(f"éŒ„éŸ³æ™‚ç™¼ç”ŸIOéŒ¯èª¤ï¼š{e}")
                    continue
                
                if len(buffer) >= window_frames:
                    segment_index += 1
                    audio_data = np.concatenate(buffer[:window_frames])
                    audio_tensor = self.process_audio(audio_data)
                    
                    if audio_tensor is not None:
                        logger.info(f"è™•ç†ç‰‡æ®µ {segment_index}")
                        future = self.executor.submit(
                            self.separate_and_save,
                            audio_tensor,
                            output_dir,
                            segment_index
                        )
                        self.futures.append(future)
                        self.processing_stats['segments_processed'] += 1
                    else:
                        self.processing_stats['segments_skipped'] += 1
                    
                    # ä¿ç•™é‡ç–Šéƒ¨åˆ†
                    buffer = buffer[slide_frames:]
                    
                    # å®šæœŸæ¸…ç†å·²å®Œæˆçš„ä»»å‹™
                    if segment_index % 10 == 0:
                        self.cleanup_futures()
                    
                    # æ¯30ç§’å ±å‘Šä¸€æ¬¡çµ±è¨ˆè³‡è¨Š
                    current_time = time.time()
                    if current_time - last_stats_time > 30:
                        self._log_statistics()
                        last_stats_time = current_time
                        
        except Exception as e:
            logger.error(f"éŒ„éŸ³éç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤ï¼š{e}")
        finally:
            self._cleanup_resources(p, stream, mixed_audio_buffer)

    def _cleanup_resources(self, p, stream, mixed_audio_buffer):
        """æ¸…ç†è³‡æº"""
        # åœæ­¢ä¸¦é—œé–‰éŸ³è¨Šæµ
        if stream is not None:
            try:
                stream.stop_stream()
                stream.close()
                logger.info("éŸ³è¨Šæµå·²é—œé–‰")
            except Exception as e:
                logger.error(f"é—œé–‰éŸ³è¨Šæµæ™‚ç™¼ç”ŸéŒ¯èª¤ï¼š{e}")
        
        if p is not None:
            try:
                p.terminate()
                logger.info("PyAudio å·²çµ‚æ­¢")
            except Exception as e:
                logger.error(f"çµ‚æ­¢ PyAudio æ™‚ç™¼ç”ŸéŒ¯èª¤ï¼š{e}")
        
        # ç­‰å¾…æ‰€æœ‰è™•ç†ä»»å‹™å®Œæˆ
        logger.info("ç­‰å¾…è™•ç†ä»»å‹™å®Œæˆ...")
        for future in self.futures:
            try:
                future.result(timeout=15.0)
            except Exception as e:
                logger.error(f"è™•ç†ä»»å‹™ç™¼ç”ŸéŒ¯èª¤ï¼š{e}")
        
        self.executor.shutdown(wait=True)
        logger.info("ç·šç¨‹æ± å·²é—œé–‰")
        
        # å„²å­˜åŸå§‹æ··åˆéŸ³è¨Š
        self._save_mixed_audio(mixed_audio_buffer)
        
        # è¨˜éŒ„æœ€çµ‚çµ±è¨ˆ
        self._log_final_statistics()
        
        # æ¸…ç†GPUè¨˜æ†¶é«”
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        
        logger.info("éŒ„éŸ³çµæŸï¼Œæ‰€æœ‰è³‡æºå·²æ¸…ç†")

    def _save_mixed_audio(self, mixed_audio_buffer):
        """å„²å­˜æ··åˆéŸ³è¨Š"""
        if not mixed_audio_buffer:
            return
            
        try:
            mixed_audio = np.concatenate(mixed_audio_buffer)
            mixed_audio = mixed_audio.reshape(-1, AUDIO_CHANNELS)
            
            timestamp = datetime.now().strftime('%Y%m%d-%H_%M_%S')
            mixed_output_file = os.path.join(
                "Audios_Storage",
                f"mixed_audio_{timestamp}.wav"
            )
            
            mixed_tensor = torch.from_numpy(mixed_audio).T.float()
            torchaudio.save(
                mixed_output_file,
                mixed_tensor,
                AUDIO_RECORDING_RATE
            )
            logger.info(f"å·²å„²å­˜åŸå§‹æ··åˆéŸ³è¨Šï¼š{mixed_output_file}")
            
        except Exception as e:
            logger.error(f"å„²å­˜æ··åˆéŸ³è¨Šæ™‚ç™¼ç”ŸéŒ¯èª¤ï¼š{e}")

    def _log_statistics(self):
        """è¨˜éŒ„çµ±è¨ˆè³‡è¨Š"""
        stats = self.processing_stats
        logger.info(f"çµ±è¨ˆ - å·²è™•ç†: {stats['segments_processed']}, "
                   f"å·²è·³é: {stats['segments_skipped']}, "
                   f"éŒ¯èª¤: {stats['errors']}, "
                   f"é€²è¡Œä¸­ä»»å‹™: {len(self.futures)}")

    def _log_final_statistics(self):
        """è¨˜éŒ„æœ€çµ‚çµ±è¨ˆè³‡è¨Š"""
        stats = self.processing_stats
        total = stats['segments_processed'] + stats['segments_skipped']
        if total > 0:
            success_rate = (stats['segments_processed'] / total) * 100
            logger.info(f"æœ€çµ‚çµ±è¨ˆ - ç¸½ç‰‡æ®µ: {total}, "
                       f"æˆåŠŸè™•ç†: {stats['segments_processed']} ({success_rate:.1f}%), "
                       f"è·³é: {stats['segments_skipped']}, "
                       f"éŒ¯èª¤: {stats['errors']}")

    def separate_and_save(self, audio_tensor, output_dir, segment_index):
        """åˆ†é›¢ä¸¦å„²å­˜éŸ³è¨Š"""
        try:
            with torch.no_grad():
                if len(audio_tensor.shape) == 2:
                    audio_tensor = audio_tensor.unsqueeze(0)
                
                separated = self.model(audio_tensor)
                
                if self.enable_noise_reduction:
                    enhanced_separated = self.enhance_separation(separated)
                else:
                    enhanced_separated = separated
                
                del separated
                if torch.cuda.is_available():
                    torch.cuda.empty_cache()
                
                timestamp = datetime.now().strftime('%Y%m%d-%H_%M_%S')
                
                # è™•ç†è¼¸å‡ºæ ¼å¼
                if len(enhanced_separated.shape) == 3:
                    if enhanced_separated.shape[1] == NUM_SPEAKERS_SEPARATION:
                        num_speakers = enhanced_separated.shape[1]
                        speaker_dim = 1
                    else:
                        num_speakers = enhanced_separated.shape[2]
                        speaker_dim = 2
                else:
                    num_speakers = 1
                    speaker_dim = 0
                
                saved_count = 0
                for i in range(min(num_speakers, NUM_SPEAKERS_SEPARATION)):
                    try:
                        if speaker_dim == 1:
                            speaker_audio = enhanced_separated[0, i, :].cpu()
                        elif speaker_dim == 2:
                            speaker_audio = enhanced_separated[0, :, i].cpu()
                        else:
                            speaker_audio = enhanced_separated.cpu().squeeze()
                        
                        # æ”¹å–„çš„æ­£è¦åŒ–è™•ç†
                        if len(speaker_audio.shape) > 1:
                            speaker_audio = speaker_audio.squeeze()
                        
                        # æª¢æŸ¥éŸ³è¨Šå“è³ª
                        rms = torch.sqrt(torch.mean(speaker_audio ** 2))
                        if rms > 0.01:  # åªä¿å­˜æœ‰æ„ç¾©çš„éŸ³è¨Š
                            # æº«å’Œçš„æ­£è¦åŒ–
                            max_val = torch.max(torch.abs(speaker_audio))
                            if max_val > 0:
                                # ä½¿ç”¨è»Ÿé™åˆ¶å™¨
                                normalized = speaker_audio / max_val
                                speaker_audio = torch.tanh(normalized * 0.9) * 0.85
                        
                            final_tensor = speaker_audio.unsqueeze(0)
                            
                            output_file = os.path.join(
                                output_dir,
                                f"speaker{i+1}_{timestamp}_{segment_index}.wav"
                            )
                            
                            torchaudio.save(
                                output_file,
                                final_tensor,
                                AUDIO_TARGET_RATE
                            )
                            saved_count += 1
                    except Exception as e:
                        logger.warning(f"å„²å­˜èªªè©±è€… {i+1} å¤±æ•—: {e}")
                
                if saved_count > 0:
                    logger.info(f"ç‰‡æ®µ {segment_index} å®Œæˆï¼Œå„²å­˜ {saved_count} å€‹æª”æ¡ˆ")
                
        except Exception as e:
            logger.error(f"è™•ç†ç‰‡æ®µ {segment_index} å¤±æ•—: {e}")
            self.processing_stats['errors'] += 1
        finally:
            if torch.cuda.is_available():
                torch.cuda.empty_cache()

    def stop_recording(self):
        """åœæ­¢éŒ„éŸ³"""
        self.is_recording = False
        logger.info("æº–å‚™åœæ­¢éŒ„éŸ³...")


def main():
    """ä¸»ç¨‹å¼"""
    try:
        logger.info("=== ConvTasNet 3äººèªéŸ³åˆ†é›¢ç³»çµ± ===")
        
        output_dir = "Audios_Storage"
        os.makedirs(output_dir, exist_ok=True)
        
        # æª¢æŸ¥ PyAudio
        try:
            p = pyaudio.PyAudio()
            device_count = p.get_device_count()
            p.terminate()
            logger.info(f"åµæ¸¬åˆ° {device_count} å€‹éŸ³è¨Šè¨­å‚™")
        except Exception as e:
            logger.error(f"PyAudio æª¢æŸ¥å¤±æ•—: {e}")
            return
        
        enable_noise_reduction = True
        snr_threshold = 8  # é™ä½é–¾å€¼ä»¥æä¾›æ›´å¤šè™•ç†
        
        separator_instance = AudioSeparator(
            enable_noise_reduction=enable_noise_reduction,
            snr_threshold=snr_threshold
        )
        
        logger.info("é–‹å§‹éŒ„éŸ³...")
        separator_instance.record_and_process(output_dir)
        
    except KeyboardInterrupt:
        logger.info("åœæ­¢éŒ„éŸ³")
        if 'separator_instance' in locals():
            separator_instance.stop_recording()
    except Exception as e:
        logger.error(f"ç¨‹å¼éŒ¯èª¤: {e}")
    finally:
        logger.info("ç¨‹å¼çµæŸ")

if __name__ == "__main__":
    main()