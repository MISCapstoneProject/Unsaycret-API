"""
===============================================================================
Voice_ID
å³æ™‚èªè€…åˆ†é›¢èˆ‡è­˜åˆ¥ç³»çµ± (Real-time Speech Separation and Speaker Identification System)
===============================================================================

ç‰ˆæœ¬ï¼šv2.2.0
ä½œè€…ï¼šEvanLo62, CYouuu
æœ€å¾Œæ›´æ–°ï¼š2025-07-09

åŠŸèƒ½æ‘˜è¦ï¼š
-----------
æœ¬ç³»çµ±å¯¦ç¾äº†å…ˆé€²çš„å³æ™‚èªéŸ³è™•ç†åŠŸèƒ½ï¼Œèƒ½å¤ åœ¨æ··åˆèªéŸ³ç’°å¢ƒä¸­å¯¦æ™‚å°‡ä¸åŒèªè€…çš„è²éŸ³åˆ†é›¢ï¼Œ
ä¸¦åˆ©ç”¨æ·±åº¦å­¸ç¿’æ¨¡å‹å°æ¯ä½èªè€…é€²è¡Œè¾¨è­˜èˆ‡è¿½è¹¤ã€‚ä¸»è¦ç‰¹é»åŒ…æ‹¬ï¼š

 1. å³æ™‚è™•ç†ï¼šé‚ŠéŒ„éŸ³é‚Šè™•ç†ï¼Œä¸éœ€ç­‰å¾…å®Œæ•´éŒ„éŸ³
 2. èªè€…åˆ†é›¢ï¼šèƒ½å¤ å°‡å¤šä½èªè€…çš„æ··åˆèªéŸ³åˆ†é›¢æˆç¨ç«‹çš„éŸ³æª”ï¼ˆæ”¯æ´æœ€å¤š3äººï¼‰
 3. å³æ™‚è­˜åˆ¥ï¼šåˆ†é›¢å¾Œç«‹å³é€²è¡Œèªè€…è­˜åˆ¥ï¼Œé¡¯ç¤ºå¯¦æ™‚è­˜åˆ¥çµæœ
 4. è²ç´‹æ›´æ–°ï¼šè‡ªå‹•æ›´æ–°èªè€…è²ç´‹å‘é‡ï¼Œæé«˜è­˜åˆ¥æº–ç¢ºç‡
 5. èªè€…ç®¡ç†ï¼šç¨ç«‹æ¨¡çµ„åŒ–çš„èªè€…èˆ‡è²ç´‹ç®¡ç†åŠŸèƒ½

** é‡è¦èªªæ˜ **ï¼šç›®å‰ä½¿ç”¨çš„èªè€…åˆ†é›¢æ¨¡å‹æ˜¯ ConvTasNet 3äººé è¨“ç·´æ¨¡å‹ï¼Œ
å› æ­¤æœ¬ç³»çµ±ä½¿ç”¨æ™‚å¯ä»¥åˆ†é›¢æœ€å¤šä¸‰å€‹èªè€…çš„æ··åˆèªéŸ³ã€‚
 
ç³»çµ±æ¨¡çµ„æ¶æ§‹ï¼š
-----------
 - speaker_system_v2.pyï¼šä¸»ç³»çµ±ï¼Œè² è²¬èªè€…åˆ†é›¢èˆ‡è­˜åˆ¥
 - main_identify_v5.pyï¼šèªè€…è­˜åˆ¥å¼•æ“ï¼Œè² è²¬è²ç´‹æ¯”å°
 - speaker_manager.pyï¼šèªè€…èˆ‡è²ç´‹ç®¡ç†æ¨¡çµ„

æŠ€è¡“æ¶æ§‹ï¼š
-----------
 - èªè€…åˆ†é›¢æ¨¡å‹: ConvTasNet (16kHz ä¸‰è²é“åˆ†é›¢)
 - èªè€…è­˜åˆ¥æ¨¡å‹: SpeechBrain ECAPA-TDNN æ¨¡å‹ (192ç¶­ç‰¹å¾µå‘é‡)
 - å‘é‡è³‡æ–™åº«: Weaviateï¼Œç”¨æ–¼å„²å­˜å’Œæª¢ç´¢èªè€…åµŒå…¥å‘é‡
 - å³æ™‚è™•ç†: å¤šåŸ·è¡Œç·’ä¸¦è¡Œè™•ç†ï¼Œé‚ŠéŒ„éŸ³é‚Šè­˜åˆ¥
 - éŸ³è¨Šå¢å¼·: é »è­œé–˜æ§é™å™ªã€ç¶­ç´æ¿¾æ³¢ã€å‹•æ…‹ç¯„åœå£“ç¸®ï¼Œæé«˜åˆ†é›¢å“è³ª

Weaviate è³‡æ–™åº«è¨­å®šï¼š
-----------
 - å®‰è£ä¸¦å•Ÿå‹• Weaviate å‘é‡è³‡æ–™åº«ï¼Œä½¿ç”¨docker-compose.ymlé…ç½®ï¼š
   ```
   docker-compose up -d
   ```
 - åŸ·è¡Œ `create_collections.py` å»ºç«‹å¿…è¦çš„2å€‹é›†åˆï¼š
   ```
   python create_collections.py
   ```
 - è‹¥è¦åŒ¯å…¥ç¾æœ‰èªè€…åµŒå…¥å‘é‡ï¼Œå¯åŸ·è¡Œï¼š
   ```
   python weaviate_studY/npy_to_weaviate.py
   ```

è™•ç†æµç¨‹ï¼š
-----------
 1. éŒ„éŸ³ï¼šé€£çºŒå¾éº¥å…‹é¢¨æ¥æ”¶éŸ³è¨Šæµ
 2. åˆ†å¡Šè™•ç†ï¼šæ¯6ç§’éŸ³è¨Š(å¯è‡ªè¨‚)ç‚ºä¸€å€‹è™•ç†å–®å…ƒï¼Œé‡ç–Šç‡50%
 3. åˆ†é›¢è™•ç†ï¼šå°‡æ¯æ®µæ··åˆéŸ³è¨Šåˆ†é›¢ç‚ºç¨ç«‹çš„è²éŸ³æµ
 4. å³æ™‚è­˜åˆ¥ï¼šå°æ¯ä½åˆ†é›¢å¾Œçš„èªè€…ç«‹å³é€²è¡Œè­˜åˆ¥
 5. é¡¯ç¤ºçµæœï¼šå³æ™‚é¡¯ç¤ºæ¯æ®µè­˜åˆ¥çµæœåŠè­˜åˆ¥å‹æ…‹

ä½¿ç”¨æ–¹å¼ï¼š
-----------
 1. ç›´æ¥é‹è¡Œä¸»ç¨‹å¼:
    ```
    python speaker_system_v2.py
    ```

 2. æŒ‰ä¸‹ Ctrl+C åœæ­¢éŒ„éŸ³å’Œè­˜åˆ¥

å‰ç½®éœ€æ±‚ï¼š
-----------
 - Python 3.9+
 - PyTorch with torchaudio
 - SpeechBrain
 - PyAudio (éŒ„éŸ³åŠŸèƒ½)
 - Weaviate å‘é‡è³‡æ–™åº« (éœ€é€šé Docker å•Ÿå‹•)
 - å…¶ä»–ä¾è³´å¥—ä»¶ (è¦‹ requirements.txt)

ç³»çµ±åƒæ•¸ï¼š
-----------
 - THRESHOLD_LOW = 0.26: éæ–¼ç›¸ä¼¼ï¼Œä¸æ›´æ–°å‘é‡
 - THRESHOLD_UPDATE = 0.34: ç›¸ä¼¼åº¦è¶³å¤ ï¼Œæ›´æ–°å‘é‡
 - THRESHOLD_NEW = 0.385: è¶…éæ­¤å€¼è¦–ç‚ºæ–°èªè€…
 - WINDOW_SIZE = 6: è™•ç†çª—å£å¤§å°ï¼ˆç§’ï¼‰
 - OVERLAP = 0.5: çª—å£é‡ç–Šç‡

è¼¸å‡ºçµæœï¼š
-----------
 - åˆ†é›¢å¾Œçš„éŸ³æª”: 16K-model/Audios-16K-IDTF/ ç›®éŒ„ä¸‹
 - æ··åˆéŸ³æª”: åŒç›®éŒ„ä¸‹ï¼Œå‰ç¶´ç‚º mixed_audio_
 - æ—¥èªŒæª”æ¡ˆ: system_output.log

è©³ç´°è³‡è¨Šï¼š
-----------
è«‹åƒè€ƒå°ˆæ¡ˆæ–‡ä»¶: https://github.com/LCY000/ProjectStudy_SpeechRecognition

===============================================================================
"""

import os

# ä¿®å¾© SVML éŒ¯èª¤ï¼šåœ¨å°å…¥ PyTorch ä¹‹å‰è¨­å®šç’°å¢ƒè®Šæ•¸
os.environ["MKL_DISABLE_FAST_MM"] = "1"
os.environ["KMP_DUPLICATE_LIB_OK"] = "TRUE"

import numpy as np
import torch
import torchaudio
import pyaudio # type: ignore
import logging
from datetime import datetime
from concurrent.futures import ThreadPoolExecutor
from typing import List, Dict, Optional, Tuple, Any, Union
from speechbrain.inference import SepformerSeparation as separator
from speechbrain.inference import SpeakerRecognition
import noisereduce as nr # type: ignore
import threading
import time
from scipy import signal
from scipy.ndimage import uniform_filter1d
from enum import Enum
from sklearn.cluster import DBSCAN # type: ignore
import librosa # type: ignore

# ä¿®æ”¹æ¨¡å‹è¼‰å…¥æ–¹å¼
try:
    from asteroid.models import ConvTasNet
    USE_ASTEROID = True
except ImportError:
    from transformers import AutoModel
    USE_ASTEROID = False

# å°å…¥æ—¥èªŒæ¨¡çµ„
from utils.logger import get_logger

# å°å…¥é…ç½® (ç’°å¢ƒè®Šæ•¸)
from utils.env_config import (
    AUDIO_RATE, MODELS_BASE_DIR, FORCE_CPU, CUDA_DEVICE_INDEX
)

# å°å…¥å¸¸æ•¸ (æ‡‰ç”¨ç¨‹å¼åƒæ•¸)  
from utils.constants import (
    THRESHOLD_LOW, THRESHOLD_UPDATE, THRESHOLD_NEW,
    DEFAULT_SEPARATION_MODEL, SPEECHBRAIN_SEPARATOR_MODEL,
    AUDIO_SAMPLE_RATE, AUDIO_CHUNK_SIZE, AUDIO_CHANNELS, 
    AUDIO_WINDOW_SIZE, AUDIO_OVERLAP, AUDIO_MIN_ENERGY_THRESHOLD, 
    AUDIO_MAX_BUFFER_MINUTES, API_MAX_WORKERS, AUDIO_TARGET_RATE
)

# å°å…¥ main_identify_v5 æ¨¡çµ„
from modules.identification import VID_identify_v5 as speaker_id

# æ–°å¢æ¨¡å‹é¡å‹æšèˆ‰
class SeparationModel(Enum):
    CONVTASNET_3SPEAKER = "convtasnet_3speaker"  # ConvTasNet 3äººæ¨¡å‹
    SEPFORMER_2SPEAKER = "sepformer_2speaker"    # SepFormer 2äººæ¨¡å‹

# æ¨¡å‹é…ç½®
MODEL_CONFIGS = {
    SeparationModel.CONVTASNET_3SPEAKER: {
        "model_name": "JorisCos/ConvTasNet_Libri3Mix_sepnoisy_16k",
        "num_speakers": 3,
        "sample_rate": AUDIO_SAMPLE_RATE,
        "use_speechbrain": False
    },
    SeparationModel.SEPFORMER_2SPEAKER: {
        "model_name": SPEECHBRAIN_SEPARATOR_MODEL,
        "num_speakers": 2,
        "sample_rate": AUDIO_SAMPLE_RATE,
        "use_speechbrain": True
    }
}

# åŸºæœ¬éŒ„éŸ³åƒæ•¸ï¼ˆå¾é…ç½®è®€å–ï¼‰
CHUNK = AUDIO_CHUNK_SIZE
FORMAT = pyaudio.paFloat32
CHANNELS = AUDIO_CHANNELS
RATE = AUDIO_RATE
TARGET_RATE = AUDIO_TARGET_RATE
WINDOW_SIZE = AUDIO_WINDOW_SIZE
OVERLAP = AUDIO_OVERLAP
DEVICE_INDEX = 2

# è™•ç†åƒæ•¸ï¼ˆå¾é…ç½®è®€å–ï¼‰
MIN_ENERGY_THRESHOLD = AUDIO_MIN_ENERGY_THRESHOLD
MAX_BUFFER_MINUTES = AUDIO_MAX_BUFFER_MINUTES

# éŸ³è¨Šè™•ç†åƒæ•¸
MIN_ENERGY_THRESHOLD = 0.001
NOISE_REDUCE_STRENGTH = 0.05  # é™ä½é™å™ªå¼·åº¦ä»¥ä¿æŒéŸ³è³ª
MAX_BUFFER_MINUTES = 5
SNR_THRESHOLD = 8  # é™ä½ SNR é–¾å€¼

# éŸ³è¨Šå“è³ªæ”¹å–„åƒæ•¸
WIENER_FILTER_STRENGTH = 0.01  # æ›´æº«å’Œçš„ç¶­ç´æ¿¾æ³¢
HIGH_FREQ_CUTOFF = 7500  # æé«˜é«˜é »æˆªæ­¢é»
DYNAMIC_RANGE_COMPRESSION = 0.7  # å‹•æ…‹ç¯„åœå£“ç¸®

# ConvTasNet æ¨¡å‹åƒæ•¸ (ä½¿ç”¨å¸¸æ•¸é…ç½®)
DEFAULT_MODEL = DEFAULT_SEPARATION_MODEL
# ä¿®æ­£ DEFAULT_MODEL çš„è³¦å€¼
if DEFAULT_SEPARATION_MODEL == "sepformer_2speaker":
    DEFAULT_MODEL = SeparationModel.SEPFORMER_2SPEAKER
elif DEFAULT_SEPARATION_MODEL == "convtasnet_3speaker":
    DEFAULT_MODEL = SeparationModel.CONVTASNET_3SPEAKER
else:
    DEFAULT_MODEL = SeparationModel.SEPFORMER_2SPEAKER  # é è¨­å€¼

MODEL_NAME = MODEL_CONFIGS[DEFAULT_MODEL]["model_name"]
NUM_SPEAKERS = MODEL_CONFIGS[DEFAULT_MODEL]["num_speakers"]

# æ–°å¢: ä¾¿åˆ©çš„æ¨¡å‹é¸æ“‡å‡½å¼
def set_default_model(model_type: SeparationModel):
    """è¨­å®šé è¨­æ¨¡å‹é¡å‹"""
    global DEFAULT_MODEL, MODEL_NAME, NUM_SPEAKERS
    DEFAULT_MODEL = model_type
    MODEL_NAME = MODEL_CONFIGS[model_type]["model_name"]
    NUM_SPEAKERS = MODEL_CONFIGS[model_type]["num_speakers"]
    logger.info(f"é è¨­æ¨¡å‹å·²è¨­å®šç‚º: {model_type.value}")

def get_available_models():
    """å–å¾—å¯ç”¨çš„æ¨¡å‹åˆ—è¡¨"""
    return {
        "convtasnet_3speaker": "ConvTasNet 3äººèªè€…åˆ†é›¢æ¨¡å‹",
        "sepformer_2speaker": "SepFormer 2äººèªè€…åˆ†é›¢æ¨¡å‹"
    }

def create_separator(model_name: str = None, **kwargs):
    """
    å»ºç«‹ AudioSeparator å¯¦ä¾‹çš„ä¾¿åˆ©å‡½å¼
    
    Args:
        model_name: æ¨¡å‹åç¨± ("convtasnet_3speaker" æˆ– "sepformer_2speaker")
        **kwargs: å…¶ä»–åƒæ•¸å‚³éçµ¦ AudioSeparator
    
    Returns:
        AudioSeparator å¯¦ä¾‹
    """
    if model_name:
        if model_name == "convtasnet_3speaker":
            model_type = SeparationModel.CONVTASNET_3SPEAKER
        elif model_name == "sepformer_2speaker":
            model_type = SeparationModel.SEPFORMER_2SPEAKER
        else:
            raise ValueError(f"ä¸æ”¯æ´çš„æ¨¡å‹: {model_name}ã€‚å¯ç”¨æ¨¡å‹: {list(get_available_models().keys())}")
    else:
        model_type = DEFAULT_MODEL
    
    return AudioSeparator(model_type=model_type, **kwargs)

# å…¨åŸŸåƒæ•¸è¨­å®šï¼Œä½¿ç”¨ v5 ç‰ˆæœ¬çš„é–¾å€¼
EMBEDDING_DIR = "embeddingFiles"  # æ‰€æœ‰èªè€…åµŒå…¥è³‡æ–™çš„æ ¹ç›®éŒ„
THRESHOLD_LOW = speaker_id.THRESHOLD_LOW     # éæ–¼ç›¸ä¼¼ï¼Œä¸æ›´æ–°
THRESHOLD_UPDATE = speaker_id.THRESHOLD_UPDATE # æ›´æ–°åµŒå…¥å‘é‡
THRESHOLD_NEW = speaker_id.THRESHOLD_NEW    # åˆ¤å®šç‚ºæ–°èªè€…

# è¼¸å‡ºç›®éŒ„
OUTPUT_DIR = "R3SI/Audio-storage"  # å„²å­˜åˆ†é›¢å¾ŒéŸ³è¨Šçš„ç›®éŒ„
IDENTIFIED_DIR = "R3SI/Identified-Speakers"

# åˆå§‹åŒ–æ—¥èªŒç³»çµ±
logger = get_logger(__name__)


# ================== èªè€…åˆ†é›¢éƒ¨åˆ† ======================

class AudioSeparator:
    def __init__(self, model_type: SeparationModel = DEFAULT_MODEL, enable_noise_reduction=True, snr_threshold=SNR_THRESHOLD):
        # è¨­å‚™é¸æ“‡é‚è¼¯ï¼šå„ªå…ˆè€ƒæ…® FORCE_CPU è¨­å®š
        if FORCE_CPU:
            self.device = "cpu"
            logger.info("ğŸ”§ FORCE_CPU=trueï¼Œå¼·åˆ¶ä½¿ç”¨ CPU é‹ç®—")
        else:
            if torch.cuda.is_available():
                # æª¢æŸ¥æŒ‡å®šçš„ CUDA è¨­å‚™æ˜¯å¦å­˜åœ¨
                if CUDA_DEVICE_INDEX < torch.cuda.device_count():
                    self.device = f"cuda:{CUDA_DEVICE_INDEX}"
                    # ç¢ºä¿è¨­å®šæ­£ç¢ºçš„è¨­å‚™
                    torch.cuda.set_device(CUDA_DEVICE_INDEX)
                    if CUDA_DEVICE_INDEX != 0:
                        logger.info(f"ğŸ¯ ä½¿ç”¨æŒ‡å®šçš„ CUDA è¨­å‚™: {CUDA_DEVICE_INDEX}")
                else:
                    logger.warning(f"âš ï¸  æŒ‡å®šçš„ CUDA è¨­å‚™ç´¢å¼• {CUDA_DEVICE_INDEX} ä¸å­˜åœ¨ï¼Œæ”¹ç”¨ cuda:0")
                    self.device = "cuda:0"
                    torch.cuda.set_device(0)
            else:
                self.device = "cpu"
                logger.info("ğŸ–¥ï¸  æœªåµæ¸¬åˆ° GPU è¨­å‚™ï¼Œä½¿ç”¨ CPU é‹ç®—")
                
        self.model_type = model_type
        self.model_config = MODEL_CONFIGS[model_type]
        self.num_speakers = self.model_config["num_speakers"]
        self.enable_noise_reduction = enable_noise_reduction
        self.snr_threshold = snr_threshold
        
        # èªè€…åµæ¸¬ç›¸é—œåƒæ•¸ - é€²ä¸€æ­¥é™ä½ç‚ºæ›´éˆæ•çš„è¨­å®š
        self.vad_threshold = 0.1  # é€²ä¸€æ­¥é™ä½èªéŸ³æ´»å‹•æª¢æ¸¬é–¾å€¼ï¼ˆåŸ0.08ï¼‰
        self.speaker_energy_threshold = 0.1  # é€²ä¸€æ­¥é™ä½èªªè©±è€…èƒ½é‡é–¾å€¼ï¼ˆåŸ0.15ï¼‰
        self.silence_threshold = 0.002  # é€²ä¸€æ­¥é™ä½éœéŸ³æª¢æ¸¬é–¾å€¼ï¼ˆåŸ0.002ï¼‰
        self.min_speech_duration = 0.8  # é€²ä¸€æ­¥é™ä½æœ€å°èªéŸ³æŒçºŒæ™‚é–“ï¼ˆåŸ0.3ç§’ï¼‰

        logger.info(f"ä½¿ç”¨è¨­å‚™: {self.device}")
        logger.info(f"æ¨¡å‹é¡å‹: {model_type.value}")
        logger.info(f"è¼‰å…¥æ¨¡å‹: {self.model_config['model_name']}")
        logger.info(f"æ”¯æ´èªè€…æ•¸é‡: {self.num_speakers}")
        
        # è¨­è¨ˆæ›´æº«å’Œçš„ä½é€šæ¿¾æ³¢å™¨
        nyquist = TARGET_RATE // 2
        cutoff = min(HIGH_FREQ_CUTOFF, nyquist - 100)
        self.lowpass_filter = signal.butter(2, cutoff / nyquist, btype='low', output='sos')
        
        try:
            logger.info("æ­£åœ¨è¼‰å…¥æ¨¡å‹...")
            self.model = self._load_model()
            logger.info("æ¨¡å‹è¼‰å…¥å®Œæˆ")
            self._test_model()
        except Exception as e:
            logger.error(f"æ¨¡å‹è¼‰å…¥å¤±æ•—: {e}")
            raise
        
        try:
            self.resampler = torchaudio.transforms.Resample(
                orig_freq=RATE,
                new_freq=TARGET_RATE
            ).to(self.device)
        except Exception as e:
            logger.error(f"é‡æ–°å–æ¨£å™¨åˆå§‹åŒ–å¤±æ•—: {e}")
            raise
        
        self.executor = ThreadPoolExecutor(max_workers=API_MAX_WORKERS)
        self.futures = []
        self.is_recording = False
        self.output_files = []  # å„²å­˜åˆ†é›¢å¾Œçš„éŸ³æª”è·¯å¾‘
        self.save_audio_files = True  # è¨­å®š: æ˜¯å¦å°‡åˆ†é›¢å¾Œçš„éŸ³è¨Šå„²å­˜ç‚ºwavæª”æ¡ˆ
        
        # è™•ç†çµ±è¨ˆ
        self.processing_stats = {
            'segments_processed': 0,
            'segments_skipped': 0,
            'errors': 0
        }
        
        self.max_buffer_size = int(RATE * MAX_BUFFER_MINUTES * 60 / CHUNK)
        logger.info("AudioSeparator åˆå§‹åŒ–å®Œæˆ")

    def _load_model(self):
        """è¼‰å…¥èªè€…åˆ†é›¢æ¨¡å‹"""
        model_name = self.model_config["model_name"]
        
        if self.model_config["use_speechbrain"]:
            # ä½¿ç”¨ SpeechBrain SepFormer æ¨¡å‹
            try:
                logger.info(f"è¼‰å…¥ SpeechBrain æ¨¡å‹: {model_name}")
                
                # æª¢æŸ¥æœ¬åœ°æ¨¡å‹ç›®éŒ„æ˜¯å¦åŒ…å«ç„¡æ•ˆçš„ç¬¦è™Ÿé€£çµ
                local_model_path = os.path.abspath(f"models/{self.model_type.value}")
                if os.path.exists(local_model_path):
                    logger.info(f"æª¢æŸ¥æœ¬åœ°æ¨¡å‹è·¯å¾‘: {local_model_path}")
                    
                    # æª¢æŸ¥æ˜¯å¦æœ‰ç„¡æ•ˆçš„ç¬¦è™Ÿé€£çµ (Windows JUNCTION æŒ‡å‘ Linux è·¯å¾‘)
                    hyperparams_file = os.path.join(local_model_path, "hyperparams.yaml")
                    if os.path.exists(hyperparams_file):
                        try:
                            # æ¸¬è©¦æª”æ¡ˆè®€å–æ¬Šé™
                            with open(hyperparams_file, 'r', encoding='utf-8') as f:
                                content = f.read(100)  # è®€å–å‰100å€‹å­—ç¬¦ä¾†æ¸¬è©¦
                            logger.info("æœ¬åœ°æ¨¡å‹æª”æ¡ˆè®€å–æ­£å¸¸")
                        except (PermissionError, OSError, UnicodeDecodeError) as e:
                            logger.warning(f"æœ¬åœ°æ¨¡å‹æª”æ¡ˆç„¡æ³•è®€å–: {e}")
                            logger.info("åµæ¸¬åˆ°ç„¡æ•ˆçš„ç¬¦è™Ÿé€£çµï¼Œæº–å‚™é‡æ–°ä¸‹è¼‰æ¨¡å‹...")
                            
                            # åˆªé™¤åŒ…å«ç„¡æ•ˆç¬¦è™Ÿé€£çµçš„ç›®éŒ„
                            try:
                                import shutil
                                shutil.rmtree(local_model_path, ignore_errors=True)
                                logger.info(f"å·²åˆªé™¤ç„¡æ•ˆçš„æ¨¡å‹ç›®éŒ„: {local_model_path}")
                            except Exception as rm_error:
                                logger.warning(f"åˆªé™¤æ¨¡å‹ç›®éŒ„æ™‚ç™¼ç”ŸéŒ¯èª¤: {rm_error}")
                
                # å˜—è©¦è¼‰å…¥æ¨¡å‹ (å¦‚æœæœ¬åœ°æª”æ¡ˆç„¡æ•ˆï¼ŒSpeechBrain æœƒè‡ªå‹•é‡æ–°ä¸‹è¼‰)
                model = separator.from_hparams(
                    source=model_name,
                    savedir=os.path.abspath(f"models/{self.model_type.value}"),
                    run_opts={"device": self.device}
                )
                logger.info("SpeechBrain æ¨¡å‹è¼‰å…¥æˆåŠŸ")
                return model
                
            except Exception as e:
                logger.error(f"SpeechBrain æ¨¡å‹è¼‰å…¥å¤±æ•—: {e}")
                
                # æœ€å¾Œå˜—è©¦ï¼šå¼·åˆ¶é‡æ–°ä¸‹è¼‰
                try:
                    logger.info("å˜—è©¦å¼·åˆ¶é‡æ–°ä¸‹è¼‰æ¨¡å‹...")
                    import shutil
                    local_model_path = os.path.abspath(f"models/{self.model_type.value}")
                    if os.path.exists(local_model_path):
                        shutil.rmtree(local_model_path, ignore_errors=True)
                        logger.info("å·²æ¸…é™¤æœ¬åœ°æ¨¡å‹å¿«å–")
                    
                    model = separator.from_hparams(
                        source=model_name,
                        savedir=os.path.abspath(f"models/{self.model_type.value}"),
                        run_opts={"device": self.device}
                    )
                    logger.info("å¼·åˆ¶é‡æ–°ä¸‹è¼‰å¾Œï¼Œæ¨¡å‹è¼‰å…¥æˆåŠŸ")
                    return model
                    
                except Exception as final_error:
                    logger.error(f"æ‰€æœ‰è¼‰å…¥å˜—è©¦å‡å¤±æ•—: {final_error}")
                    raise Exception(f"æ¨¡å‹è¼‰å…¥å®Œå…¨å¤±æ•—ã€‚è«‹æª¢æŸ¥ç¶²è·¯é€£ç·šå’Œæ¨¡å‹å¯ç”¨æ€§ã€‚åŸå§‹éŒ¯èª¤: {e}")
        else:
            # ä½¿ç”¨ ConvTasNet æ¨¡å‹ï¼ˆåŸæœ‰é‚è¼¯ï¼‰
            if USE_ASTEROID:
                try:
                    model = ConvTasNet.from_pretrained(model_name)
                    model = model.to(self.device)
                    model.eval()
                    return model
                except Exception as e:
                    logger.warning(f"Asteroid è¼‰å…¥å¤±æ•—ï¼Œå˜—è©¦å…¶ä»–è¼‰å…¥æ–¹å¼...")
            
            try:
                model = AutoModel.from_pretrained(
                    model_name,
                    trust_remote_code=True
                )
                model = model.to(self.device)
                model.eval()
                return model
            except Exception as e:
                logger.info("å˜—è©¦ä½¿ç”¨ torch.hub è¼‰å…¥...")
                return self._manual_load_model()

    def _manual_load_model(self):
        """æ‰‹å‹•è¼‰å…¥ ConvTasNet æ¨¡å‹"""
        try:
            model_dir = f"models/{self.model_type.value}"
            os.makedirs(model_dir, exist_ok=True)
            model = torch.hub.load('JorisCos/ConvTasNet', 'ConvTasNet_Libri3Mix_sepnoisy_16k', pretrained=True)
            model = model.to(self.device)
            model.eval()
            return model
        except Exception as e:
            logger.error(f"æ‰€æœ‰è¼‰å…¥æ–¹å¼å‡å¤±æ•—: {e}")
            raise

    def _test_model(self):
        """æ¸¬è©¦æ¨¡å‹"""
        try:
            with torch.no_grad():
                if self.model_config["use_speechbrain"]:
                    # SpeechBrain SepFormer æ¨¡å‹æ¸¬è©¦
                    # SepFormer æœŸæœ›è¼¸å…¥æ ¼å¼ç‚º [batch, samples]
                    test_audio = torch.randn(1, AUDIO_SAMPLE_RATE).to(self.device)
                    logger.debug(f"SpeechBrain æ¸¬è©¦éŸ³è¨Šå½¢ç‹€: {test_audio.shape}")
                    output = self.model.separate_batch(test_audio)
                else:
                    # ConvTasNet æ¨¡å‹æ¸¬è©¦
                    # ConvTasNet æœŸæœ›è¼¸å…¥æ ¼å¼ç‚º [batch, channels, samples]
                    test_audio = torch.randn(1, 1, AUDIO_SAMPLE_RATE).to(self.device)
                    logger.debug(f"ConvTasNet æ¸¬è©¦éŸ³è¨Šå½¢ç‹€: {test_audio.shape}")
                    output = self.model(test_audio)
                    
            logger.info("æ¨¡å‹æ¸¬è©¦é€šé")
            logger.debug(f"è¼¸å‡ºå½¢ç‹€: {output.shape if hasattr(output, 'shape') else type(output)}")
            
        except Exception as e:
            logger.error(f"æ¨¡å‹æ¸¬è©¦å¤±æ•—: {e}")
            logger.error(f"æ¸¬è©¦éŸ³è¨Šå½¢ç‹€: {test_audio.shape if 'test_audio' in locals() else 'N/A'}")
            raise

    def estimate_snr(self, signal):
        """ä¼°ç®—ä¿¡è™Ÿé›œè¨Šæ¯”"""
        try:
            signal_power = np.mean(signal ** 2)
            if len(signal) > 1000:
                noise_estimate = np.std(signal[-1000:]) ** 2
            else:
                noise_estimate = np.std(signal) ** 2 * 0.1
            noise_estimate = max(noise_estimate, 1e-10)
            snr = 10 * np.log10(signal_power / noise_estimate)
            return snr
        except:
            return 0

    def wiener_filter(self, audio_signal):
        """ç¶­ç´æ¿¾æ³¢å™¨ - æ›´æº«å’Œçš„è™•ç†"""
        try:
            f, t, stft = signal.stft(audio_signal, fs=TARGET_RATE, nperseg=512, noverlap=256)
            
            # ä½¿ç”¨æ›´æº«å’Œçš„é›œè¨Šä¼°è¨ˆ
            quiet_samples = min(int(TARGET_RATE * 0.05), len(audio_signal) // 8)
            noise_sample = audio_signal[:quiet_samples]
            _, _, noise_stft = signal.stft(noise_sample, fs=TARGET_RATE, nperseg=512, noverlap=256)
            noise_power = np.mean(np.abs(noise_stft) ** 2, axis=1, keepdims=True)
            
            signal_power = np.abs(stft) ** 2
            wiener_gain = signal_power / (signal_power + WIENER_FILTER_STRENGTH * noise_power)
            
            # é™åˆ¶å¢ç›Šç¯„åœä»¥é¿å…éåº¦è™•ç†
            wiener_gain = np.clip(wiener_gain, 0.1, 1.0)
            
            filtered_stft = stft * wiener_gain
            _, filtered_audio = signal.istft(filtered_stft, fs=TARGET_RATE)
            
            return filtered_audio[:len(audio_signal)]
        except:
            return audio_signal

    def smooth_audio(self, audio_signal):
        """éŸ³è¨Šå¹³æ»‘è™•ç†"""
        try:
            # ç§»é™¤çªç„¶çš„è·³ç–Š
            diff = np.diff(audio_signal)
            threshold = np.std(diff) * 3  # æ›´å¯¬é¬†çš„é–¾å€¼
            artifact_indices = np.where(np.abs(diff) > threshold)[0]
            
            for idx in artifact_indices[:20]:  # é™åˆ¶è™•ç†æ•¸é‡
                if 0 < idx < len(audio_signal) - 1:
                    audio_signal[idx] = (audio_signal[idx-1] + audio_signal[idx+1]) / 2
            
            # è¼•å¾®å¹³æ»‘
            audio_signal = uniform_filter1d(audio_signal, size=3)
            
            # è¼•å¾®ä½é€šæ¿¾æ³¢
            audio_signal = signal.sosfilt(self.lowpass_filter, audio_signal)
            
            return audio_signal
        except:
            return audio_signal

    def dynamic_range_compression(self, audio_signal):
        """å‹•æ…‹ç¯„åœå£“ç¸®"""
        try:
            # è»Ÿé™åˆ¶å™¨
            threshold = 0.8
            ratio = DYNAMIC_RANGE_COMPRESSION
            
            # è¨ˆç®—çµ•å°å€¼
            abs_signal = np.abs(audio_signal)
            
            # å°è¶…éé–¾å€¼çš„éƒ¨åˆ†é€²è¡Œå£“ç¸®
            mask = abs_signal > threshold
            compressed = np.copy(audio_signal)
            
            if np.any(mask):
                over_threshold = abs_signal[mask]
                compressed_magnitude = threshold + (over_threshold - threshold) * ratio
                compressed[mask] = np.sign(audio_signal[mask]) * compressed_magnitude
            
            return compressed
        except:
            return audio_signal

    def spectral_gating(self, audio):
        """æ”¹è‰¯çš„é »è­œé–˜æ§é™å™ª"""
        try:
            noise_sample_length = max(int(TARGET_RATE * 0.05), 1)
            noise_sample = audio[:noise_sample_length]
            
            return nr.reduce_noise(
                y=audio,
                y_noise=noise_sample,
                sr=TARGET_RATE,
                prop_decrease=NOISE_REDUCE_STRENGTH,
                stationary=False,  # éç©©æ…‹é›œè¨Šè™•ç†
                n_jobs=1
            )
        except:
            return audio

    def enhance_separation(self, separated_signals):
        """å¢å¼·åˆ†é›¢æ•ˆæœ - æ”¹å–„éŸ³è³ª"""
        if not self.enable_noise_reduction:
            return separated_signals
        
        # è™•ç†å½¢ç‹€ - æ ¹æ“šä¸åŒæ¨¡å‹èª¿æ•´
        if self.model_config["use_speechbrain"]:
            # SpeechBrain æ¨¡å‹è¼¸å‡ºæ ¼å¼è™•ç†
            if len(separated_signals.shape) == 3:
                # æ ¼å¼é€šå¸¸ç‚º [batch, time, speakers]
                enhanced_signals = torch.zeros_like(separated_signals)
                speaker_dim = 2
                time_dim = 1
            else:
                separated_signals = separated_signals.unsqueeze(0)
                enhanced_signals = torch.zeros_like(separated_signals)
                speaker_dim = 2
                time_dim = 1
        else:
            # ConvTasNet æ¨¡å‹è¼¸å‡ºæ ¼å¼è™•ç†ï¼ˆåŸæœ‰é‚è¼¯ï¼‰
            if len(separated_signals.shape) == 3:
                if separated_signals.shape[1] == self.num_speakers:
                    enhanced_signals = torch.zeros_like(separated_signals)
                    speaker_dim = 1
                    time_dim = 2
                elif separated_signals.shape[2] == self.num_speakers:
                    separated_signals = separated_signals.transpose(1, 2)
                    enhanced_signals = torch.zeros_like(separated_signals)
                    speaker_dim = 1
                    time_dim = 2
                else:
                    enhanced_signals = torch.zeros_like(separated_signals)
                    speaker_dim = 2
                    time_dim = 1
            else:
                if separated_signals.shape[0] == self.num_speakers:
                    separated_signals = separated_signals.unsqueeze(0)
                    enhanced_signals = torch.zeros_like(separated_signals)
                    speaker_dim = 1
                    time_dim = 2
                else:
                    separated_signals = separated_signals.unsqueeze(0).transpose(1, 2)
                    enhanced_signals = torch.zeros_like(separated_signals)
                    speaker_dim = 1
                    time_dim = 2
        
        num_speakers = separated_signals.shape[speaker_dim]
        
        for i in range(min(num_speakers, self.num_speakers)):
            if speaker_dim == 1:
                current_signal = separated_signals[0, i, :].cpu().numpy()
            elif speaker_dim == 2:
                current_signal = separated_signals[0, :, i].cpu().numpy()
            else:
                current_signal = separated_signals[0, :, i].cpu().numpy()
            
            # å¤šéšæ®µéŸ³è³ªæ”¹å–„
            processed_signal = current_signal
            
            # 1. ç¶­ç´æ¿¾æ³¢
            signal_snr = self.estimate_snr(current_signal)
            if signal_snr < self.snr_threshold + 3:
                processed_signal = self.wiener_filter(processed_signal)
            
            # 2. å‚³çµ±é™å™ªï¼ˆåƒ…åœ¨å¿…è¦æ™‚ï¼‰
            if signal_snr < self.snr_threshold:
                processed_signal = self.spectral_gating(processed_signal)
            
            # 3. éŸ³è¨Šå¹³æ»‘å’Œä¿®å¾©
            processed_signal = self.smooth_audio(processed_signal)
            
            # 4. å‹•æ…‹ç¯„åœå£“ç¸®
            processed_signal = self.dynamic_range_compression(processed_signal)
            
            # 5. æœ€çµ‚æ­£è¦åŒ–
            max_val = np.max(np.abs(processed_signal))
            if max_val > 0:
                processed_signal = processed_signal / max_val * 0.95
            
            length = min(len(processed_signal), separated_signals.shape[time_dim])
            
            if speaker_dim == 1:
                enhanced_signals[0, i, :length] = torch.from_numpy(processed_signal[:length]).to(self.device)
            elif speaker_dim == 2:
                enhanced_signals[0, :length, i] = torch.from_numpy(processed_signal[:length]).to(self.device)
            else:
                enhanced_signals[0, :length, i] = torch.from_numpy(processed_signal[:length]).to(self.device)
        
        return enhanced_signals
        
    def set_save_audio_files(self, save: bool) -> None:
        """
        è¨­å®šæ˜¯å¦å„²å­˜åˆ†é›¢å¾Œçš„éŸ³è¨Šæª”æ¡ˆ
        
        Args:
            save: True è¡¨ç¤ºå„²å­˜éŸ³è¨Šæª”æ¡ˆï¼ŒFalse è¡¨ç¤ºä¸å„²å­˜
        """
        self.save_audio_files = save
        logger.info(f"éŸ³è¨Šæª”æ¡ˆå„²å­˜è¨­å®šï¼š{'å·²å•Ÿç”¨' if save else 'å·²åœç”¨'}")

    def process_audio(self, audio_data: np.ndarray) -> torch.Tensor:
        """è™•ç†éŸ³è¨Šæ ¼å¼ï¼šå°‡åŸå§‹éŒ„éŸ³è³‡æ–™è½‰æ›ç‚ºæ¨¡å‹å¯ç”¨çš„æ ¼å¼"""
        try:
            # è½‰æ›ç‚º float32
            if FORMAT == pyaudio.paInt16:
                audio_float = audio_data.astype(np.float32) / 32768.0
            else:
                audio_float = audio_data.astype(np.float32)
            
            # èƒ½é‡æª¢æ¸¬ï¼šéä½å‰‡ç•¥é
            energy = np.mean(np.abs(audio_float))
            if energy < MIN_ENERGY_THRESHOLD:
                logger.debug(f"éŸ³è¨Šèƒ½é‡ ({energy:.6f}) ä½æ–¼é–¾å€¼ ({MIN_ENERGY_THRESHOLD})")
                return None
            
            # é‡å¡‘ç‚ºæ­£ç¢ºå½¢ç‹€
            if len(audio_float.shape) == 1:
                audio_float = audio_float.reshape(-1, CHANNELS)

            # èª¿æ•´å½¢ç‹€ä»¥ç¬¦åˆæ¨¡å‹è¼¸å…¥ï¼š[channels, time]
            audio_tensor = torch.from_numpy(audio_float).T.float()

            # å¦‚æœæ˜¯é›™è²é“è€Œæ¨¡å‹åªæ”¯æ´å–®è²é“å‰‡å–å¹³å‡
            if audio_tensor.shape[0] == 2:
                audio_tensor = torch.mean(audio_tensor, dim=0, keepdim=True)

            # ç§»è‡³ GPU ä¸¦é‡æ–°å–æ¨£è‡³ 16kHz
            audio_tensor = audio_tensor.to(self.device)
            resampled = self.resampler(audio_tensor)
            
            # ç¢ºä¿å½¢ç‹€æ­£ç¢º
            if len(resampled.shape) == 1:
                resampled = resampled.unsqueeze(0)
            
            return resampled
            
        except Exception as e:
            logger.error(f"éŸ³è¨Šè™•ç†éŒ¯èª¤ï¼š{e}")
            self.processing_stats['errors'] += 1
            return None

    def cleanup_futures(self):
        """æ¸…ç†å·²å®Œæˆçš„ä»»å‹™"""
        completed_futures = []
        for future in self.futures:
            if future.done():
                try:
                    future.result()  # ç²å–çµæœä»¥æ•ç²ä»»ä½•ç•°å¸¸
                except Exception as e:
                    logger.error(f"è™•ç†ä»»å‹™ç™¼ç”ŸéŒ¯èª¤ï¼š{e}")
                    self.processing_stats['errors'] += 1
                completed_futures.append(future)
        
        # ç§»é™¤å·²å®Œæˆçš„ä»»å‹™
        for future in completed_futures:
            self.futures.remove(future)

    def record_and_process(self, output_dir):
        """éŒ„éŸ³ä¸¦è™•ç†éŸ³è¨Šçš„ä¸»è¦æ–¹æ³•"""
        # ç¢ºä¿è¼¸å‡ºç›®éŒ„å­˜åœ¨
        os.makedirs(output_dir, exist_ok=True)
        # å„²å­˜åŸå§‹æ··åˆéŸ³è¨Šçš„ç·©è¡å€
        mixed_audio_buffer = []
        
        try:
            # æ­¥é©Ÿ0: åˆå§‹åŒ–èªè€…è­˜åˆ¥å™¨
            identifier = SpeakerIdentifier()

            # æ­¥é©Ÿ1: åˆå§‹åŒ–éŒ„éŸ³è£ç½®
            p = pyaudio.PyAudio()
            
            # æª¢æŸ¥è¨­å‚™å¯ç”¨æ€§
            if DEVICE_INDEX is not None:
                device_info = p.get_device_info_by_index(DEVICE_INDEX)
                logger.info(f"ä½¿ç”¨éŸ³è¨Šè¨­å‚™: {device_info['name']}")
            
            stream = p.open(
                format=FORMAT,
                channels=CHANNELS,
                rate=RATE,
                input=True,
                frames_per_buffer=CHUNK,
                input_device_index=DEVICE_INDEX
            )
            
            logger.info("é–‹å§‹éŒ„éŸ³...")
            
            # æ­¥é©Ÿ2: è¨ˆç®—ç·©è¡å€åƒæ•¸
            samples_per_window = int(WINDOW_SIZE * RATE)
            window_frames = int(samples_per_window / CHUNK)
            overlap_frames = int((OVERLAP * RATE) / CHUNK)
            slide_frames = window_frames - overlap_frames
            
            # åˆå§‹åŒ–è™•ç†è®Šæ•¸
            buffer = []
            segment_index = 0
            self.is_recording = True
            last_stats_time = time.time()
            
            # æ­¥é©Ÿ3: éŒ„éŸ³ä¸»å¾ªç’°
            while self.is_recording:
                try:
                    data = stream.read(CHUNK, exception_on_overflow=False)
                    frame = np.frombuffer(data, dtype=np.float32 if FORMAT == pyaudio.paFloat32 else np.int16)
                    buffer.append(frame)
                    
                    # é™åˆ¶ mixed_audio_buffer å¤§å°ä»¥é˜²æ­¢è¨˜æ†¶é«”è€—ç›¡
                    mixed_audio_buffer.append(frame.copy())
                    if len(mixed_audio_buffer) > self.max_buffer_size:
                        mixed_audio_buffer.pop(0)
                        
                except IOError as e:
                    logger.warning(f"éŒ„éŸ³æ™‚ç™¼ç”ŸIOéŒ¯èª¤ï¼š{e}")
                    continue
                
                # æ­¥é©Ÿ4: ç•¶ç´¯ç©è¶³å¤ è³‡æ–™æ™‚é€²è¡Œè™•ç†
                if len(buffer) >= window_frames:
                    segment_index += 1
                    audio_data = np.concatenate(buffer[:window_frames])
                    audio_tensor = self.process_audio(audio_data)
                    
                    # æ­¥é©Ÿ5: å¦‚æœéŸ³è¨Šæœ‰æ•ˆï¼Œå•Ÿå‹•èªè€…åˆ†é›¢è™•ç†
                    if audio_tensor is not None:
                        logger.info(f"è™•ç†ç‰‡æ®µ {segment_index}")
                        future = self.executor.submit(
                            self.separate_and_save,
                            audio_tensor,
                            output_dir,
                            segment_index
                        )
                        self.futures.append(future)
                        self.processing_stats['segments_processed'] += 1
                    else:
                        self.processing_stats['segments_skipped'] += 1
                    
                    # æ­¥é©Ÿ6: ç§»å‹•è¦–çª—
                    buffer = buffer[slide_frames:]
                    
                    # å®šæœŸæ¸…ç†å·²å®Œæˆçš„ä»»å‹™
                    if segment_index % 10 == 0:
                        self.cleanup_futures()
                    
                    # æ¯30ç§’å ±å‘Šä¸€æ¬¡çµ±è¨ˆè³‡è¨Š
                    current_time = time.time()
                    if current_time - last_stats_time > 30:
                        self._log_statistics()
                        last_stats_time = current_time
                    
        except Exception as e:
            logger.error(f"éŒ„éŸ³éç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤ï¼š{e}")
        finally:
            # æ­¥é©Ÿ7: æ¸…ç†è³‡æº
            self._cleanup_resources(p, stream, mixed_audio_buffer, output_dir)

    def _cleanup_resources(self, p, stream, mixed_audio_buffer, output_dir):
        """æ¸…ç†è³‡æº"""
        # åœæ­¢ä¸¦é—œé–‰éŸ³è¨Šæµ
        if stream is not None:
            try:
                stream.stop_stream()
                stream.close()
                logger.info("éŸ³è¨Šæµå·²é—œé–‰")
            except Exception as e:
                logger.error(f"é—œé–‰éŸ³è¨Šæµæ™‚ç™¼ç”ŸéŒ¯èª¤ï¼š{e}")
        
        if p is not None:
            try:
                p.terminate()
                logger.info("PyAudio å·²çµ‚æ­¢")
            except Exception as e:
                logger.error(f"çµ‚æ­¢ PyAudio æ™‚ç™¼ç”ŸéŒ¯èª¤ï¼š{e}")
        
        # ç­‰å¾…æ‰€æœ‰è™•ç†ä»»å‹™å®Œæˆ
        logger.info("ç­‰å¾…è™•ç†ä»»å‹™å®Œæˆ...")
        for future in self.futures:
            try:
                future.result(timeout=15.0)
            except Exception as e:
                logger.error(f"è™•ç†ä»»å‹™ç™¼ç”ŸéŒ¯èª¤ï¼š{e}")
        
        self.executor.shutdown(wait=True)
        logger.info("ç·šç¨‹æ± å·²é—œé–‰")
        
        # å„²å­˜åŸå§‹æ··åˆéŸ³è¨Š
        self._save_mixed_audio(mixed_audio_buffer, output_dir)
        
        # è¨˜éŒ„æœ€çµ‚çµ±è¨ˆ
        self._log_final_statistics()
        
        # æ¸…ç†GPUè¨˜æ†¶é«”
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        
        logger.info("éŒ„éŸ³çµæŸï¼Œæ‰€æœ‰è³‡æºå·²æ¸…ç†")

    def _save_mixed_audio(self, mixed_audio_buffer, output_dir):
        """å„²å­˜æ··åˆéŸ³è¨Š"""
        if not mixed_audio_buffer:
            return ""
            
        try:
            mixed_audio = np.concatenate(mixed_audio_buffer)
            mixed_audio = mixed_audio.reshape(-1, CHANNELS)
            
            timestamp = datetime.now().strftime('%Y%m%d-%H_%M_%S')
            mixed_output_file = os.path.join(
                output_dir,
                f"mixed_audio_{timestamp}.wav"
            )
            
            mixed_tensor = torch.from_numpy(mixed_audio).T.float()
            torchaudio.save(
                mixed_output_file,
                mixed_tensor,
                RATE
            )
            logger.info(f"å·²å„²å­˜åŸå§‹æ··åˆéŸ³è¨Šï¼š{mixed_output_file}")
            return mixed_output_file
            
        except Exception as e:
            logger.error(f"å„²å­˜æ··åˆéŸ³è¨Šæ™‚ç™¼ç”ŸéŒ¯èª¤ï¼š{e}")
            return ""

    def _log_statistics(self):
        """è¨˜éŒ„çµ±è¨ˆè³‡è¨Š"""
        stats = self.processing_stats
        logger.info(f"çµ±è¨ˆ - å·²è™•ç†: {stats['segments_processed']}, "
                   f"å·²è·³é: {stats['segments_skipped']}, "
                   f"éŒ¯èª¤: {stats['errors']}, "
                   f"é€²è¡Œä¸­ä»»å‹™: {len(self.futures)}")

    def _log_final_statistics(self):
        """è¨˜éŒ„æœ€çµ‚çµ±è¨ˆè³‡è¨Š"""
        stats = self.processing_stats
        total = stats['segments_processed'] + stats['segments_skipped']
        if total > 0:
            success_rate = (stats['segments_processed'] / total) * 100
            logger.info(f"æœ€çµ‚çµ±è¨ˆ - ç¸½ç‰‡æ®µ: {total}, "
                       f"æˆåŠŸè™•ç†: {stats['segments_processed']} ({success_rate:.1f}%), "
                       f"è·³é: {stats['segments_skipped']}, "
                       f"éŒ¯èª¤: {stats['errors']}")

    def detect_speaker_count(self, audio_tensor: torch.Tensor) -> int:
        """
        åµæ¸¬éŸ³è¨Šä¸­çš„èªªè©±è€…æ•¸é‡
        
        Args:
            audio_tensor: è¼¸å…¥éŸ³è¨Šå¼µé‡ [channels, samples] æˆ– [batch, channels, samples]
            
        Returns:
            int: åµæ¸¬åˆ°çš„èªªè©±è€…æ•¸é‡
        """
        try:
            # ç¢ºä¿éŸ³è¨Šæ ¼å¼æ­£ç¢º
            if len(audio_tensor.shape) == 3:
                audio_data = audio_tensor[0, 0, :].cpu().numpy()
            elif len(audio_tensor.shape) == 2:
                audio_data = audio_tensor[0, :].cpu().numpy()
            else:
                audio_data = audio_tensor.cpu().numpy()
            
            # 0. åŸºæœ¬éœéŸ³æª¢æ¸¬ - é€²ä¸€æ­¥æ”¾å¯¬æ¢ä»¶
            audio_rms = np.sqrt(np.mean(audio_data ** 2))
            audio_max = np.max(np.abs(audio_data))
            audio_std = np.std(audio_data)
            
            logger.info(f"éŸ³è¨Šçµ±è¨ˆ - RMS: {audio_rms:.6f}, Max: {audio_max:.6f}, STD: {audio_std:.6f}, éœéŸ³é–¾å€¼: {self.silence_threshold:.6f}")
            
            # æ›´å¯¬é¬†çš„éœéŸ³æª¢æ¸¬ï¼šåªè¦æœ‰ä¸€å€‹æŒ‡æ¨™è¶…éé–¾å€¼å°±èªç‚ºå¯èƒ½æœ‰èªéŸ³
            is_silent = (audio_rms < self.silence_threshold and 
                        audio_max < self.silence_threshold * 4 and 
                        audio_std < self.silence_threshold * 1.5)
            
            if is_silent:
                logger.info(f"åˆ¤å®šç‚ºéœéŸ³ - æ‰€æœ‰æŒ‡æ¨™éƒ½ä½æ–¼é–¾å€¼")
                return 0
            
            logger.info(f"é€šééœéŸ³æª¢æ¸¬ï¼Œé€²è¡ŒèªéŸ³æ´»å‹•åµæ¸¬...")
            
            # 1. ç°¡åŒ–çš„èªéŸ³æ´»å‹•æª¢æ¸¬ (VAD)
            frame_length = int(TARGET_RATE * 0.025)  # 25ms å¹€
            hop_length = int(TARGET_RATE * 0.010)   # 10ms è·³èº
            
            # è¨ˆç®—çŸ­æ™‚èƒ½é‡
            frames = librosa.util.frame(audio_data, frame_length=frame_length, hop_length=hop_length)
            energy = np.sum(frames ** 2, axis=0)
            
            # æ­£è¦åŒ–èƒ½é‡
            if np.max(energy) > 0:
                energy = energy / np.max(energy)
            
            # ç°¡åŒ–çš„VADï¼šä¸»è¦åŸºæ–¼èƒ½é‡æª¢æ¸¬
            energy_active = energy > self.vad_threshold
            
            # é¡å¤–çš„éé›¶ç‡æª¢æ¸¬ä½œç‚ºè¼”åŠ©ï¼ˆä½†ä¸æ˜¯å¿…éœ€çš„ï¼‰
            try:
                zero_crossings = []
                for i in range(min(frames.shape[1], 100)):  # é™åˆ¶è™•ç†æ•¸é‡ä»¥æé«˜é€Ÿåº¦
                    frame = frames[:, i]
                    zcr = np.sum(np.diff(np.sign(frame)) != 0) / len(frame)
                    zero_crossings.append(zcr)
                zero_crossings = np.array(zero_crossings)
                
                # å¦‚æœæœ‰åˆç†çš„éé›¶ç‡ï¼Œçµåˆä½¿ç”¨ï¼›å¦å‰‡åªç”¨èƒ½é‡
                if len(zero_crossings) > 0:
                    zcr_active = (zero_crossings > 0.005) & (zero_crossings < 0.8)  # éå¸¸å¯¬é¬†çš„ç¯„åœ
                    if np.sum(zcr_active) > len(zcr_active) * 0.05:  # å¦‚æœè‡³å°‘5%çš„å¹€æœ‰åˆç†éé›¶ç‡
                        combined_active = energy_active[:len(zcr_active)] & zcr_active
                        if np.sum(combined_active) > np.sum(energy_active) * 0.3:  # å¦‚æœçµåˆæª¢æ¸¬çµæœä¸æœƒéåº¦é™ä½
                            vad_frames = np.zeros_like(energy_active, dtype=bool)
                            vad_frames[:len(combined_active)] = combined_active
                        else:
                            vad_frames = energy_active
                    else:
                        vad_frames = energy_active
                else:
                    vad_frames = energy_active
            except Exception as e:
                logger.debug(f"éé›¶ç‡æª¢æ¸¬å¤±æ•—ï¼Œä½¿ç”¨ç´”èƒ½é‡æª¢æ¸¬: {e}")
                vad_frames = energy_active
            
            logger.info(f"VADçµæœ - ç¸½å¹€æ•¸: {len(vad_frames)}, æ´»å‹•å¹€æ•¸: {np.sum(vad_frames)}, æ¯”ä¾‹: {np.sum(vad_frames)/len(vad_frames):.3f}")
            
            # 2. æ›´å¯¬é¬†çš„èªéŸ³å€æ®µæª¢æŸ¥
            if np.any(vad_frames):
                total_speech_ratio = np.sum(vad_frames) / len(vad_frames)
                
                # å¤§å¹…é™ä½èªéŸ³æ´»å‹•æ¯”ä¾‹è¦æ±‚
                if total_speech_ratio < 0.02:  # åªè¦2%çš„å¹€æœ‰æ´»å‹•å°±å¯èƒ½æ˜¯èªéŸ³
                    logger.info(f"èªéŸ³æ´»å‹•æ¯”ä¾‹å¤ªä½: {total_speech_ratio:.3f}")
                    return 0
                
                # æª¢æŸ¥é€£çºŒå€æ®µï¼ˆä½†è¦æ±‚æ›´å¯¬é¬†ï¼‰
                diff = np.diff(np.concatenate(([False], vad_frames, [False])).astype(int))
                starts = np.where(diff == 1)[0]
                ends = np.where(diff == -1)[0]
                
                if len(starts) > 0 and len(ends) > 0:
                    speech_durations = (ends - starts) * hop_length / TARGET_RATE
                    max_duration = np.max(speech_durations) if len(speech_durations) > 0 else 0
                    
                    logger.info(f"èªéŸ³å€æ®µåˆ†æ - å€æ®µæ•¸: {len(starts)}, æœ€é•·æŒçºŒæ™‚é–“: {max_duration:.2f}ç§’, æœ€å°è¦æ±‚: {self.min_speech_duration:.2f}ç§’")
                    
                    # å¦‚æœæœ‰ä»»ä½•å€æ®µè¶…éæœ€å°è¦æ±‚ï¼Œæˆ–è€…ç¸½æ´»å‹•æ¯”ä¾‹è¶³å¤ 
                    if max_duration >= self.min_speech_duration or total_speech_ratio > 0.05:
                        logger.info("é€šéèªéŸ³å€æ®µæª¢æŸ¥")
                    else:
                        logger.info(f"èªéŸ³å€æ®µå¤ªçŸ­ä¸”æ´»å‹•æ¯”ä¾‹ä¸è¶³")
                        return 0
                else:
                    # å¦‚æœç„¡æ³•è¨ˆç®—å€æ®µï¼Œæª¢æŸ¥ç¸½é«”æ´»å‹•
                    if total_speech_ratio < 0.03:
                        logger.info(f"ç„¡æ³•è¨ˆç®—å€æ®µä¸”æ´»å‹•æ¯”ä¾‹ä¸è¶³: {total_speech_ratio:.3f}")
                        return 0
            else:
                logger.info("æœªæª¢æ¸¬åˆ°ä»»ä½•èªéŸ³æ´»å‹•")
                return 0
            
            # 3. å¦‚æœé€šéäº†æ‰€æœ‰æª¢æŸ¥ï¼Œå˜—è©¦é€²è¡Œèªªè©±è€…æ•¸é‡ä¼°è¨ˆ
            try:
                # ç°¡åŒ–çš„MFCCåˆ†æ
                mfccs = librosa.feature.mfcc(
                    y=audio_data, 
                    sr=TARGET_RATE,
                    n_mfcc=13,
                    hop_length=hop_length,
                    n_fft=frame_length*2
                )
                
                # ç¢ºä¿ç¶­åº¦åŒ¹é…
                min_frames = min(mfccs.shape[1], len(vad_frames))
                mfccs = mfccs[:, :min_frames]
                vad_frames = vad_frames[:min_frames]
                
                if np.any(vad_frames):
                    active_mfccs = mfccs[:, vad_frames]
                    
                    if active_mfccs.shape[1] < 5:  # é€²ä¸€æ­¥é™ä½è¦æ±‚
                        logger.info(f"æœ‰æ•ˆèªéŸ³å¹€æ•¸å¾ˆå°‘ ({active_mfccs.shape[1]})ï¼Œç›´æ¥åˆ¤å®šç‚ºå–®ä¸€èªè€…")
                        return 1
                    
                    # ç°¡åŒ–çš„èªªè©±è€…æ•¸é‡ä¼°è¨ˆ
                    features = active_mfccs.T
                    audio_duration = len(audio_data) / TARGET_RATE
                    
                    # å°æ–¼çŸ­éŸ³è¨Šï¼Œç›´æ¥åˆ¤å®šç‚ºå–®ä¸€èªè€…
                    if audio_duration < 3.0 or features.shape[0] < 20:
                        logger.info(f"çŸ­éŸ³è¨Š ({audio_duration:.2f}ç§’) æˆ–ç‰¹å¾µå°‘ ({features.shape[0]})ï¼Œåˆ¤å®šç‚ºå–®ä¸€èªè€…")
                        return 1
                    
                    # å˜—è©¦èšé¡åˆ†æï¼ˆä½†å¦‚æœå¤±æ•—å°±å›å‚³1ï¼‰
                    try:
                        clustering = DBSCAN(
                            eps=1.2,  # æ›´å¤§çš„èšé¡åŠå¾‘
                            min_samples=max(3, int(features.shape[0] * 0.1))
                        ).fit(features)
                        
                        unique_labels = set(clustering.labels_)
                        if -1 in unique_labels:
                            unique_labels.remove(-1)
                        
                        detected_speakers = len(unique_labels)
                        
                        if detected_speakers == 0:
                            detected_speakers = 1  # å¾Œå‚™æ–¹æ¡ˆ
                        
                        logger.info(f"èšé¡åˆ†æçµæœ: {detected_speakers} ä½èªªè©±è€…")
                        
                    except Exception as e:
                        logger.info(f"èšé¡åˆ†æå¤±æ•—ï¼Œé è¨­ç‚ºå–®ä¸€èªè€…: {e}")
                        detected_speakers = 1
                    
                else:
                    detected_speakers = 1
                    
            except Exception as e:
                logger.info(f"MFCCåˆ†æå¤±æ•—ï¼Œé è¨­ç‚ºå–®ä¸€èªè€…: {e}")
                detected_speakers = 1
            
            # 4. æœ€çµ‚é™åˆ¶å’Œé©—è­‰
            detected_speakers = min(detected_speakers, self.num_speakers)
            detected_speakers = max(detected_speakers, 1)  # æ—¢ç„¶é€šéäº†èªéŸ³æª¢æ¸¬ï¼Œè‡³å°‘æ˜¯1å€‹èªªè©±è€…
            
            logger.info(f"æœ€çµ‚åµæ¸¬çµæœ: {detected_speakers} ä½èªªè©±è€…")
            return detected_speakers
            
        except Exception as e:
            logger.warning(f"èªè€…æ•¸é‡åµæ¸¬å¤±æ•—ï¼Œä½¿ç”¨å¾Œå‚™æ–¹æ¡ˆ: {e}")
            return self._fallback_speaker_detection(audio_data if 'audio_data' in locals() else audio_tensor.cpu().numpy())

    def _fallback_speaker_detection(self, audio_data: np.ndarray) -> int:
        """
        å¾Œå‚™çš„èªè€…æ•¸é‡åµæ¸¬æ–¹æ³•ï¼ŒåŸºæ–¼ç°¡åŒ–çš„èƒ½é‡åˆ†æ
        
        Args:
            audio_data: éŸ³è¨Šæ•¸æ“š
            
        Returns:
            int: ä¼°è¨ˆçš„èªªè©±è€…æ•¸é‡
        """
        try:
            # éå¸¸å¯¬é¬†çš„éœéŸ³æª¢æ¸¬
            audio_rms = np.sqrt(np.mean(audio_data ** 2))
            audio_max = np.max(np.abs(audio_data))
            
            logger.info(f"å¾Œå‚™æª¢æ¸¬ - RMS: {audio_rms:.6f}, Max: {audio_max:.6f}")
            
            # åªè¦æœ‰ä¸€å€‹æŒ‡æ¨™è¶…éå¾ˆä½çš„é–¾å€¼å°±èªç‚ºæœ‰èªéŸ³
            if audio_rms < self.silence_threshold * 0.3 and audio_max < self.silence_threshold:
                logger.info("å¾Œå‚™æª¢æ¸¬ï¼šåˆ¤å®šç‚ºéœéŸ³")
                return 0
            
            # å¦‚æœé€šééœéŸ³æª¢æ¸¬ï¼Œè‡³å°‘å›å‚³1å€‹èªªè©±è€…
            logger.info("å¾Œå‚™æª¢æ¸¬ï¼šåˆ¤å®šç‚ºæœ‰èªéŸ³æ´»å‹•")
            return 1
            
        except Exception as e:
            logger.warning(f"å¾Œå‚™èªè€…åµæ¸¬ä¹Ÿå¤±æ•—: {e}")
            # çµ‚æ¥µå¾Œå‚™æ–¹æ¡ˆï¼šåªè¦éŸ³è¨Šä¸æ˜¯å®Œå…¨éœéŸ³å°±èªç‚ºæœ‰1å€‹èªªè©±è€…
            try:
                if np.any(np.abs(audio_data) > 0.0001):  # æ¥µä½çš„é–¾å€¼
                    logger.info("çµ‚æ¥µå¾Œå‚™ï¼šåµæ¸¬åˆ°éé›¶éŸ³è¨Š")
                    return 1
                else:
                    logger.info("çµ‚æ¥µå¾Œå‚™ï¼šéŸ³è¨Šå®Œå…¨éœéŸ³")
                    return 0
            except:
                logger.info("æ‰€æœ‰æª¢æ¸¬éƒ½å¤±æ•—ï¼Œé è¨­å›å‚³1")
                return 1  # æœ€ä¿éšªçš„é¸æ“‡

    def separate_and_save(self, audio_tensor, output_dir, segment_index):
        """åˆ†é›¢ä¸¦å„²å­˜éŸ³è¨Šï¼Œä¸¦å›å‚³ (path, start, end) åˆ—è¡¨ã€‚"""
        try:
            # æ–°å¢ï¼šåœ¨åˆ†é›¢å‰å…ˆåµæ¸¬èªªè©±è€…æ•¸é‡
            detected_speakers = self.detect_speaker_count(audio_tensor)
            logger.info(f"ç‰‡æ®µ {segment_index} - åµæ¸¬åˆ° {detected_speakers} ä½èªªè©±è€…")
            
            # å¦‚æœæ²’æœ‰åµæ¸¬åˆ°èªªè©±è€…ï¼Œè·³éè™•ç†
            if detected_speakers == 0:
                logger.info(f"ç‰‡æ®µ {segment_index} - æœªåµæ¸¬åˆ°èªªè©±è€…ï¼Œè·³éè™•ç†")
                return []
            
            # åˆå§‹åŒ–ç´¯è¨ˆæ™‚é–“æˆ³
            current_t0 = getattr(self, "_current_t0", 0.0)
            results = []   # ç”¨ä¾†æ”¶ (path, start, end)
            seg_duration = audio_tensor.shape[-1] / TARGET_RATE
            
            with torch.no_grad():
                
                # æ ¹æ“šæ¨¡å‹é¡å‹é¸æ“‡ä¸åŒçš„åˆ†é›¢æ–¹æ³•
                # åœ¨åˆ†é›¢æ–¹æ³•ä¸­ï¼Œç¢ºä¿ SpeechBrain æ¨¡å‹å¾—åˆ°æ­£ç¢ºæ ¼å¼
                if self.model_config["use_speechbrain"]:
                    # ç¢ºä¿è¼¸å…¥æ˜¯ [batch, samples] æ ¼å¼
                    if len(audio_tensor.shape) == 3:
                        # å¦‚æœæ˜¯ [batch, channels, samples]ï¼Œéœ€è¦å»æ‰ channels ç¶­åº¦
                        if audio_tensor.shape[1] == 1:
                            audio_tensor = audio_tensor.squeeze(1)  # è®Šæˆ [batch, samples]
                    separated = self.model.separate_batch(audio_tensor)
                else:
                    # ConvTasNet éœ€è¦ [batch, channels, samples] æ ¼å¼
                    if len(audio_tensor.shape) == 2:
                        audio_tensor = audio_tensor.unsqueeze(0)  # åŠ ä¸Š batch ç¶­åº¦
                    separated = self.model(audio_tensor)
                
                if self.enable_noise_reduction:
                    enhanced_separated = self.enhance_separation(separated)
                else:
                    enhanced_separated = separated
                
                del separated
                if torch.cuda.is_available():
                    torch.cuda.empty_cache()
                
                timestamp = datetime.now().strftime('%Y%m%d-%H_%M_%S')
                
                # è™•ç†è¼¸å‡ºæ ¼å¼
                if self.model_config["use_speechbrain"]:
                    # SpeechBrain æ¨¡å‹è¼¸å‡ºè™•ç†
                    if len(enhanced_separated.shape) == 3:
                        num_speakers = enhanced_separated.shape[2]
                        speaker_dim = 2
                    else:
                        num_speakers = 1
                        speaker_dim = 0
                else:
                    # ConvTasNet æ¨¡å‹è¼¸å‡ºè™•ç†ï¼ˆåŸæœ‰é‚è¼¯ï¼‰
                    if len(enhanced_separated.shape) == 3:
                        if enhanced_separated.shape[1] == self.num_speakers:
                            num_speakers = enhanced_separated.shape[1]
                            speaker_dim = 1
                        else:
                            num_speakers = enhanced_separated.shape[2]
                            speaker_dim = 2
                    else:
                        num_speakers = 1
                        speaker_dim = 0
                
                saved_count = 0
                start_time = current_t0
                
                # æ ¹æ“šåµæ¸¬åˆ°çš„èªªè©±è€…æ•¸é‡é™åˆ¶è¼¸å‡º
                effective_speakers = min(detected_speakers, num_speakers, self.num_speakers)
                
                for i in range(effective_speakers):
                    try:
                        if speaker_dim == 1:
                            speaker_audio = enhanced_separated[0, i, :].cpu()
                        elif speaker_dim == 2:
                            speaker_audio = enhanced_separated[0, :, i].cpu()
                        else:
                            speaker_audio = enhanced_separated.cpu().squeeze()
                        
                        # æ”¹å–„çš„æ­£è¦åŒ–è™•ç†
                        if len(speaker_audio.shape) > 1:
                            speaker_audio = speaker_audio.squeeze()
                        
                        # æª¢æŸ¥éŸ³è¨Šå“è³ª
                        rms = torch.sqrt(torch.mean(speaker_audio ** 2))
                        if rms > 0.01:  # åªä¿å­˜æœ‰æ„ç¾©çš„éŸ³è¨Š
                            # æº«å’Œçš„æ­£è¦åŒ–
                            max_val = torch.max(torch.abs(speaker_audio))
                            if max_val > 0:
                                # ä½¿ç”¨è»Ÿé™åˆ¶å™¨
                                normalized = speaker_audio / max_val
                                speaker_audio = torch.tanh(normalized * 0.9) * 0.85
                        
                            final_tensor = speaker_audio.unsqueeze(0)
                            
                            output_file = os.path.join(
                                output_dir,
                                f"speaker{i+1}.wav"
                                # f"speaker{i+1}_{timestamp}_{segment_index}.wav"
                            )
                            
                            torchaudio.save(
                                output_file,
                                final_tensor,
                                TARGET_RATE
                            )

                            results.append((output_file,
                                    start_time,
                                    start_time + seg_duration))
                            self.output_files.append(output_file)

                            saved_count += 1
                    except Exception as e:
                        logger.warning(f"å„²å­˜èªè€… {i+1} å¤±æ•—: {e}")
                
                if saved_count > 0:
                    logger.info(f"ç‰‡æ®µ {segment_index} å®Œæˆï¼Œå¯¦éš›å„²å­˜ {saved_count}/{effective_speakers} å€‹æª”æ¡ˆ")
                
            # æ›´æ–°ç´¯è¨ˆæ™‚é–“åˆ°ä¸‹ä¸€æ®µ
            current_t0 += seg_duration
            self._current_t0 = current_t0

            if not results:
                raise RuntimeError("Speaker separation produced no valid tracks")

            return results

        except Exception as e:
            logger.error(f"è™•ç†ç‰‡æ®µ {segment_index} å¤±æ•—: {e}")
            self.processing_stats['errors'] += 1
        finally:
            if torch.cuda.is_available():
                torch.cuda.empty_cache()

    def separate_and_identify(self, audio_tensor: torch.Tensor, output_dir: str, segment_index: int) -> None:
        """åˆ†é›¢éŸ³è¨Šä¸¦ç›´æ¥é€²è¡ŒèªéŸ³è­˜åˆ¥ï¼Œå¯é¸æ“‡æ˜¯å¦å„²å­˜éŸ³è¨Šæª”æ¡ˆ"""
        try:
            # æ–°å¢ï¼šåœ¨åˆ†é›¢å‰å…ˆåµæ¸¬èªªè©±è€…æ•¸é‡
            detected_speakers = self.detect_speaker_count(audio_tensor)
            logger.info(f"ç‰‡æ®µ {segment_index} - åµæ¸¬åˆ° {detected_speakers} ä½èªªè©±è€…")
            
            # å¦‚æœæ²’æœ‰åµæ¸¬åˆ°èªªè©±è€…ï¼Œè·³éè™•ç†
            if detected_speakers == 0:
                logger.info(f"ç‰‡æ®µ {segment_index} - æœªåµæ¸¬åˆ°èªªè©±è€…ï¼Œè·³éè™•ç†")
                return []
            
            audio_files = []
            audio_streams = []
            
            timestamp_obj = datetime.now()
            timestamp = timestamp_obj.strftime('%Y%m%d-%H_%M_%S')
            
            with torch.no_grad():
                # ç¢ºä¿è¼¸å…¥å½¢ç‹€æ­£ç¢º
                if len(audio_tensor.shape) == 2:
                    audio_tensor = audio_tensor.unsqueeze(0)
                
                # æ­¥é©Ÿ1: èªè€…åˆ†é›¢ - æ ¹æ“šæ¨¡å‹é¡å‹é¸æ“‡ä¸åŒæ–¹æ³•
                if self.model_config["use_speechbrain"]:
                    separated = self.model.separate_batch(audio_tensor)
                else:
                    separated = self.model(audio_tensor)
                
                if self.enable_noise_reduction:
                    enhanced_separated = self.enhance_separation(separated)
                else:
                    enhanced_separated = separated
                
                del separated
                if torch.cuda.is_available():
                    torch.cuda.empty_cache()
                
                # è™•ç†è¼¸å‡ºæ ¼å¼
                if self.model_config["use_speechbrain"]:
                    # SpeechBrain æ¨¡å‹è¼¸å‡ºè™•ç†
                    if len(enhanced_separated.shape) == 3:
                        num_speakers = enhanced_separated.shape[2]
                        speaker_dim = 2
                    else:
                        num_speakers = 1
                        speaker_dim = 0
                else:
                    # ConvTasNet æ¨¡å‹è¼¸å‡ºè™•ç†ï¼ˆåŸæœ‰é‚è¼¯ï¼‰
                    if len(enhanced_separated.shape) == 3:
                        if enhanced_separated.shape[1] == self.num_speakers:
                            num_speakers = enhanced_separated.shape[1]
                            speaker_dim = 1
                        else:
                            num_speakers = enhanced_separated.shape[2]
                            speaker_dim = 2
                    else:
                        num_speakers = 1
                        speaker_dim = 0
                
                saved_count = 0
                for i in range(min(num_speakers, self.num_speakers)):
                    try:
                        if speaker_dim == 1:
                            speaker_audio = enhanced_separated[0, i, :].cpu()
                        elif speaker_dim == 2:
                            speaker_audio = enhanced_separated[0, :, i].cpu()
                        else:
                            speaker_audio = enhanced_separated.cpu().squeeze()
                        
                        # æ”¹å–„çš„æ­£è¦åŒ–è™•ç†
                        if len(speaker_audio.shape) > 1:
                            speaker_audio = speaker_audio.squeeze()
                        
                        # æª¢æŸ¥éŸ³è¨Šå“è³ª
                        rms = torch.sqrt(torch.mean(speaker_audio ** 2))
                        if rms > 0.01:  # åªå„²å­˜æœ‰æ„ç¾©çš„éŸ³è¨Š
                            # æº«å’Œçš„æ­£è¦åŒ–
                            max_val = torch.max(torch.abs(speaker_audio))
                            if max_val > 0:
                                # ä½¿ç”¨è»Ÿé™åˆ¶å™¨
                                normalized = speaker_audio / max_val
                                speaker_audio = torch.tanh(normalized * 0.9) * 0.85
                        
                            final_audio = speaker_audio.numpy()
                            final_tensor = speaker_audio.unsqueeze(0)
                            
                            # å„²å­˜éŸ³è¨Šä¸²æµè³‡æ–™ä¾›ç›´æ¥è¾¨è­˜ä½¿ç”¨
                            audio_streams.append({
                                'audio_data': final_audio,
                                'sample_rate': TARGET_RATE,
                                'name': f"speaker{i+1}_{timestamp}_{segment_index}"
                            })
                            
                            # å¦‚æœè¨­å®šè¦å„²å­˜éŸ³è¨Šæª”æ¡ˆï¼Œå‰‡é¡å¤–å„²å­˜åˆ†é›¢æª”æ¡ˆ
                            if self.save_audio_files:
                                output_file = os.path.join(
                                    output_dir,
                                    f"speaker{i+1}_{timestamp}_{segment_index}.wav"
                                )
                                
                                torchaudio.save(
                                    output_file,
                                    final_tensor,
                                    TARGET_RATE
                                )
                                
                                audio_files.append(output_file)
                                self.output_files.append(output_file)
                            
                            saved_count += 1
                            
                    except Exception as e:
                        logger.warning(f"è™•ç†èªè€… {i+1} æ™‚å¤±æ•—: {e}")
                
                if saved_count > 0:
                    logger.info(f"ç‰‡æ®µ {segment_index} åˆ†é›¢å®Œæˆï¼Œå…±è™•ç† {saved_count} å€‹èªè€…")
            
            # æ­¥é©Ÿ2: å³æ™‚é€²è¡Œèªè€…è­˜åˆ¥
            logger.info(
                f"ç‰‡æ®µ {segment_index} åˆ†é›¢å®Œæˆï¼Œé–‹å§‹é€²è¡Œå³æ™‚èªè€…è­˜åˆ¥...",
                extra={"simple": True}
            )
            
            try:
                identifier = SpeakerIdentifier()
                
                results = {}
                if audio_streams:
                    results = identifier.process_audio_streams(audio_streams, timestamp_obj)
                
                # ä½¿ç”¨ç°¡åŒ–æ ¼å¼è¼¸å‡ºè­˜åˆ¥çµæœ
                result_message = []
                for audio_name, (speaker, distance, result) in results.items():
                    result_message.append(f"ã€{audio_name} â†’ {result}ã€‘")
                
                if result_message:
                    message = f"ç‰‡æ®µ {segment_index} è­˜åˆ¥çµæœ:  " + "  ".join(result_message)
                    logger.info(
                        message,
                        extra={"simple": True}
                    )
                    
            except Exception as e:
                logger.error(f"è­˜åˆ¥ç‰‡æ®µ {segment_index} æ™‚ç™¼ç”ŸéŒ¯èª¤ï¼š{e}")
            
            logger.info(f"ç‰‡æ®µ {segment_index} è™•ç†å®Œæˆ")
            
        except Exception as e:
            logger.error(f"è™•ç†ç‰‡æ®µ {segment_index} å¤±æ•—: {e}")
            self.processing_stats['errors'] += 1
        finally:
            if torch.cuda.is_available():
                torch.cuda.empty_cache()

    def stop_recording(self):
        """åœæ­¢éŒ„éŸ³"""
        self.is_recording = False
        logger.info("æº–å‚™åœæ­¢éŒ„éŸ³...")

    def get_output_files(self):
        """ç²å–æ‰€æœ‰åˆ†é›¢å¾Œçš„éŸ³æª”è·¯å¾‘"""
        return self.output_files


# ================== èªè€…è­˜åˆ¥éƒ¨åˆ† ======================

class SpeakerIdentifier:
    """èªè€…è­˜åˆ¥é¡ï¼Œè² è²¬å‘¼å« v5 ç‰ˆæœ¬çš„èªè€…è­˜åˆ¥åŠŸèƒ½ï¼Œä½¿ç”¨å–®ä¾‹æ¨¡å¼"""
    
    _instance = None
    
    def __new__(cls) -> 'SpeakerIdentifier':
        """å¯¦ç¾å–®ä¾‹æ¨¡å¼ï¼Œç¢ºä¿å…¨å±€åªæœ‰ä¸€å€‹å¯¦ä¾‹"""
        if cls._instance is None:
            cls._instance = super(SpeakerIdentifier, cls).__new__(cls)
            cls._instance._initialized = False
        return cls._instance
    
    def __init__(self) -> None:
        """åˆå§‹åŒ–èªè€…è­˜åˆ¥å™¨ï¼Œä½¿ç”¨ v5 ç‰ˆæœ¬çš„ SpeakerIdentifier"""
        # è‹¥å·²åˆå§‹åŒ–ï¼Œå‰‡è·³é
        if hasattr(self, '_initialized') and self._initialized:
            return
            
        try:
            # åˆå§‹åŒ– v5 æ¨¡çµ„ (å®ƒä¹Ÿæœƒä½¿ç”¨å–®ä¾‹æ¨¡å¼ï¼Œé¿å…é‡è¤‡åŠ è¼‰æ¨¡å‹)
            self.identifier = speaker_id.SpeakerIdentifier()
            
            # è¨­ç½®è©³ç´°åº¦ï¼Œæ¸›å°‘éå¿…è¦è¼¸å‡º
            self.identifier.set_verbose(False)
            
            logger.info("èªè€…è­˜åˆ¥å™¨åˆå§‹åŒ–å®Œæˆ")
            self._initialized = True
        except Exception as e:
            logger.error(f"åˆå§‹åŒ–èªè€…è­˜åˆ¥å™¨æ™‚ç™¼ç”ŸéŒ¯èª¤ï¼š{e}")
            raise
    
    def process_audio_streams(self, audio_streams: list, timestamp: datetime) -> dict:
        """
        è™•ç†å¤šå€‹éŸ³è¨Šæµä¸¦é€²è¡Œèªè€…è­˜åˆ¥
        
        Args:
            audio_streams: éŸ³è¨Šæµè³‡æ–™åˆ—è¡¨ï¼Œæ¯å€‹å…ƒç´ åŒ…å« 'audio_data', 'sample_rate', 'name'
            timestamp: éŸ³è¨Šæµçš„æ™‚é–“æˆ³è¨˜ç‰©ä»¶
            
        Returns:
            dict: éŸ³è¨Šæµåç¨± -> (èªè€…åç¨±, ç›¸ä¼¼åº¦, è­˜åˆ¥çµæœæè¿°)
        """
        results = {}
        
        try:
            for stream in audio_streams:
                audio_data = stream['audio_data']
                sample_rate = stream['sample_rate']
                name = stream['name']
                
                logger.info(f"è­˜åˆ¥éŸ³è¨Šæµ: {name}")
                
                # å‘¼å« v5 ç‰ˆæœ¬çš„èªè€…è­˜åˆ¥åŠŸèƒ½ï¼Œå‚³å…¥æ™‚é–“æˆ³è¨˜
                result = self.identifier.process_audio_stream(
                    audio_data, 
                    sample_rate, 
                    audio_source=name,
                    timestamp=timestamp
                )
                
                if result:
                    speaker_id_, speaker_name, distance = result
                    
                    # æ ¹æ“šè·é›¢åˆ¤æ–·è­˜åˆ¥çµæœ
                    if distance == -1:
                        # è·é›¢ç‚º -1 è¡¨ç¤ºæ–°å»ºç«‹çš„èªè€…
                        result_desc = f"æ–°èªè€… {speaker_name} \t(å·²å»ºç«‹æ–°è²ç´‹:{distance:.4f})"
                    elif distance < THRESHOLD_LOW:
                        result_desc = f"èªè€… {speaker_name} \t(è²éŸ³éå¸¸ç›¸ä¼¼:{distance:.4f})"
                    elif distance < THRESHOLD_UPDATE:
                        result_desc = f"èªè€… {speaker_name} \t(å·²æ›´æ–°è²ç´‹:{distance:.4f})"
                    elif distance < THRESHOLD_NEW:
                        result_desc = f"èªè€… {speaker_name} \t(æ–°å¢æ–°çš„è²ç´‹:{distance:.4f})"
                    else:
                        # æ­¤è™•ä¸æ‡‰è©²åŸ·è¡Œåˆ°ï¼Œå› ç‚ºè·é›¢å¤§æ–¼ THRESHOLD_NEW æ™‚æ‡‰è©²å‰µå»ºæ–°èªè€…
                        result_desc = f"èªè€… {speaker_name} \t(åˆ¤æ–·ä¸æ˜ç¢º):{distance:.4f}"
                    
                    results[name] = (speaker_name, distance, result_desc)
                    # logger.info(f"çµæœ: {result_desc}")
                else:
                    results[name] = (None, -1, "è­˜åˆ¥å¤±æ•—")
                    logger.warning("è­˜åˆ¥å¤±æ•—")
        except Exception as e:
            logger.error(f"è™•ç†éŸ³è¨Šæµæ™‚ç™¼ç”ŸéŒ¯èª¤ï¼š{e}")
        
        return results
    
def check_weaviate_connection() -> bool:
    """
    æª¢æŸ¥ Weaviate è³‡æ–™åº«é€£ç·šç‹€æ…‹ã€‚

    Returns:
        bool: è‹¥é€£ç·šæˆåŠŸå›å‚³ Trueï¼Œå¦å‰‡å›å‚³ Falseã€‚
    """
    try:
        import weaviate  # type: ignore
        client = weaviate.connect_to_local()
        # æª¢æŸ¥æ˜¯å¦èƒ½å­˜å–å¿…è¦é›†åˆ
        if not client.is_live():
            logger.error("Weaviate æœå‹™æœªå•Ÿå‹•æˆ–ç„¡æ³•å­˜å–ã€‚")
            return False
        if not (client.collections.exists("Speaker") and client.collections.exists("VoicePrint")):
            logger.error("Weaviate ç¼ºå°‘å¿…è¦é›†åˆ (Speaker æˆ– VoicePrint)ã€‚è«‹å…ˆåŸ·è¡Œ create_collections.pyã€‚")
            return False
        return True
    except Exception as e:
        logger.error(f"Weaviate é€£ç·šå¤±æ•—ï¼š{e}")
        return False
    
def run_realtime(output_dir: str = OUTPUT_DIR, model_type: SeparationModel = None, model_name: str = None) -> str:
    """æ–¹ä¾¿å¤–éƒ¨å‘¼å«çš„éŒ„éŸ³è™•ç†å‡½å¼ï¼Œæ”¯æ´æ¨¡å‹é¸æ“‡"""
    if model_name:
        separator = create_separator(model_name)
    elif model_type:
        separator = AudioSeparator(model_type=model_type)
    else:
        separator = AudioSeparator(model_type=DEFAULT_MODEL)
    return separator.record_and_process(output_dir)


def run_offline(file_path: str, output_dir: str = OUTPUT_DIR, save_files: bool = True, 
                model_type: SeparationModel = None, model_name: str = None) -> None:
    """æ–¹ä¾¿å¤–éƒ¨å‘¼å«çš„é›¢ç·šéŸ³æª”è™•ç†å‡½å¼ï¼Œæ”¯æ´æ¨¡å‹é¸æ“‡"""
    if model_name:
        separator = create_separator(model_name)
    elif model_type:
        separator = AudioSeparator(model_type=model_type)
    else:
        separator = AudioSeparator(model_type=DEFAULT_MODEL)
    separator.set_save_audio_files(save_files)
    separator.process_audio_file(file_path, output_dir)

# if __name__ == '__main__':
#     run_realtime(output_dir=OUTPUT_DIR, model_type=SeparationModel.SEPFORMER_2SPEAKER)