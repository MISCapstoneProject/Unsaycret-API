"""
===============================================================================
å³æ™‚éŒ„éŸ³èˆ‡èªéŸ³åˆ†é›¢æ¨¡çµ„ (Real-time Recording & Speech Separation Module)
===============================================================================

ç‰ˆæœ¬ï¼šv3.0.0
ä½œè€…ï¼šEvanLo62
æœ€å¾Œæ›´æ–°ï¼š2025-08-24

æ¨¡çµ„æ¦‚è¦ï¼š
-----------
æœ¬æ¨¡çµ„æä¾›å³æ™‚èªéŸ³åˆ†é›¢èˆ‡èªè€…è­˜åˆ¥è§£æ±ºæ–¹æ¡ˆï¼Œæ•´åˆå…ˆé€²çš„æ·±åº¦å­¸ç¿’æŠ€è¡“ï¼Œ
å¯¦ç¾å¤šèªè€…èªéŸ³çš„ç²¾ç¢ºåˆ†é›¢èˆ‡å³æ™‚èº«ä»½è­˜åˆ¥ã€‚æ”¯æ´é‚ŠéŒ„éŸ³é‚Šè™•ç†çš„ä¸²æµæ¨¡å¼ï¼Œ
ç‚ºèªéŸ³æœƒè­°ã€å®¢æœç³»çµ±ã€èªéŸ³åŠ©ç†ç­‰æ‡‰ç”¨å ´æ™¯æä¾›å¼·å¤§çš„æŠ€è¡“æ”¯æ´ã€‚

ğŸ¯ æ ¸å¿ƒåŠŸèƒ½ï¼š
 â€¢ å³æ™‚èªéŸ³åˆ†é›¢ï¼šæ”¯æ´ 2-3 äººåŒæ™‚èªªè©±çš„èªéŸ³åˆ†é›¢
 â€¢ æ™ºæ…§èªè€…åµæ¸¬ï¼šè‡ªå‹•åµæ¸¬èªè€…æ•¸é‡ï¼Œå‹•æ…‹èª¿æ•´åˆ†é›¢ç­–ç•¥
 â€¢ éŸ³è¨Šå“è³ªå„ªåŒ–ï¼šå¤šå±¤é™å™ªèˆ‡éŸ³è³ªå¢å¼·è™•ç†
 â€¢ å½ˆæ€§éƒ¨ç½²æ¶æ§‹ï¼šæ”¯æ´ CPU/GPU æ··åˆé‹ç®—ï¼Œå¯æ“´å±•è‡³å¢é›†éƒ¨ç½²

ğŸ”§ æŠ€è¡“æ¶æ§‹ï¼š
-----------
 åˆ†é›¢å¼•æ“    ï¼šSpeechBrain SepFormer (16kHz å„ªåŒ–ç‰ˆæœ¬)
 éŸ³è¨Šè™•ç†    ï¼šPyTorch + torchaudio (CUDA åŠ é€Ÿ)
 ä¸¦ç™¼è™•ç†    ï¼šThreadPoolExecutor (å¤šåŸ·è¡Œç·’æœ€ä½³åŒ–)
 å“è³ªå¢å¼·    ï¼šé »è­œé–˜æ§é™å™ª + ç¶­ç´æ¿¾æ³¢ + å‹•æ…‹ç¯„åœå£“ç¸®

ğŸ“Š æ•ˆèƒ½æŒ‡æ¨™ï¼š
-----------
 â€¢ è™•ç†å»¶é²ï¼š< 500ms (å³æ™‚è™•ç†)
 â€¢ åˆ†é›¢ç²¾åº¦ï¼šSNR æå‡ 10-15dB
 â€¢ è­˜åˆ¥æº–ç¢ºç‡ï¼š> 95% (å·²çŸ¥èªè€…)
 â€¢ è¨˜æ†¶é«”ä½¿ç”¨ï¼š< 2GB (GPUæ¨¡å¼)
 â€¢ ä¸¦ç™¼èƒ½åŠ›ï¼šæ”¯æ´ 10+ åŒæ™‚æœƒè©±

ğŸš€ ä½¿ç”¨å ´æ™¯ï¼š
-----------
 âœ… å¤šäººèªéŸ³æœƒè­°è¨˜éŒ„èˆ‡åˆ†æ
 âœ… å®¢æœé›»è©±è‡ªå‹•åˆ†é›¢èˆ‡å“è³ªç›£æ§
 âœ… æ•™è‚²è¨“ç·´èªéŸ³å…§å®¹åˆ†æ
 âœ… åª’é«”è¨ªè«‡è‡ªå‹•è½‰éŒ„
 âœ… æ³•åº­è¨˜éŒ„èªè€…å€åˆ†

ğŸ”§ ç³»çµ±éœ€æ±‚ï¼š
-----------
 æœ€ä½é…ç½®ï¼š
  - Python 3.9+
  - RAM: 8GB+
  - å„²å­˜ç©ºé–“: 5GB+
  - ç¶²è·¯: ç©©å®šé€£ç·š (æ¨¡å‹ä¸‹è¼‰)

 å»ºè­°é…ç½®ï¼š
  - GPU: NVIDIA RTX 3060+ (8GB VRAM)
  - RAM: 16GB+
  - CPU: Intel i7 / AMD Ryzen 7+
  - SSD: 50GB+ å¯ç”¨ç©ºé–“

ğŸŒŸ é€²éšåŠŸèƒ½ï¼š
-----------
 â€¢ éŸ³è¨Šå“è³ªè©•ä¼°ï¼šSNR è‡ªå‹•åµæ¸¬èˆ‡é©æ‡‰æ€§è™•ç†
 â€¢ å‚™ç”¨åˆ†é›¢ç­–ç•¥ï¼šèªè€…åµæ¸¬å¤±æ•—æ™‚çš„æ™ºæ…§é™ç´šè™•ç†
 â€¢ å½ˆæ€§è¼¸å‡ºæ ¼å¼ï¼šæ”¯æ´æª”æ¡ˆå„²å­˜æˆ–è¨˜æ†¶é«”ä¸²æµ
 â€¢ æ•ˆèƒ½ç›£æ§ï¼šå³æ™‚çµ±è¨ˆè™•ç†æ•ˆç‡èˆ‡è³‡æºä½¿ç”¨

ğŸ“ æ ¸å¿ƒé¡åˆ¥ï¼š
-----------
 AudioSeparator     ï¼šä¸»è¦åˆ†é›¢å¼•æ“ï¼Œè² è²¬éŸ³è¨Šåˆ†é›¢èˆ‡å“è³ªè™•ç†
 SeparationModel    ï¼šæ¨¡å‹é…ç½®åˆ—èˆ‰ï¼Œæ”¯æ´ 2/3 äººåˆ†é›¢æ¨¡å‹

âš™ï¸ è¨­å®šåƒæ•¸ï¼š
-----------
 WINDOW_SIZE        = 6      # è™•ç†çª—å£ (ç§’)
 OVERLAP           = 0.5     # çª—å£é‡ç–Šç‡
 TARGET_RATE       = 16000   # ç›®æ¨™å–æ¨£ç‡
 THRESHOLD_NEW     = 0.385   # æ–°èªè€…åˆ¤å®šé–¾å€¼
 MIN_ENERGY        = 0.001   # æœ€å°éŸ³è¨Šèƒ½é‡é–¾å€¼

ğŸ“ˆ è¼¸å‡ºè³‡æ–™ï¼š
-----------
 åˆ†é›¢éŸ³æª”ï¼š./R3SI/Audio-storage/speaker{N}.wav
 æ··åˆéŸ³æª”ï¼š./R3SI/Audio-storage/mixed_audio_{timestamp}.wav
 è™•ç†æ—¥èªŒï¼šå³æ™‚è¼¸å‡ºè‡³ loggerï¼Œæ”¯æ´å¤šå±¤ç´šè¨˜éŒ„
 è­˜åˆ¥çµæœï¼šJSON æ ¼å¼ï¼ŒåŒ…å«èªè€…åç¨±ã€å„èªè€…éŸ³è¨Šã€æ™‚é–“æˆ³

ğŸ”— ç›¸é—œæ¨¡çµ„ï¼š
-----------
 â€¢ utils.logger (çµ±ä¸€æ—¥èªŒç®¡ç†)
 â€¢ utils.env_config (ç’°å¢ƒè®Šæ•¸é…ç½®)  
 â€¢ utils.constants (ç³»çµ±å¸¸æ•¸å®šç¾©)

ğŸ“š ä½¿ç”¨ç¯„ä¾‹ï¼š
-----------
 # å³æ™‚éŒ„éŸ³åˆ†é›¢
 separator = AudioSeparator(model_type=SeparationModel.SEPFORMER_3SPEAKER)
 separator.record_and_process("./output")
 
 # é›¢ç·šæª”æ¡ˆè™•ç†
 run_offline("meeting.wav", "./output", model_name="sepformer_3speaker")

ğŸ’¡ æœ€ä½³å¯¦è¸ï¼š
-----------
 1. ä½¿ç”¨ GPU åŠ é€Ÿä»¥ç²å¾—æœ€ä½³æ•ˆèƒ½
 2. å®šæœŸæ¸…ç†è¼¸å‡ºç›®éŒ„é¿å…å„²å­˜ç©ºé–“ä¸è¶³
 3. ç›£æ§ç³»çµ±è³‡æºä½¿ç”¨ï¼Œé¿å…è¨˜æ†¶é«”æ´©æ¼
 4. åœ¨ç”Ÿç”¢ç’°å¢ƒä¸­å•Ÿç”¨è©³ç´°æ—¥èªŒè¨˜éŒ„

ğŸ“ æŠ€è¡“æ”¯æ´ï¼š
-----------
 å°ˆæ¡ˆå€‰åº«ï¼šhttps://github.com/MISCapstoneProject/Unsaycret-API/tree/v0.4.2
 å•é¡Œå›å ±ï¼šGitHub Issues
 æŠ€è¡“æ–‡ä»¶ï¼šREADME.md & docs/

/*
 *                                                     __----~~~~~~~~~~~------___
 *                                    .  .   ~~//====......          __--~ ~~
 *                    -.            \_|//     |||\  ~~~~~~::::... /~
 *                 ___-==_       _-~o~  \/    |||  \            _/~~-
 *         __---~~~.==~||\=_    -_--~/_-~|-   |\   \        _/~
 *     _-~~     .=~    |  \-_    '-~7  /-   /  ||    \      /
 *   .~       .~       |   \ -_    /  /-   /   ||      \   /
 *  /  ____  /         |     \ ~-_/  /|- _/   .||       \ /
 *  |~~    ~~|--~~~~--_ \     ~==-/   | \~--===~~        .\
 *           '         ~-|      /|    |-~\~~       __--~~
 *                       |-~~-_/ |    |   ~\_   _-~            /\
 *                            /  \     \__   \/~                \__
 *                        _--~ _/ | .-~~____--~-/                  ~~==.
 *                       ((->/~   '.|||' -_|    ~~-/ ,              . _||
 *                                  -_     ~\      ~~---l__i__i__i--~~_/
 *                                  _-~-__   ~)  \--______________--~~
 *                                //.-~~~-~_--~- |-------~~~~~~~~
 *                                       //.-~~~--\
 *                       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
 * 
 *                               ç¥ç¸ä¿ä½‘            æ°¸ç„¡BUG
 */

===============================================================================
"""
from __future__ import annotations
import os
import numpy as np
import torch
import torchaudio
import pyaudio # type: ignore
import logging
from datetime import datetime
from concurrent.futures import ThreadPoolExecutor
from speechbrain.inference import SepformerSeparation as separator
import noisereduce as nr # type: ignore
import threading
import time
from scipy import signal
from scipy.ndimage import uniform_filter1d
from enum import Enum

# å°å…¥èªè€…æ•¸é‡è­˜åˆ¥æ¨¡çµ„
from pyannote.audio import Pipeline

# å°å…¥æ—¥èªŒæ¨¡çµ„
from utils.logger import get_logger

# å°å…¥é…ç½® (ç’°å¢ƒè®Šæ•¸)
from utils.env_config import (
    AUDIO_RATE, FORCE_CPU, CUDA_DEVICE_INDEX, HF_ACCESS_TOKEN
)

# å°å…¥å¸¸æ•¸ (æ‡‰ç”¨ç¨‹å¼åƒæ•¸)  
from utils.constants import (
    DEFAULT_SEPARATION_MODEL,
    AUDIO_SAMPLE_RATE, AUDIO_CHUNK_SIZE, AUDIO_CHANNELS, 
    AUDIO_WINDOW_SIZE, AUDIO_OVERLAP, AUDIO_MIN_ENERGY_THRESHOLD, 
    AUDIO_MAX_BUFFER_MINUTES, API_MAX_WORKERS, AUDIO_TARGET_RATE
)

# å°å…¥å‹•æ…‹æ¨¡å‹ç®¡ç†å™¨
from .dynamic_model_manager import (
    SeparationModel,
    MODEL_CONFIGS,
    create_dynamic_model_manager,
    get_available_models
)

# å°å…¥èªè€…è¨ˆæ•¸å™¨
from .speaker_counter import SpeakerCounter

# å°å…¥å–®äººé¸è·¯å™¨
from .best_speaker_selector import SingleSpeakerSelector

from .assess_quality import assess_audio_quality
from .process_before_id import _gentle_blend, _hf_hiss_suppress, _prep_id_audio, _soft_spectral_floor, _tpdf_dither, crosstalk_suppress, fade_io, framewise_dominance_gate, stft_wiener_refine, tf_mask_refine

# åŸºæœ¬éŒ„éŸ³åƒæ•¸ï¼ˆå¾é…ç½®è®€å–ï¼‰
CHUNK = AUDIO_CHUNK_SIZE
FORMAT = pyaudio.paFloat32
CHANNELS = AUDIO_CHANNELS
RATE = AUDIO_RATE
TARGET_RATE = AUDIO_TARGET_RATE
WINDOW_SIZE = AUDIO_WINDOW_SIZE
OVERLAP = AUDIO_OVERLAP
DEVICE_INDEX = None

# è™•ç†åƒæ•¸ï¼ˆå¾é…ç½®è®€å–ï¼‰
MIN_ENERGY_THRESHOLD = AUDIO_MIN_ENERGY_THRESHOLD
MAX_BUFFER_MINUTES = AUDIO_MAX_BUFFER_MINUTES

# éŸ³è¨Šè™•ç†åƒæ•¸
MIN_ENERGY_THRESHOLD = 0.001
NOISE_REDUCE_STRENGTH = 0.05  # é™ä½é™å™ªå¼·åº¦ä»¥ä¿æŒéŸ³è³ª
MAX_BUFFER_MINUTES = 5
SNR_THRESHOLD = 8  # é™ä½ SNR é–¾å€¼

# éŸ³è¨Šå“è³ªæ”¹å–„åƒæ•¸
WIENER_FILTER_STRENGTH = 0.01  # æ›´æº«å’Œçš„ç¶­ç´æ¿¾æ³¢
HIGH_FREQ_CUTOFF = 7500  # æé«˜é«˜é »æˆªæ­¢é»
DYNAMIC_RANGE_COMPRESSION = 0.7  # å‹•æ…‹ç¯„åœå£“ç¸®


DEFAULT_MODEL = DEFAULT_SEPARATION_MODEL

# ä¿®æ­£ DEFAULT_MODEL çš„è³¦å€¼
if DEFAULT_SEPARATION_MODEL == "sepformer_2speaker":
    DEFAULT_MODEL = SeparationModel.SEPFORMER_2SPEAKER
elif DEFAULT_SEPARATION_MODEL == "sepformer_3speaker":
    DEFAULT_MODEL = SeparationModel.SEPFORMER_3SPEAKER
else:
    DEFAULT_MODEL = SeparationModel.SEPFORMER_3SPEAKER  # é è¨­å€¼æ”¹ç‚ºæ‚¨çš„æ¨¡å‹

MODEL_NAME = MODEL_CONFIGS[DEFAULT_MODEL]["model_name"]
NUM_SPEAKERS = MODEL_CONFIGS[DEFAULT_MODEL]["num_speakers"]


# è¼¸å‡ºç›®éŒ„
OUTPUT_DIR = "R3SI/Audio-storage"  # å„²å­˜åˆ†é›¢å¾ŒéŸ³è¨Šçš„ç›®éŒ„
IDENTIFIED_DIR = "R3SI/Identified-Speakers"

# åˆå§‹åŒ–æ—¥èªŒç³»çµ±
logger = get_logger(__name__)

# ç¢ºä¿æ•´å€‹æ¨¡çµ„çš„ DEBUG è¨Šæ¯æœƒå°å‡ºï¼ˆå«æ‰€æœ‰ handlerï¼‰
# import logging
# logger.setLevel(logging.DEBUG)
# for h in logger.handlers:
#     try:
#         h.setLevel(logging.DEBUG)
#     except Exception:
#         pass

# åœ¨æª”æ¡ˆé ‚éƒ¨æ·»åŠ å…¨åŸŸå¿«å–
_GLOBAL_SEPARATOR_CACHE = {}
_GLOBAL_SPEAKER_PIPELINE_CACHE = None

# ================== èªè€…åˆ†é›¢é¡åˆ¥ ======================

class AudioSeparator:
    def __init__(self, model_type: SeparationModel = DEFAULT_MODEL, enable_noise_reduction=True, snr_threshold=SNR_THRESHOLD, enable_dynamic_model=True):
        # è¨­å‚™é¸æ“‡é‚è¼¯ï¼šå„ªå…ˆè€ƒæ…® FORCE_CPU è¨­å®š
        if FORCE_CPU:
            self.device = "cpu"
            logger.info("ğŸ”§ FORCE_CPU=trueï¼Œå¼·åˆ¶ä½¿ç”¨ CPU é‹ç®—")
        else:
            if torch.cuda.is_available():
                # æª¢æŸ¥æŒ‡å®šçš„ CUDA è¨­å‚™æ˜¯å¦å­˜åœ¨
                if CUDA_DEVICE_INDEX < torch.cuda.device_count():
                    self.device = f"cuda:{CUDA_DEVICE_INDEX}"
                    # ç¢ºä¿è¨­å®šæ­£ç¢ºçš„è¨­å‚™
                    torch.cuda.set_device(CUDA_DEVICE_INDEX)
                    if CUDA_DEVICE_INDEX != 0:
                        logger.info(f"ğŸ¯ ä½¿ç”¨æŒ‡å®šçš„ CUDA è¨­å‚™: {CUDA_DEVICE_INDEX}")
                else:
                    logger.warning(f"âš ï¸  æŒ‡å®šçš„ CUDA è¨­å‚™ç´¢å¼• {CUDA_DEVICE_INDEX} ä¸å­˜åœ¨ï¼Œæ”¹ç”¨ cuda:0")
                    self.device = "cuda:0"
                    torch.cuda.set_device(0)
            else:
                self.device = "cpu"
                logger.info("ğŸ–¥ï¸  æœªåµæ¸¬åˆ° GPU è¨­å‚™ï¼Œä½¿ç”¨ CPU é‹ç®—")
                
        self.model_type = model_type
        self.model_config = MODEL_CONFIGS[model_type]
        self.num_speakers = self.model_config["num_speakers"]
        
        # é—œé–‰é™å™ªåŠŸèƒ½ä»¥ä¿æŒåŸå§‹éŸ³è³ª
        self.enable_noise_reduction = enable_noise_reduction  # å¼·åˆ¶é—œé–‰ä»¥ä¿æŒéŸ³è³ªä¸€è‡´æ€§
        self.snr_threshold = snr_threshold
        
        logger.info(f"ä½¿ç”¨è¨­å‚™: {self.device}")
        logger.info(f"æ¨¡å‹é¡å‹: {model_type.value}")
        logger.info(f"è¼‰å…¥æ¨¡å‹: {self.model_config['model_name']}")
        logger.info(f"æ”¯æ´èªè€…æ•¸é‡: {self.num_speakers}")
        
        # è¨­è¨ˆæ›´æº«å’Œçš„ä½é€šæ¿¾æ³¢å™¨
        nyquist = TARGET_RATE // 2
        cutoff = min(HIGH_FREQ_CUTOFF, nyquist - 100)
        self.lowpass_filter = signal.butter(2, cutoff / nyquist, btype='low', output='sos')
        
        # æ–°å¢å‹•æ…‹æ¨¡å‹ç®¡ç†å™¨ç›¸é—œå±¬æ€§
        self.enable_dynamic_model = enable_dynamic_model
        
        if self.enable_dynamic_model:
            # ä½¿ç”¨å‹•æ…‹æ¨¡å‹ç®¡ç†å™¨
            self.model_manager = create_dynamic_model_manager(self.device)
            logger.info("å•Ÿç”¨å‹•æ…‹æ¨¡å‹é¸æ“‡æ©Ÿåˆ¶")
            
            # é è¼‰å…¥é è¨­æ¨¡å‹
            self.model_manager.preload_model(model_type)
            self.model, self.current_model_type = self.model_manager.get_model_for_speakers(self.num_speakers)
        else:
            # ä½¿ç”¨å›ºå®šæ¨¡å‹ï¼ˆåŸæœ‰é‚è¼¯ï¼‰
            self.model_manager = None
            self.current_model_type = model_type
            try:
                logger.info("æ­£åœ¨è¼‰å…¥æ¨¡å‹...")
                self.model = self._load_model()
                logger.info("æ¨¡å‹è¼‰å…¥å®Œæˆ")
                self._test_model()
            except Exception as e:
                logger.error(f"æ¨¡å‹è¼‰å…¥å¤±æ•—: {e}")
                raise
        
        # å–®äººæƒ…å¢ƒï¼šè‡ªå‹•é¸è²é“å™¨ï¼ˆV2 åƒæ•¸ï¼‰
        self.single_selector = SingleSpeakerSelector(
            sr=TARGET_RATE,          # ä¾‹å¦‚ 16000ï¼Œè«‹ç¢ºä¿è·Ÿä½ è™•ç†éŸ³æª”çš„å¯¦éš›æ¡æ¨£ç‡ä¸€è‡´
            frame_ms=20,
            hop_ms=10,
            alpha=1.5,               # èƒ½é‡å¼ VAD é–€æª»ï¼ˆåƒ…ç”¨æ–¼çµ±è¨ˆ/ç‰¹å¾µçš„ gatingï¼‰
            min_rms=1e-6,

            # é€™å››å€‹æ˜¯æ–°ç‰ˆçš„æ¬Šé‡åç¨±
            w_sisdr=0.60,            # ä¸»ç‰¹å¾µï¼šå° mix çš„ SI-SDRï¼ˆæŠ•å½±åˆ†æ•¸ï¼‰
            w_band=0.25,             # 300â€“3400 Hz äººè²é »å¸¶èƒ½é‡ä½”æ¯”
            w_tonality=0.15,         # 1 - spectral flatness
            w_zcr_penalty=0.10,      # é›¶äº¤è¶Šç‡æ‡²ç½°ï¼ˆè¶Šé«˜è¶Šæ‡²ç½°ï¼‰

            tie_tol=0.02,
        )
        
        try:
            self.resampler = torchaudio.transforms.Resample(
                orig_freq=RATE,
                new_freq=TARGET_RATE
            ).to(self.device)
        except Exception as e:
            logger.error(f"é‡æ–°å–æ¨£å™¨åˆå§‹åŒ–å¤±æ•—: {e}")
            raise
        
        self.executor = ThreadPoolExecutor(max_workers=API_MAX_WORKERS)
        self.futures = []
        self.is_recording = False
        self.output_files = []  # å„²å­˜åˆ†é›¢å¾Œçš„éŸ³æª”è·¯å¾‘
        self.save_audio_files = True  # è¨­å®š: æ˜¯å¦å°‡åˆ†é›¢å¾Œçš„éŸ³è¨Šå„²å­˜ç‚ºwavæª”æ¡ˆ
        
        # æ™‚é–“è¿½è¹¤ç›¸é—œè®Šæ•¸
        self.session_start_time = None  # è¨˜éŒ„ session é–‹å§‹çš„çµ•å°æ™‚é–“
        self._current_t0 = 0.0  # ç´¯è¨ˆç›¸å°æ™‚é–“
        
        # è™•ç†çµ±è¨ˆ
        self.processing_stats = {
            'segments_processed': 0,
            'segments_skipped': 0,
            'errors': 0
        }
        
        self.max_buffer_size = int(RATE * MAX_BUFFER_MINUTES * 60 / CHUNK)

        # åˆå§‹åŒ–èªè€…è¨ˆæ•¸ç®¡ç·š
        self._init_speaker_count_pipeline()

        # æ”¹ç”¨ç¨ç«‹é¡åˆ¥é›†ä¸­ç®¡ç†èªè€…æ•¸é‡åµæ¸¬ï¼Œä¸¦å‚³å…¥å¿«å–çš„ç®¡ç·š
        self.spk_counter = SpeakerCounter(
            hf_token=HF_ACCESS_TOKEN, 
            device=self.device, 
            pipeline=getattr(self, 'speaker_count_pipeline', None),
            logger=logger
        )
        logger.info("èªè€…è¨ˆæ•¸å™¨åˆå§‹åŒ–å®Œæˆ")
        
        self._last_single_route_idx = None
        self._last_single_route_score = None
        
        logger.info("AudioSeparator åˆå§‹åŒ–å®Œæˆ")

    def _init_speaker_count_pipeline(self):
        """åˆå§‹åŒ–èªè€…è¨ˆæ•¸ç®¡ç·š - ä½¿ç”¨å…¨åŸŸå¿«å–"""
        global _GLOBAL_SPEAKER_PIPELINE_CACHE
        
        try:
            # æª¢æŸ¥æ˜¯å¦å·²æœ‰å…¨åŸŸå¿«å–çš„ç®¡ç·š
            if _GLOBAL_SPEAKER_PIPELINE_CACHE is not None:
                self.speaker_count_pipeline = _GLOBAL_SPEAKER_PIPELINE_CACHE
                logger.info("ä½¿ç”¨å¿«å–çš„èªè€…è¨ˆæ•¸ç®¡ç·š")
                return
            
            if HF_ACCESS_TOKEN:
                pipeline = Pipeline.from_pretrained(
                    "pyannote/speaker-diarization-3.1", 
                    use_auth_token=HF_ACCESS_TOKEN
                )
                # å°‡ç®¡ç·šç§»åˆ°ç›¸åŒè¨­å‚™
                if hasattr(self, 'device'):
                    pipeline.to(torch.device(self.device))
                
                # å¿«å–åˆ°å…¨åŸŸè®Šæ•¸
                _GLOBAL_SPEAKER_PIPELINE_CACHE = pipeline
                self.speaker_count_pipeline = pipeline
                logger.info("èªè€…è¨ˆæ•¸ç®¡ç·šè¼‰å…¥ä¸¦å¿«å–æˆåŠŸ")
            else:
                logger.warning("æœªæä¾› HF_ACCESS_TOKENï¼Œèªè€…è¨ˆæ•¸åŠŸèƒ½å°‡å—é™")
                self.speaker_count_pipeline = None
        except Exception as e:
            logger.warning(f"èªè€…è¨ˆæ•¸ç®¡ç·šè¼‰å…¥å¤±æ•—: {e}")
            self.speaker_count_pipeline = None

    def _load_model(self):
        """è¼‰å…¥èªè€…åˆ†é›¢æ¨¡å‹"""
        model_name = self.model_config["model_name"]
        
        # ä½¿ç”¨ SpeechBrain SepFormer æ¨¡å‹
        try:
            logger.info(f"è¼‰å…¥ SpeechBrain æ¨¡å‹: {model_name}")
            
            # æª¢æŸ¥æœ¬åœ°æ¨¡å‹ç›®éŒ„æ˜¯å¦åŒ…å«ç„¡æ•ˆçš„ç¬¦è™Ÿé€£çµ
            local_model_path = os.path.abspath(f"models/{self.model_type.value}")
            if os.path.exists(local_model_path):
                logger.info(f"æª¢æŸ¥æœ¬åœ°æ¨¡å‹è·¯å¾‘: {local_model_path}")
                
                # æª¢æŸ¥æ˜¯å¦æœ‰ç„¡æ•ˆçš„ç¬¦è™Ÿé€£çµ (Windows JUNCTION æŒ‡å‘ Linux è·¯å¾‘)
                hyperparams_file = os.path.join(local_model_path, "hyperparams.yaml")
                if os.path.exists(hyperparams_file):
                    try:
                        # æ¸¬è©¦æª”æ¡ˆè®€å–æ¬Šé™
                        with open(hyperparams_file, 'r', encoding='utf-8') as f:
                            content = f.read(100)  # è®€å–å‰100å€‹å­—ç¬¦ä¾†æ¸¬è©¦
                        logger.info("æœ¬åœ°æ¨¡å‹æª”æ¡ˆè®€å–æ­£å¸¸")
                    except (PermissionError, OSError, UnicodeDecodeError) as e:
                        logger.warning(f"æœ¬åœ°æ¨¡å‹æª”æ¡ˆç„¡æ³•è®€å–: {e}")
                        logger.info("åµæ¸¬åˆ°ç„¡æ•ˆçš„ç¬¦è™Ÿé€£çµï¼Œæº–å‚™é‡æ–°ä¸‹è¼‰æ¨¡å‹...")
                        
                        # åˆªé™¤åŒ…å«ç„¡æ•ˆç¬¦è™Ÿé€£çµçš„ç›®éŒ„
                        try:
                            import shutil
                            shutil.rmtree(local_model_path, ignore_errors=True)
                            logger.info(f"å·²åˆªé™¤ç„¡æ•ˆçš„æ¨¡å‹ç›®éŒ„: {local_model_path}")
                        except Exception as rm_error:
                            logger.warning(f"åˆªé™¤æ¨¡å‹ç›®éŒ„æ™‚ç™¼ç”ŸéŒ¯èª¤: {rm_error}")
            
            # å˜—è©¦è¼‰å…¥æ¨¡å‹ (å¦‚æœæœ¬åœ°æª”æ¡ˆç„¡æ•ˆï¼ŒSpeechBrain æœƒè‡ªå‹•é‡æ–°ä¸‹è¼‰)
            model = separator.from_hparams(
                source=model_name,
                savedir=os.path.abspath(f"models/{self.model_type.value}"),
                run_opts={"device": self.device}
            )
            logger.info("SpeechBrain æ¨¡å‹è¼‰å…¥æˆåŠŸ")
            return model
            
        except Exception as e:
            logger.error(f"SpeechBrain æ¨¡å‹è¼‰å…¥å¤±æ•—: {e}")
            
            # æœ€å¾Œå˜—è©¦ï¼šå¼·åˆ¶é‡æ–°ä¸‹è¼‰
            try:
                logger.info("å˜—è©¦å¼·åˆ¶é‡æ–°ä¸‹è¼‰æ¨¡å‹...")
                import shutil
                local_model_path = os.path.abspath(f"models/{self.model_type.value}")
                if os.path.exists(local_model_path):
                    shutil.rmtree(local_model_path, ignore_errors=True)
                    logger.info("å·²æ¸…é™¤æœ¬åœ°æ¨¡å‹å¿«å–")
                
                model = separator.from_hparams(
                    source=model_name,
                    savedir=os.path.abspath(f"models/{self.model_type.value}"),
                    run_opts={"device": self.device}
                )
                logger.info("å¼·åˆ¶é‡æ–°ä¸‹è¼‰å¾Œï¼Œæ¨¡å‹è¼‰å…¥æˆåŠŸ")
                return model
                
            except Exception as final_error:
                logger.error(f"æ‰€æœ‰è¼‰å…¥å˜—è©¦å‡å¤±æ•—: {final_error}")
                raise Exception(f"æ¨¡å‹è¼‰å…¥å®Œå…¨å¤±æ•—ã€‚è«‹æª¢æŸ¥ç¶²è·¯é€£ç·šå’Œæ¨¡å‹å¯ç”¨æ€§ã€‚åŸå§‹éŒ¯èª¤: {e}")

    def _test_model(self):
        """æ¸¬è©¦æ¨¡å‹"""
        try:
            with torch.no_grad():
                # SpeechBrain SepFormer æ¨¡å‹æ¸¬è©¦
                # SepFormer æœŸæœ›è¼¸å…¥æ ¼å¼ç‚º [batch, samples]
                test_audio = torch.randn(1, AUDIO_SAMPLE_RATE).to(self.device)
                logger.debug(f"SpeechBrain æ¸¬è©¦éŸ³è¨Šå½¢ç‹€: {test_audio.shape}")
                output = self.model.separate_batch(test_audio)
                    
            logger.info("æ¨¡å‹æ¸¬è©¦é€šé")
            logger.debug(f"è¼¸å‡ºå½¢ç‹€: {output.shape if hasattr(output, 'shape') else type(output)}")
            
        except Exception as e:
            logger.error(f"æ¨¡å‹æ¸¬è©¦å¤±æ•—: {e}")
            logger.error(f"æ¸¬è©¦éŸ³è¨Šå½¢ç‹€: {test_audio.shape if 'test_audio' in locals() else 'N/A'}")
            raise
        
    def _infer_layout(self, est: torch.Tensor) -> tuple[str, int, int]:
        """
        å›å‚³ (layout, spk_axis, time_axis)
        layout âˆˆ {'BST','BTS'}ï¼›BST è¡¨ç¤º [B, S, T]ã€BTS è¡¨ç¤º [B, T, S]
        """
        assert est.dim() == 3, f"unexpected est shape: {tuple(est.shape)}"
        B, D1, D2 = est.shape[0], est.shape[1], est.shape[2]
        # å“ªå€‹ç¶­åº¦åƒã€Œèªªè©±è€…ã€? ï¼ˆå¾ˆå°ä¸”åœ¨ 1-4 ä¹‹é–“ï¼‰
        if 1 <= D1 <= 4 and not (1 <= D2 <= 4):
            return "BST", 1, 2  # [B, S, T]
        if 1 <= D2 <= 4 and not (1 <= D1 <= 4):
            return "BTS", 2, 1  # [B, T, S]
        # éƒ½åƒæˆ–éƒ½ä¸åƒï¼šåå¥½ [B, S, T]
        return "BST", 1, 2

    def _normalize_estimates(self, est: torch.Tensor) -> tuple[torch.Tensor, str, int, int]:
        """
        å°æ¯å€‹èªªè©±è€…ã€Œæ²¿æ™‚é–“è»¸ã€åš peak normalizeï¼ˆå¸¸æ•¸ç¸®æ”¾ï¼‰ï¼Œé¿å…æ™‚é–“é»ä¾è³´çš„å¤±çœŸã€‚
        å›å‚³ (normalized, layout, spk_axis, time_axis)
        """
        if est.dim() == 2:
            peak = est.abs().amax(dim=-1, keepdim=True).clamp_min(1e-8)
            return est / peak, "BT", -1, -1
        layout, s_ax, t_ax = self._infer_layout(est)
        peak = est.abs().amax(dim=t_ax, keepdim=True).clamp_min(1e-8)
        return est / peak, layout, s_ax, t_ax

    def _get_appropriate_model(self, num_speakers: int) -> tuple[separator, SeparationModel]:
        """
        å–å¾—é©ç•¶çš„æ¨¡å‹å¯¦ä¾‹
        
        Args:
            num_speakers: åµæ¸¬åˆ°çš„èªè€…æ•¸é‡
            
        Returns:
            tuple: (æ¨¡å‹å¯¦ä¾‹, æ¨¡å‹é¡å‹)
        """
        if self.enable_dynamic_model and self.model_manager:
            return self.model_manager.get_model_for_speakers(num_speakers)
        else:
            # å›ºå®šæ¨¡å‹æ¨¡å¼
            return self.model, self.current_model_type

    def estimate_snr(self, signal):
        """ä¼°ç®—ä¿¡è™Ÿé›œè¨Šæ¯”"""
        try:
            signal_power = np.mean(signal ** 2)
            if len(signal) > 1000:
                noise_estimate = np.std(signal[-1000:]) ** 2
            else:
                noise_estimate = np.std(signal) ** 2 * 0.1
            noise_estimate = max(noise_estimate, 1e-10)
            snr = 10 * np.log10(signal_power / noise_estimate)
            return snr
        except:
            return 0

    def wiener_filter(self, audio_signal):
        """ç¶­ç´æ¿¾æ³¢å™¨ - æ›´æº«å’Œçš„è™•ç†"""
        try:
            f, t, stft = signal.stft(audio_signal, fs=TARGET_RATE, nperseg=512, noverlap=256)
            
            # ä½¿ç”¨æ›´æº«å’Œçš„é›œè¨Šä¼°è¨ˆ
            quiet_samples = min(int(TARGET_RATE * 0.05), len(audio_signal) // 8)
            noise_sample = audio_signal[:quiet_samples]
            _, _, noise_stft = signal.stft(noise_sample, fs=TARGET_RATE, nperseg=512, noverlap=256)
            noise_power = np.mean(np.abs(noise_stft) ** 2, axis=1, keepdims=True)
            
            signal_power = np.abs(stft) ** 2
            wiener_gain = signal_power / (signal_power + WIENER_FILTER_STRENGTH * noise_power)
            
            # é™åˆ¶å¢ç›Šç¯„åœä»¥é¿å…éåº¦è™•ç†
            wiener_gain = np.clip(wiener_gain, 0.1, 1.0)
            
            filtered_stft = stft * wiener_gain
            _, filtered_audio = signal.istft(filtered_stft, fs=TARGET_RATE)
            
            return filtered_audio[:len(audio_signal)]
        except:
            return audio_signal

    def smooth_audio(self, audio_signal):
        """éŸ³è¨Šå¹³æ»‘è™•ç†"""
        try:
            # ç§»é™¤çªç„¶çš„è·³ç–Š
            diff = np.diff(audio_signal)
            threshold = np.std(diff) * 3  # æ›´å¯¬é¬†çš„é–¾å€¼
            artifact_indices = np.where(np.abs(diff) > threshold)[0]
            
            for idx in artifact_indices[:20]:  # é™åˆ¶è™•ç†æ•¸é‡
                if 0 < idx < len(audio_signal) - 1:
                    audio_signal[idx] = (audio_signal[idx-1] + audio_signal[idx+1]) / 2
            
            # è¼•å¾®å¹³æ»‘
            audio_signal = uniform_filter1d(audio_signal, size=3)
            
            # è¼•å¾®ä½é€šæ¿¾æ³¢
            audio_signal = signal.sosfilt(self.lowpass_filter, audio_signal)
            
            return audio_signal
        except:
            return audio_signal

    def dynamic_range_compression(self, audio_signal):
        """å‹•æ…‹ç¯„åœå£“ç¸®"""
        try:
            # è»Ÿé™åˆ¶å™¨
            threshold = 0.8
            ratio = DYNAMIC_RANGE_COMPRESSION
            
            # è¨ˆç®—çµ•å°å€¼
            abs_signal = np.abs(audio_signal)
            
            # å°è¶…éé–¾å€¼çš„éƒ¨åˆ†é€²è¡Œå£“ç¸®
            mask = abs_signal > threshold
            compressed = np.copy(audio_signal)
            
            if np.any(mask):
                over_threshold = abs_signal[mask]
                compressed_magnitude = threshold + (over_threshold - threshold) * ratio
                compressed[mask] = np.sign(audio_signal[mask]) * compressed_magnitude
            
            return compressed
        except:
            return audio_signal

    def spectral_gating(self, audio):
        """æ”¹è‰¯çš„é »è­œé–˜æ§é™å™ª"""
        try:
            noise_sample_length = max(int(TARGET_RATE * 0.05), 1)
            noise_sample = audio[:noise_sample_length]
            
            return nr.reduce_noise(
                y=audio,
                y_noise=noise_sample,
                sr=TARGET_RATE,
                prop_decrease=NOISE_REDUCE_STRENGTH,
                stationary=False,  # éç©©æ…‹é›œè¨Šè™•ç†
                n_jobs=1
            )
        except:
            return audio

    def enhance_separation(self, separated_signals):
        """å¢å¼·åˆ†é›¢æ•ˆæœ - æ”¹å–„éŸ³è³ª"""
        if not self.enable_noise_reduction:
            return separated_signals
        
        # SpeechBrain æ¨¡å‹è¼¸å‡ºæ ¼å¼è™•ç†
        if len(separated_signals.shape) == 3:
            # æ ¼å¼é€šå¸¸ç‚º [batch, time, speakers]
            enhanced_signals = torch.zeros_like(separated_signals)
            speaker_dim = 2
            time_dim = 1
        else:
            separated_signals = separated_signals.unsqueeze(0)
            enhanced_signals = torch.zeros_like(separated_signals)
            speaker_dim = 2
            time_dim = 1
        
        num_speakers = separated_signals.shape[speaker_dim]
        
        for i in range(min(num_speakers, self.num_speakers)):
            if speaker_dim == 2:
                current_signal = separated_signals[0, :, i].cpu().numpy()
            else:
                current_signal = separated_signals[0, :, i].cpu().numpy()
            
            # å¤šéšæ®µéŸ³è³ªæ”¹å–„
            processed_signal = current_signal
            
            # 1. ç¶­ç´æ¿¾æ³¢
            signal_snr = self.estimate_snr(current_signal)
            if signal_snr < self.snr_threshold + 3:
                processed_signal = self.wiener_filter(processed_signal)
            
            # 2. å‚³çµ±é™å™ªï¼ˆåƒ…åœ¨å¿…è¦æ™‚ï¼‰
            if signal_snr < self.snr_threshold:
                processed_signal = self.spectral_gating(processed_signal)
            
            # 3. éŸ³è¨Šå¹³æ»‘å’Œä¿®å¾©
            processed_signal = self.smooth_audio(processed_signal)
            
            # 4. å‹•æ…‹ç¯„åœå£“ç¸®
            processed_signal = self.dynamic_range_compression(processed_signal)
            
            # 5. æœ€çµ‚æ­£è¦åŒ–
            max_val = np.max(np.abs(processed_signal))
            if max_val > 0:
                processed_signal = processed_signal / max_val * 0.95
            
            length = min(len(processed_signal), separated_signals.shape[time_dim])
            
            if speaker_dim == 2:
                enhanced_signals[0, :length, i] = torch.from_numpy(processed_signal[:length]).to(self.device)
            else:
                enhanced_signals[0, :length, i] = torch.from_numpy(processed_signal[:length]).to(self.device)
        
        return enhanced_signals
        
    def set_save_audio_files(self, save: bool) -> None:
        """
        è¨­å®šæ˜¯å¦å„²å­˜åˆ†é›¢å¾Œçš„éŸ³è¨Šæª”æ¡ˆ
        
        Args:
            save: True è¡¨ç¤ºå„²å­˜éŸ³è¨Šæª”æ¡ˆï¼ŒFalse è¡¨ç¤ºä¸å„²å­˜
        """
        self.save_audio_files = save
        logger.info(f"éŸ³è¨Šæª”æ¡ˆå„²å­˜è¨­å®šï¼š{'å·²å•Ÿç”¨' if save else 'å·²åœç”¨'}")

    def process_audio(self, audio_data: np.ndarray) -> torch.Tensor:
        """è™•ç†éŸ³è¨Šæ ¼å¼ï¼šå°‡åŸå§‹éŒ„éŸ³è³‡æ–™è½‰æ›ç‚ºæ¨¡å‹å¯ç”¨çš„æ ¼å¼"""
        try:
            # è½‰æ›ç‚º float32
            if FORMAT == pyaudio.paInt16:
                audio_float = audio_data.astype(np.float32) / 32768.0
            else:
                audio_float = audio_data.astype(np.float32)
            
            # èƒ½é‡æª¢æ¸¬ï¼šéä½å‰‡ç•¥é
            energy = np.mean(np.abs(audio_float))
            if energy < MIN_ENERGY_THRESHOLD:
                logger.debug(f"éŸ³è¨Šèƒ½é‡ ({energy:.6f}) ä½æ–¼é–¾å€¼ ({MIN_ENERGY_THRESHOLD})")
                return None
            
            # é‡å¡‘ç‚ºæ­£ç¢ºå½¢ç‹€
            if len(audio_float.shape) == 1:
                audio_float = audio_float.reshape(-1, CHANNELS)

            # èª¿æ•´å½¢ç‹€ä»¥ç¬¦åˆæ¨¡å‹è¼¸å…¥ï¼š[channels, time]
            audio_tensor = torch.from_numpy(audio_float).T.float()

            # å¦‚æœæ˜¯é›™è²é“è€Œæ¨¡å‹åªæ”¯æ´å–®è²é“å‰‡å–å¹³å‡
            if audio_tensor.shape[0] == 2:
                audio_tensor = torch.mean(audio_tensor, dim=0, keepdim=True)

            # ç§»è‡³ GPU ä¸¦é‡æ–°å–æ¨£è‡³ 16kHz
            audio_tensor = audio_tensor.to(self.device)
            resampled = self.resampler(audio_tensor)
            
            # ç¢ºä¿å½¢ç‹€æ­£ç¢º
            if len(resampled.shape) == 1:
                resampled = resampled.unsqueeze(0)
            
            return resampled
            
        except Exception as e:
            logger.error(f"éŸ³è¨Šè™•ç†éŒ¯èª¤ï¼š{e}")
            self.processing_stats['errors'] += 1
            return None

    def cleanup_futures(self):
        """æ¸…ç†å·²å®Œæˆçš„ä»»å‹™"""
        completed_futures = []
        for future in self.futures:
            if future.done():
                try:
                    future.result()  # ç²å–çµæœä»¥æ•ç²ä»»ä½•ç•°å¸¸
                except Exception as e:
                    logger.error(f"è™•ç†ä»»å‹™ç™¼ç”ŸéŒ¯èª¤ï¼š{e}")
                    self.processing_stats['errors'] += 1
                completed_futures.append(future)
        
        # ç§»é™¤å·²å®Œæˆçš„ä»»å‹™
        for future in completed_futures:
            self.futures.remove(future)

    def record_and_process(self, output_dir):
        """éŒ„éŸ³ä¸¦è™•ç†éŸ³è¨Šçš„ä¸»è¦æ–¹æ³•"""
        # ç¢ºä¿è¼¸å‡ºç›®éŒ„å­˜åœ¨
        os.makedirs(output_dir, exist_ok=True)
        # å„²å­˜åŸå§‹æ··åˆéŸ³è¨Šçš„ç·©è¡å€
        mixed_audio_buffer = []
        
        try:
            # æ­¥é©Ÿ0: åˆå§‹åŒ–èªè€…è­˜åˆ¥å™¨
            # identifier = SpeakerIdentifier()

            # æ­¥é©Ÿ1: åˆå§‹åŒ–éŒ„éŸ³è£ç½®
            p = pyaudio.PyAudio()
            
            # æª¢æŸ¥è¨­å‚™å¯ç”¨æ€§
            if DEVICE_INDEX is not None:
                device_info = p.get_device_info_by_index(DEVICE_INDEX)
                logger.info(f"ä½¿ç”¨éŸ³è¨Šè¨­å‚™: {device_info['name']}")
            
            stream = p.open(
                format=FORMAT,
                channels=CHANNELS,
                rate=RATE,
                input=True,
                frames_per_buffer=CHUNK,
                input_device_index=DEVICE_INDEX
            )
            
            logger.info("é–‹å§‹éŒ„éŸ³...")
            
            # æ­¥é©Ÿ2: è¨ˆç®—ç·©è¡å€åƒæ•¸
            samples_per_window = int(WINDOW_SIZE * RATE)
            window_frames = int(samples_per_window / CHUNK)
            overlap_frames = int((OVERLAP * RATE) / CHUNK)
            slide_frames = window_frames - overlap_frames
            
            # åˆå§‹åŒ–è™•ç†è®Šæ•¸
            buffer = []
            segment_index = 0
            self.is_recording = True
            last_stats_time = time.time()
            
            # æ­¥é©Ÿ3: éŒ„éŸ³ä¸»å¾ªç’°
            while self.is_recording:
                try:
                    data = stream.read(CHUNK, exception_on_overflow=False)
                    frame = np.frombuffer(data, dtype=np.float32 if FORMAT == pyaudio.paFloat32 else np.int16)
                    buffer.append(frame)
                    
                    # é™åˆ¶ mixed_audio_buffer å¤§å°ä»¥é˜²æ­¢è¨˜æ†¶é«”è€—ç›¡
                    mixed_audio_buffer.append(frame.copy())
                    if len(mixed_audio_buffer) > self.max_buffer_size:
                        mixed_audio_buffer.pop(0)
                        
                except IOError as e:
                    logger.warning(f"éŒ„éŸ³æ™‚ç™¼ç”ŸIOéŒ¯èª¤ï¼š{e}")
                    continue
                
                # æ­¥é©Ÿ4: ç•¶ç´¯ç©è¶³å¤ è³‡æ–™æ™‚é€²è¡Œè™•ç†
                if len(buffer) >= window_frames:
                    segment_index += 1
                    audio_data = np.concatenate(buffer[:window_frames])
                    audio_tensor = self.process_audio(audio_data)
                    
                    # æ­¥é©Ÿ5: å¦‚æœéŸ³è¨Šæœ‰æ•ˆï¼Œå•Ÿå‹•èªè€…åˆ†é›¢è™•ç†
                    if audio_tensor is not None:
                        logger.info(f"è™•ç†ç‰‡æ®µ {segment_index}")
                        future = self.executor.submit(
                            self.separate_and_save,
                            audio_tensor,
                            output_dir,
                            segment_index
                        )
                        self.futures.append(future)
                        self.processing_stats['segments_processed'] += 1
                    else:
                        self.processing_stats['segments_skipped'] += 1
                    
                    # æ­¥é©Ÿ6: ç§»å‹•è¦–çª—
                    buffer = buffer[slide_frames:]
                    
                    # å®šæœŸæ¸…ç†å·²å®Œæˆçš„ä»»å‹™
                    if segment_index % 10 == 0:
                        self.cleanup_futures()
                    
                    # æ¯30ç§’å ±å‘Šä¸€æ¬¡çµ±è¨ˆè³‡è¨Š
                    current_time = time.time()
                    if current_time - last_stats_time > 30:
                        self._log_statistics()
                        last_stats_time = current_time
                    
        except Exception as e:
            logger.error(f"éŒ„éŸ³éç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤ï¼š{e}")
        finally:
            # æ­¥é©Ÿ7: æ¸…ç†è³‡æº
            self._cleanup_resources(p, stream, mixed_audio_buffer, output_dir)

    def _cleanup_resources(self, p, stream, mixed_audio_buffer, output_dir):
        """æ¸…ç†è³‡æº"""
        # åœæ­¢ä¸¦é—œé–‰éŸ³è¨Šæµ
        if stream is not None:
            try:
                stream.stop_stream()
                stream.close()
                logger.info("éŸ³è¨Šæµå·²é—œé–‰")
            except Exception as e:
                logger.error(f"é—œé–‰éŸ³è¨Šæµæ™‚ç™¼ç”ŸéŒ¯èª¤ï¼š{e}")
        
        if p is not None:
            try:
                p.terminate()
                logger.info("PyAudio å·²çµ‚æ­¢")
            except Exception as e:
                logger.error(f"çµ‚æ­¢ PyAudio æ™‚ç™¼ç”ŸéŒ¯èª¤ï¼š{e}")
        
        # ç­‰å¾…æ‰€æœ‰è™•ç†ä»»å‹™å®Œæˆ
        logger.info("ç­‰å¾…è™•ç†ä»»å‹™å®Œæˆ...")
        for future in self.futures:
            try:
                future.result(timeout=15.0)
            except Exception as e:
                logger.error(f"è™•ç†ä»»å‹™ç™¼ç”ŸéŒ¯èª¤ï¼š{e}")
        
        self.executor.shutdown(wait=True)
        logger.info("ç·šç¨‹æ± å·²é—œé–‰")
        
        # å„²å­˜åŸå§‹æ··åˆéŸ³è¨Š
        self._save_mixed_audio(mixed_audio_buffer, output_dir)
        
        # è¨˜éŒ„æœ€çµ‚çµ±è¨ˆ
        self._log_final_statistics()
        
        # æ¸…ç†æ¨¡å‹ç®¡ç†å™¨
        if self.model_manager:
            self.model_manager.cleanup()
        
        # æ¸…ç†èªè€…è¨ˆæ•¸ç®¡ç·š
        if hasattr(self, 'speaker_count_pipeline') and self.speaker_count_pipeline is not None:
            try:
                # æ¸…ç†ç®¡ç·šè³‡æº
                del self.speaker_count_pipeline
                self.speaker_count_pipeline = None
                logger.info("èªè€…è¨ˆæ•¸ç®¡ç·šå·²æ¸…ç†")
            except Exception as e:
                logger.error(f"æ¸…ç†èªè€…è¨ˆæ•¸ç®¡ç·šæ™‚ç™¼ç”ŸéŒ¯èª¤ï¼š{e}")
        
        # æ¸…ç†GPUè¨˜æ†¶é«”
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        
        logger.info("éŒ„éŸ³çµæŸï¼Œæ‰€æœ‰è³‡æºå·²æ¸…ç†")

    def _save_mixed_audio(self, mixed_audio_buffer, output_dir):
        """å„²å­˜æ··åˆéŸ³è¨Š"""
        if not mixed_audio_buffer:
            return ""
            
        try:
            mixed_audio = np.concatenate(mixed_audio_buffer)
            mixed_audio = mixed_audio.reshape(-1, CHANNELS)
            
            timestamp = datetime.now().strftime('%Y%m%d-%H_%M_%S')
            mixed_output_file = os.path.join(
                output_dir,
                f"mixed_audio_{timestamp}.wav"
            )
            
            mixed_tensor = torch.from_numpy(mixed_audio).T.float()
            torchaudio.save(
                mixed_output_file,
                mixed_tensor,
                RATE
            )
            logger.info(f"å·²å„²å­˜åŸå§‹æ··åˆéŸ³è¨Šï¼š{mixed_output_file}")
            return mixed_output_file
            
        except Exception as e:
            logger.error(f"å„²å­˜æ··åˆéŸ³è¨Šæ™‚ç™¼ç”ŸéŒ¯èª¤ï¼š{e}")
            return ""

    def _log_statistics(self):
        """è¨˜éŒ„çµ±è¨ˆè³‡è¨Š"""
        stats = self.processing_stats
        logger.info(f"çµ±è¨ˆ - å·²è™•ç†: {stats['segments_processed']}, "
                   f"å·²è·³é: {stats['segments_skipped']}, "
                   f"éŒ¯èª¤: {stats['errors']}, "
                   f"é€²è¡Œä¸­ä»»å‹™: {len(self.futures)}")

    def _log_final_statistics(self):
        """è¨˜éŒ„æœ€çµ‚çµ±è¨ˆè³‡è¨Š"""
        stats = self.processing_stats
        total = stats['segments_processed'] + stats['segments_skipped']
        if total > 0:
            success_rate = (stats['segments_processed'] / total) * 100
            logger.info(f"æœ€çµ‚çµ±è¨ˆ - ç¸½ç‰‡æ®µ: {total}, "
                       f"æˆåŠŸè™•ç†: {stats['segments_processed']} ({success_rate:.1f}%), "
                       f"è·³é: {stats['segments_skipped']}, "
                       f"éŒ¯èª¤: {stats['errors']}")

    def separate_and_save(self, audio_tensor, output_dir, segment_index, absolute_start_time=None):
        """
        åˆ†é›¢ä¸¦å„²å­˜éŸ³è¨Šï¼Œä¸¦å›å‚³ (path, start, end) åˆ—è¡¨ã€‚
        æµç¨‹ï¼šèªè€…è¨ˆæ•¸ â†’ å‹•æ…‹æ¨¡å‹é¸æ“‡ â†’ åˆ†é›¢ â†’ å¼·åŒ–(å¯é¸) â†’ å„²å­˜
        
        Args:
            audio_tensor: éŸ³è¨Šå¼µé‡
            output_dir: è¼¸å‡ºç›®éŒ„
            segment_index: ç‰‡æ®µç´¢å¼•
            absolute_start_time: éŸ³è¨Šçš„çµ•å°é–‹å§‹æ™‚é–“ï¼ˆdatetime ç‰©ä»¶ï¼‰
        """
        try:
            # å…ˆä»¥å¯¬é¬†ç¯„åœè·‘ä¸€æ¬¡ï¼Œä¸¦å¥—ç”¨é‡ç–Šæ„ŸçŸ¥å¾Œè™•ç†ï¼›è‹¥ä½ çš„æ‰¹æ¬¡ç¢ºå®šé›™äººï¼Œå¯è¨­ expected_min/max=2
            detected_speakers = self.spk_counter.count_with_refine(
                audio=audio_tensor,
                sample_rate=TARGET_RATE,
                expected_min=1,
                expected_max=3,
                first_pass_range=(1, 3),
                allow_zero=True,         # <== å…è¨±å›å‚³ 0ï¼ˆç„¡èªéŸ³ï¼‰
                debug=False
            )

            logger.info(f"ç‰‡æ®µ {segment_index} - åµæ¸¬åˆ° {detected_speakers} ä½èªªè©±è€…")
            
            # å‚™æ´ï¼šç¬¬ä¸€æ¬¡å› 0 â†’ åªæœ‰åœ¨ã€Œå¼·æœ‰è²ã€æ‰é‡è©¦ 1â€“2 äºº
            if detected_speakers == 0:
                ok, m = self.spk_counter._has_voice(audio_tensor, TARGET_RATE, return_metrics=True)
                # èˆ‡ SpeakerCounter åŒæ­¥æˆ–æ›´åš´çš„æ¢ä»¶
                strong_voice = ok and (m["voiced_ratio"] >= 0.12) and (m["voiced_union"] >= 0.50) and (m.get("loud_frac", 0.0) >= 0.05)
                if not strong_voice:
                    logger.info(f"ç‰‡æ®µ {segment_index} - ç„¡èªéŸ³/éçŸ­ï¼ˆratio={m['voiced_ratio']:.3f}, union={m['voiced_union']:.2f}s, loud={m.get('loud_frac',0.0):.3f}ï¼‰ï¼Œè·³é")
                    return []

                logger.warning(f"ç‰‡æ®µ {segment_index} - ç¬¬ä¸€æ¬¡åµæ¸¬ 0ï¼Œä½†èªéŸ³è·¡è±¡åå¼·ï¼Œå˜—è©¦ 1â€“2 äººé‡è©¦")
                retry = self.spk_counter.count_with_refine(
                    audio=audio_tensor, sample_rate=TARGET_RATE,
                    expected_min=1, expected_max=2,
                    first_pass_range=(1, 2),
                    allow_zero=False,           # å·²ç¢ºèªå¼·æœ‰è²ï¼Œå°±ä¸è¦å†å› 0
                    debug=False
                )
                detected_speakers = max(1, int(retry))
            
            # å‹•æ…‹é¸æ“‡æ¨¡å‹
            current_model, current_model_type = self._get_appropriate_model(detected_speakers)
            
            # ä½¿ç”¨å‹•æ…‹æ¨¡å‹ç®¡ç†å™¨å–å¾—æ¨¡å‹é…ç½®
            if self.model_manager:
                model_config = self.model_manager.get_model_config(current_model_type)
            else:
                model_config = MODEL_CONFIGS[current_model_type]
            
            logger.debug(f"ä½¿ç”¨æ¨¡å‹: {current_model_type.value} (åµæ¸¬èªè€…: {detected_speakers})")
            
            # è¨˜éŒ„çµ•å°æ™‚é–“æˆ³
            if absolute_start_time is None:
                from datetime import timezone, timedelta
                taipei_tz = timezone(timedelta(hours=8))
                absolute_start_time = datetime.now(taipei_tz)
            
            # åˆå§‹åŒ–ç´¯è¨ˆæ™‚é–“æˆ³
            current_t0 = getattr(self, "_current_t0", 0.0)
            results = []   # ç”¨ä¾†æ”¶ (path, start, end, absolute_timestamp)
            seg_duration = audio_tensor.shape[-1] / TARGET_RATE
            
            with torch.no_grad():
                
                # ç¢ºä¿è¼¸å…¥æ˜¯ [batch, samples] æ ¼å¼
                if len(audio_tensor.shape) == 3:
                    # å¦‚æœæ˜¯ [batch, channels, samples]ï¼Œéœ€è¦å»æ‰ channels ç¶­åº¦
                    if audio_tensor.shape[1] == 1:
                        audio_tensor = audio_tensor.squeeze(1)  # è®Šæˆ [batch, samples]
                
                # ä½¿ç”¨é¸å®šçš„æ¨¡å‹é€²è¡Œåˆ†é›¢
                separated = current_model.separate_batch(audio_tensor)
                separated, layout, spk_axis, time_axis = self._normalize_estimates(separated)
                
                # å–å¾—æ··éŸ³ï¼ˆåŸå§‹è¼¸å…¥ï¼‰ä¸€ç¶­æ³¢å½¢
                mix_wave = audio_tensor[0].detach().cpu()
                
                # 1) åˆ†é›¢å¾Œå…ˆä¸å‹•ï¼šä¿ç•™åŸå§‹ for è©•åˆ†/é¸è·¯ï¼ˆSI-SDR å°å¸¸æ•¸ç¸®æ”¾ä¸æ•æ„Ÿï¼‰
                raw_for_select = separated

                # 2) åƒ…ç‚ºæœ€å¾Œè¼¸å‡ºã€Œå¯é¸ã€åšå¼·åŒ–ï¼ˆä¸è¦ç”¨å¼·åŒ–å¾Œè¨Šè™Ÿåšä»»ä½•è©•åˆ†/é¸è·¯ï¼‰
                enhanced_separated = self.enhance_separation(separated) if self.enable_noise_reduction else separated

                # 3) æ¨æ–· layoutï¼Œçµ±ä¸€å–å¾—ã€Œæ¨¡å‹è¼¸å‡ºèªªè©±è€…æ•¸ã€èˆ‡å–ç‰‡å‡½å¼
                if layout == "BST":           # [B, S, T]
                    model_output_speakers = enhanced_separated.shape[spk_axis]
                    def _get_cand(idx):  return raw_for_select[0, idx, :].detach().cpu()
                    def _get_final(idx): return enhanced_separated[0, idx, :].detach().cpu()
                elif layout == "BTS":         # [B, T, S]
                    model_output_speakers = enhanced_separated.shape[2]
                    def _get_cand(idx):  return raw_for_select[0, :, idx].detach().cpu()
                    def _get_final(idx): return enhanced_separated[0, :, idx].detach().cpu()
                else:                         # "BT" â†’ å–®ä¸€è·¯è¼¸å‡ºï¼ˆç„¡æ³•åšå¤šè·¯é¸è·¯ï¼‰
                    model_output_speakers = 1
                    def _get_cand(idx):  return raw_for_select[0, :].detach().cpu()
                    def _get_final(idx): return enhanced_separated[0, :].detach().cpu()

                # â€”â€” å–®äººæƒ…å¢ƒï¼šç”¨ã€ŒåŸå§‹ã€å€™é¸åšé¸è·¯èˆ‡ SI-SDR åˆ†æ•¸ â€”â€”
                if detected_speakers == 1 and model_output_speakers >= 2:
                    candidates = [_get_cand(j) for j in range(model_output_speakers)]
                    try:
                        best_idx, best_tensor_raw, stats_list = self.single_selector.select(candidates, mix_wave, return_stats=True)
                        if stats_list is not None:
                            s = stats_list[best_idx]
                            logger.info(
                                f"1-spk é¸è·¯ï¼šspeaker{best_idx+1} | "
                                f"SI-SDR={s['si_sdr_db']:.2f} dB, band={s['band_ratio']:.2f}, "
                                f"tonality={s['tonality']:.2f}, zcr_penalty={s['zcr_penalty']:.2f}, rms={s['rms']:.4f}"
                            )
                        # çœŸçš„è¦è¼¸å‡ºæ™‚ï¼Œæ‰æ‹¿ã€ŒåŒä¸€ç´¢å¼•ã€çš„ enhancedï¼ˆæˆ–åŸå§‹ï¼Œè¦–è¨­å®šï¼‰
                        best_tensor = _get_final(best_idx)
                        enhanced_separated = best_tensor.unsqueeze(0).unsqueeze(-1).to(self.device)  # -> [1, T, 1]
                        model_output_speakers = 1
                        
                        if hasattr(self, "_last_single_route_idx") and self._last_single_route_idx is not None:
                            prev_idx = self._last_single_route_idx
                            prev_score = self._last_single_route_score if self._last_single_route_score is not None else -1e9
                            cur_score = s.get("score", s["si_sdr_db"])  # ä½ çš„ selector è‹¥æœ‰æ•´é«” score å°±ç”¨å®ƒï¼Œå¦å‰‡ç”¨ SI-SDR ä»£æ›¿
                            # è‹¥å…©è·¯åˆ†æ•¸å·®ç•°å¾ˆå°ï¼Œé–å®šä¸Šæ¬¡çš„è·¯å¾‘ï¼ˆé¿å…ä¾†å›è·³ï¼‰
                            if abs(cur_score - prev_score) < 0.03 and best_idx != prev_idx:
                                best_idx = prev_idx
                        
                        # æ›´æ–°è·¯å¾‘è¨˜æ†¶
                        self._last_single_route_idx = best_idx
                        self._last_single_route_score = s.get("score", s["si_sdr_db"])
                        
                    except Exception:
                        logger.exception("å–®äººé¸è·¯å¤±æ•—ï¼Œæ”¹ç”¨ speaker1 ä½œç‚ºä¿å®ˆè¼¸å‡º")
                        best_tensor = _get_final(0)
                        enhanced_separated = best_tensor.unsqueeze(0).unsqueeze(-1).to(self.device)
                        model_output_speakers = 1
                
                # æŠŠ estimates çµ±ä¸€æˆ [S, T] on CPU
                if enhanced_separated.ndim == 3:   # [B, T, S]
                    est_ST = enhanced_separated[0].transpose(0, 1).detach().cpu()  # [S, T]
                else:
                    est_ST = enhanced_separated.detach().cpu().unsqueeze(0)        # [1, T]

                # å–®äººæƒ…å¢ƒï¼šè‹¥æ¨¡å‹æœ‰ >=2 è·¯ï¼Œåƒ…ä¿ç•™è¢«é¸ä¸­çš„é‚£ä¸€è·¯ï¼ˆç”¨ä½ ä¸Šé¢ç®—å‡ºçš„ best_idxï¼‰
                if detected_speakers == 1 and est_ST.shape[0] >= 2:
                    try:
                        # ä½ ä¸Šé¢å·²ç¶“é¸å‡º best_idxï¼›é€™è£¡åªä¿ç•™é‚£ä¸€è·¯
                        est_ST = est_ST[best_idx:best_idx+1, :]
                    except Exception:
                        est_ST = est_ST[0:1, :]

                # === 1) Projection-backï¼šç”¨æ··éŸ³èƒ½é‡åšç·šæ€§é‡å®šæ¨™ï¼ˆè¶…å°æˆæœ¬ã€å¾ˆæœ‰æ•ˆï¼‰ ===
                x = mix_wave  # [T] CPU
                for s in range(est_ST.shape[0]):
                    y = est_ST[s]
                    denom = torch.dot(y, y).clamp_min(1e-8)
                    alpha = torch.dot(x, y) / denom
                    est_ST[s] = alpha * y

                # === 2) Mixture-consistent Wienerï¼ˆç¨å¾®éŠ³ä¸€é»ï¼Œä½†ä¸æ¿€é€²ï¼‰ ===
                if detected_speakers == 1:
                    est_ST = stft_wiener_refine(
                        est_ST, x,
                        n_fft=1024, hop=256, win_length=1024,
                        wiener_p=0.7
                    )
                # est_ST = stft_wiener_refine(
                #     est_ST, x,
                #     n_fft=1024, hop=256, win_length=1024,
                #     wiener_p=0.8
                # )

                # ï¼ˆå¯é¸ï¼‰å°å¤šäººå†åŠ ä¸€é»é»æ™‚é–“æ¡†ä¸»å°é–€æ§ï¼Œå£“å°æ¼éŸ³ï¼ˆå¦‚æœä½ æœ¬ä¾†å°±æœ‰ framewise_dominance_gateï¼Œå°±æ²¿ç”¨ï¼‰
                # if int(detected_speakers) >= 2:
                #     est_ST = stft_wiener_refine(
                #         est_ST, x,
                #         n_fft=1024, hop=256, win_length=1024,
                #         wiener_p=2.2
                #     )
                #     est_ST = framewise_dominance_gate(
                #         est_ST, frame=320, hop=160,
                #         rel_ratio=0.24,   # åŸå…ˆ 0.36ï¼›æ„ˆé«˜æ„ˆä¸åš´ï¼Œè¼ƒè‡ªç„¶
                #         min_floor=0.12,   # åŸå…ˆ 0.10ï¼›å¢Šé«˜ä¸€é»é¿å…ä¹¾è£‚èˆ‡æ²™ç ‚
                #         fade=240          # ç¨å¾®æ‹‰é•· crossfadeï¼Œé‚Šç•Œæ›´å¹³æ»‘
                #     )
                
                del separated
                if torch.cuda.is_available():
                    torch.cuda.empty_cache()
                
                timestamp = datetime.now().strftime('%Y%m%d-%H_%M_%S')
                
                # æ ¹æ“šå¯¦éš›æƒ…æ³æ±ºå®šè¦åˆ†é›¢å¤šå°‘å€‹èªè€…
                # ç­–ç•¥ï¼šä½¿ç”¨åµæ¸¬åˆ°çš„èªè€…æ•¸é‡ï¼Œä½†ä¸è¶…éæ¨¡å‹è¼¸å‡ºçš„é€šé“æ•¸
                # effective_speakers = min(detected_speakers, model_output_speakers, model_config["num_speakers"])
                # >>> FIX: ä»¥ est_ST çš„ S ç‚ºæº–ï¼›é¿å…ç”¨æ¨¡å‹åŸå§‹è¼¸å‡ºæ•¸æˆ–åµæ¸¬æ•¸å°è‡´ä¸ä¸€è‡´
                S, T = est_ST.shape
                effective_speakers = min(int(detected_speakers), int(S), int(model_config["num_speakers"]))
                
                logger.debug(
                    f"åˆ†é›¢åƒæ•¸ - åµæ¸¬: {detected_speakers}, "
                    f"est_STé€šé“: {S}, æ¨¡å‹æ”¯æ´: {model_config['num_speakers']}, æœ‰æ•ˆ: {effective_speakers}"
                )
                
                saved_count = 0
                start_time = current_t0
                
                for i in range(effective_speakers):
                    try:
                        # >>> FIX: est_ST æ˜¯ [S, T]ï¼Œæ­£ç¢ºå–æ³•ï¼š
                        speaker_audio = est_ST[i].contiguous()  # 1D [T]

                        fade_ms = 24.0 if int(detected_speakers) == 1 else 16.0
                        
                        # å…ˆåšæ·¡å…¥æ·¡å‡ºï¼Œæ¸›å°‘é‚Šç•Œå™ªé»
                        speaker_audio = fade_io(speaker_audio.clone(), TARGET_RATE, fade_ms=fade_ms)

                        # å‹•æ…‹ç¯„åœä¿è­·ï¼ˆåªåœ¨å¿…è¦æ™‚ç¸®æ”¾ï¼‰ï¼Œç„¶å¾Œ clamp
                        max_val = float(torch.max(torch.abs(speaker_audio)))
                        if max_val > 0.97:
                            speaker_audio = speaker_audio * (0.95 / max_val)
                        speaker_audio = speaker_audio.clamp_(-1.0, 1.0)

                        # èƒ½é‡é–€æª»ï¼ˆä¿å®ˆä¸€é»ï¼‰
                        rms = torch.sqrt(torch.mean(speaker_audio ** 2))
                        if rms <= 0.004:
                            logger.debug(f"èªè€… {i+1} èƒ½é‡å¤ªä½ (RMS={rms:.6f}), è·³éå„²å­˜")
                            continue
                    
                        id_audio = _prep_id_audio(speaker_audio, TARGET_RATE)   # â† ä¸€å¾‹èµ°é€™æ¢çµ¦èªè€…è¾¨è­˜
                        
                        final_tensor = id_audio.unsqueeze(0).cpu()  # [1, T]
                        final_tensor = _tpdf_dither(final_tensor, level_db=-92.0)
                        
                        # åœ¨ä¿å­˜å‰å¾Œã€æˆ–éŒ„éŸ³æ¨¡å¼æ¯æ®µè™•ç†å®Œï¼Œå¿«é€Ÿè©•ä¼°ä¸€æ¬¡
                        metrics = assess_audio_quality(id_audio, TARGET_RATE, logger=logger)
                        logger.info(f"å“è³ª {metrics['grade']}({metrics['quality_score']:.1f}) | "
                                    f"rms={metrics['rms_dbfs']:.1f}dBFS, snrâ‰ˆ{metrics['snr_db_est']:.1f}dB, "
                                    f"centroid={metrics['spectral_centroid_hz']:.0f}Hz, clip={metrics['clipping_pct']*100:.2f}%")

                        # è‹¥æœ‰åƒè€ƒè¨Šè™Ÿï¼ˆä¾‹å¦‚æ··éŸ³ï¼‰ï¼Œä¹Ÿå¯ä»¥ï¼š
                        metrics = assess_audio_quality(speaker_audio, TARGET_RATE, logger=logger, ref_wave=mix_wave)
                        logger.info(f"SI-SDR={metrics.get('si_sdr_db', float('nan')):.2f} dB")
                        
                        output_file = os.path.join(
                            output_dir,
                            f"speaker{i+1}.wav"
                            # f"speaker{i+1}_{timestamp}_{segment_index}.wav"
                        )
                        
                        # ä¿å­˜éŸ³è¨Šæ™‚ä½¿ç”¨è¼ƒé«˜çš„å“è³ªè¨­å®š
                        torchaudio.save(
                            output_file,
                            final_tensor,
                            TARGET_RATE,
                            bits_per_sample=16  # æŒ‡å®š16ä½å…ƒç¢ºä¿éŸ³è³ª
                        )
                        
                        # å¦å¤–å­˜ä¸€ä»½çµ¦äººè½ï¼ˆä¸å½±éŸ¿è¾¨è­˜æµç¨‹ï¼‰
                        if detected_speakers == 1 and getattr(self, "save_pretty_copy", False):
                            pretty_path = output_file.replace(".wav", "_pretty.wav")
                            pretty = _gentle_blend(speaker_audio, mix_wave, ratio=0.08)
                            torchaudio.save(pretty_path, pretty.unsqueeze(0).cpu(), TARGET_RATE, bits_per_sample=16)
                        
                        # è¨ˆç®—çµ•å°æ™‚é–“æˆ³
                        absolute_timestamp = absolute_start_time.timestamp() + start_time

                        results.append((output_file,
                                start_time,
                                start_time + seg_duration,
                                absolute_timestamp))  # åŠ å…¥çµ•å°æ™‚é–“æˆ³
                        self.output_files.append(output_file)
                        
                        saved_count += 1
                        
                    except Exception as e:
                        logger.warning(f"å„²å­˜èªè€… {i+1} å¤±æ•—: {e}")
                
                if saved_count > 0:
                    logger.info(f"ç‰‡æ®µ {segment_index} å®Œæˆï¼Œå„²å­˜ {saved_count}/{effective_speakers} å€‹æª”æ¡ˆ (ä½¿ç”¨ {current_model_type.value})")
                
            # æ›´æ–°ç´¯è¨ˆæ™‚é–“åˆ°ä¸‹ä¸€æ®µ
            current_t0 += seg_duration
            self._current_t0 = current_t0

            if not results:
                raise RuntimeError("Speaker separation produced no valid tracks")

            return results

        except Exception as e:
            logger.error(f"è™•ç†ç‰‡æ®µ {segment_index} å¤±æ•—: {e}")
            self.processing_stats['errors'] += 1
        finally:
            if torch.cuda.is_available():
                torch.cuda.empty_cache()

    def stop_recording(self):
        """åœæ­¢éŒ„éŸ³"""
        self.is_recording = False
        logger.info("æº–å‚™åœæ­¢éŒ„éŸ³...")

    def get_output_files(self):
        """ç²å–æ‰€æœ‰åˆ†é›¢å¾Œçš„éŸ³æª”è·¯å¾‘"""
        return self.output_files

# æ·»åŠ å…¨åŸŸå‡½å¼ä¾†ç®¡ç†å¿«å–
def get_cached_separator(model_type: SeparationModel = DEFAULT_MODEL, enable_dynamic_model: bool = True, **kwargs) -> AudioSeparator:
    """
    å–å¾—å¿«å–çš„ AudioSeparator å¯¦ä¾‹ï¼Œé¿å…é‡è¤‡åˆå§‹åŒ–
    
    Args:
        model_type: æ¨¡å‹é¡å‹
        enable_dynamic_model: æ˜¯å¦å•Ÿç”¨å‹•æ…‹æ¨¡å‹
        **kwargs: å…¶ä»–åƒæ•¸
    
    Returns:
        AudioSeparator å¯¦ä¾‹
    """
    global _GLOBAL_SEPARATOR_CACHE
    
    # å»ºç«‹å¿«å–éµ
    cache_key = f"{model_type.value}_{enable_dynamic_model}_{hash(tuple(sorted(kwargs.items())))}"
    
    # æª¢æŸ¥å¿«å–
    if cache_key in _GLOBAL_SEPARATOR_CACHE:
        logger.info(f"ä½¿ç”¨å¿«å–çš„ AudioSeparator: {cache_key}")
        return _GLOBAL_SEPARATOR_CACHE[cache_key]
    
    # å»ºç«‹æ–°å¯¦ä¾‹ä¸¦å¿«å–
    logger.info(f"å»ºç«‹æ–°çš„ AudioSeparator: {cache_key}")
    separator = AudioSeparator(
        model_type=model_type, 
        enable_dynamic_model=enable_dynamic_model, 
        **kwargs
    )
    _GLOBAL_SEPARATOR_CACHE[cache_key] = separator
    
    return separator

def clear_separator_cache():
    """æ¸…ç†æ‰€æœ‰å¿«å–çš„åˆ†é›¢å™¨å¯¦ä¾‹"""
    global _GLOBAL_SEPARATOR_CACHE, _GLOBAL_SPEAKER_PIPELINE_CACHE
    
    # æ¸…ç†åˆ†é›¢å™¨å¿«å–
    for separator in _GLOBAL_SEPARATOR_CACHE.values():
        try:
            if hasattr(separator, 'model_manager') and separator.model_manager:
                separator.model_manager.cleanup()
        except Exception as e:
            logger.warning(f"æ¸…ç†åˆ†é›¢å™¨æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
    
    _GLOBAL_SEPARATOR_CACHE.clear()
    
    # æ¸…ç†èªè€…è¨ˆæ•¸ç®¡ç·šå¿«å–
    if _GLOBAL_SPEAKER_PIPELINE_CACHE is not None:
        try:
            del _GLOBAL_SPEAKER_PIPELINE_CACHE
            _GLOBAL_SPEAKER_PIPELINE_CACHE = None
            logger.info("å·²æ¸…ç†èªè€…è¨ˆæ•¸ç®¡ç·šå¿«å–")
        except Exception as e:
            logger.warning(f"æ¸…ç†èªè€…è¨ˆæ•¸ç®¡ç·šå¿«å–æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")

def check_weaviate_connection() -> bool:
    """
    æª¢æŸ¥ Weaviate è³‡æ–™åº«é€£ç·šç‹€æ…‹ã€‚

    Returns:
        bool: è‹¥é€£ç·šæˆåŠŸå›å‚³ Trueï¼Œå¦å‰‡å›å‚³ Falseã€‚
    """
    try:
        import weaviate  # type: ignore
        client = weaviate.connect_to_local()
        # æª¢æŸ¥æ˜¯å¦èƒ½å­˜å–å¿…è¦é›†åˆ
        if not client.is_live():
            logger.error("Weaviate æœå‹™æœªå•Ÿå‹•æˆ–ç„¡æ³•å­˜å–ã€‚")
            return False
        if not (client.collections.exists("Speaker") and client.collections.exists("VoicePrint")):
            logger.error("Weaviate ç¼ºå°‘å¿…è¦é›†åˆ (Speaker æˆ– VoicePrint)ã€‚è«‹å…ˆåŸ·è¡Œ create_collections.pyã€‚")
            return False
        return True
    except Exception as e:
        logger.error(f"Weaviate é€£ç·šå¤±æ•—ï¼š{e}")
        return False
    
def run_realtime(output_dir: str = OUTPUT_DIR, model_type: SeparationModel = None, model_name: str = None, enable_dynamic_model: bool = True) -> str:
    """æ–¹ä¾¿å¤–éƒ¨å‘¼å«çš„éŒ„éŸ³è™•ç†å‡½å¼ï¼Œæ”¯æ´æ¨¡å‹é¸æ“‡å’Œå‹•æ…‹æ¨¡å‹ - ä½¿ç”¨å¿«å–"""
    if enable_dynamic_model:
        separator = get_cached_separator(model_type=DEFAULT_MODEL, enable_dynamic_model=True)
    elif model_name:
        # è½‰æ› model_name ç‚º model_type
        if model_name == "sepformer_2speaker":
            model_type = SeparationModel.SEPFORMER_2SPEAKER
        elif model_name == "sepformer_3speaker":
            model_type = SeparationModel.SEPFORMER_3SPEAKER
        else:
            raise ValueError(f"ä¸æ”¯æ´çš„æ¨¡å‹: {model_name}")
        separator = get_cached_separator(model_type=model_type, enable_dynamic_model=False)
    elif model_type:
        separator = get_cached_separator(model_type=model_type, enable_dynamic_model=False)
    else:
        separator = get_cached_separator(model_type=DEFAULT_MODEL, enable_dynamic_model=False)
    return separator.record_and_process(output_dir)

def run_offline(file_path: str, output_dir: str = OUTPUT_DIR, save_files: bool = True, 
                model_type: SeparationModel = None, model_name: str = None, enable_dynamic_model: bool = True) -> None:
    """æ–¹ä¾¿å¤–éƒ¨å‘¼å«çš„é›¢ç·šéŸ³æª”è™•ç†å‡½å¼ï¼Œæ”¯æ´æ¨¡å‹é¸æ“‡å’Œå‹•æ…‹æ¨¡å‹ - ä½¿ç”¨å¿«å–"""
    if enable_dynamic_model:
        separator = get_cached_separator(model_type=DEFAULT_MODEL, enable_dynamic_model=True)
    elif model_name:
        # è½‰æ› model_name ç‚º model_type
        if model_name == "sepformer_2speaker":
            model_type = SeparationModel.SEPFORMER_2SPEAKER
        elif model_name == "sepformer_3speaker":
            model_type = SeparationModel.SEPFORMER_3SPEAKER
        else:
            raise ValueError(f"ä¸æ”¯æ´çš„æ¨¡å‹: {model_name}")
        separator = get_cached_separator(model_type=model_type, enable_dynamic_model=False)
    elif model_type:
        separator = get_cached_separator(model_type=model_type, enable_dynamic_model=False)
    else:
        separator = get_cached_separator(model_type=DEFAULT_MODEL, enable_dynamic_model=False)
    separator.set_save_audio_files(save_files)
    separator.process_audio_file(file_path, output_dir)