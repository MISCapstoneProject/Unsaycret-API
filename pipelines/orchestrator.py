# pipelines/orchestrator.py
# åœ¨æ–‡ä»¶æœ€é¡¶éƒ¨
import torch, os
from utils.logger import get_logger

logger = get_logger(__name__)

logger.info("ğŸ–¥  GPU available: %s", torch.cuda.is_available())
if torch.cuda.is_available():
    logger.info("   Device: %s", torch.cuda.get_device_name(0))
# ç„¶å¾Œå† import ä½ çš„æ¨¡å—

import threading
import json, uuid, pathlib, datetime as dt
import torchaudio
from concurrent.futures import ThreadPoolExecutor
from modules.separation.separator import AudioSeparator
from modules.identification.VID_identify_v5 import SpeakerIdentifier
from modules.asr.whisper_asr import WhisperASR

# ---------- 1. è‡ªå‹•åµæ¸¬ GPU ----------
use_gpu = torch.cuda.is_available()
logger.info(f"ğŸš€ ä½¿ç”¨è¨­å‚™: {'cuda' if use_gpu else 'cpu'}")


sep = AudioSeparator()
spk = SpeakerIdentifier()
asr = WhisperASR(model_name="medium", gpu=use_gpu)

# ---------- 3. è™•ç†å–®ä¸€ç‰‡æ®µçš„å‡½å¼ ----------

def process_segment(seg_path, t0, t1):
    logger.info(f"ğŸ”§ åŸ·è¡Œç·’ {threading.get_ident()} æ­£åœ¨è™•ç† segment ({t0:.2f} - {t1:.2f})")

    speaker_id, name, dist = spk.process_audio_file(seg_path)
    text, conf, words       = asr.transcribe(seg_path)
    return {
        "start": round(t0, 2),
        "end":   round(t1, 2),
        "speaker": name,
        "distance": round(float(dist), 3),
        "text": text,
        "confidence": round(conf, 2),
        "words": words,
    }

# ---------- 4. ä¸» pipeline å‡½å¼ ----------
def make_pretty(seg: dict) -> dict:
    """æŠŠä¸€æ®µ segment è½‰æˆæ˜“è®€æ ¼å¼"""
    return {
        "time": f"{seg['start']:.2f}s â†’ {seg['end']:.2f}s",
        "speaker": seg["speaker"],
        "similarity": f"{seg['distance']:.3f}",
        "confidence": f"{seg['confidence']*100:.1f}%",
        "text": seg["text"],
        "word_count": len(seg["words"]),
    }

def run_pipeline(raw_wav: str, max_workers: int = 1):
    # (ä¿æŒå’Œä½ ä¸€æ¨£ï¼Œåªæœ‰ 1 æ¢åŸ·è¡Œç·’)
    waveform, sr = torchaudio.load(raw_wav)
    out_dir = pathlib.Path("work_output") / dt.datetime.now().strftime("%Y%m%d_%H%M%S")
    out_dir.mkdir(parents=True, exist_ok=True)

    segments = sep.separate_and_save(waveform, str(out_dir), segment_index=0)

    logger.info(f"ğŸ”„ è™•ç† {len(segments)} æ®µ... (max_workers={max_workers})")
    with ThreadPoolExecutor(max_workers=max_workers) as ex:
        bundle = list(ex.map(lambda s: process_segment(*s), segments))

    bundle.sort(key=lambda x: x["start"])

    # -------- æ–°å¢ prettified bundle --------
    pretty_bundle = [make_pretty(s) for s in bundle]

    json_path = out_dir / "output.json"
    with open(json_path, "w", encoding="utf-8") as f:
        json.dump({"segments": bundle}, f, ensure_ascii=False, indent=2)

    logger.info(f"âœ… Pipeline finished â†’ {json_path}")
    return bundle, pretty_bundle

if __name__ == "__main__":
    import sys
    run_pipeline(sys.argv[1])

