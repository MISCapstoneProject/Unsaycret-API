import argparse
import json
import os
import pathlib
import tempfile
import argparse
import json
import os
import pathlib
import tempfile
import threading
import time
import queue
from pathlib import Path
from datetime import datetime as dt
from concurrent.futures import ThreadPoolExecutor, Future

import torch
import torchaudio
import pyaudio  # type: ignore
import wave

from utils.logger import get_logger
from modules.separation.separator import AudioSeparator
from modules.identification.VID_identify_v5 import SpeakerIdentifier
from modules.asr.whisper_asr import WhisperASR

logger = get_logger(__name__)

logger.info("ğŸ–¥ GPU available: %s", torch.cuda.is_available())
if torch.cuda.is_available():
    logger.info("   Device: %s", torch.cuda.get_device_name(0))

logger = get_logger(__name__)

logger.info("ğŸ–¥ GPU available: %s", torch.cuda.is_available())
if torch.cuda.is_available():
    logger.info("   Device: %s", torch.cuda.get_device_name(0))

# ---------- 1. è‡ªå‹•åµæ¸¬ GPU ----------
use_gpu = torch.cuda.is_available()
logger.info(f"ğŸš€ ä½¿ç”¨è¨­å‚™: {'cuda' if use_gpu else 'cpu'}")

sep = AudioSeparator()
spk = SpeakerIdentifier()
asr = WhisperASR(model_name="medium", gpu=use_gpu)

def _timed_call(func, *args):
    t0 = time.perf_counter()
    res = func(*args)
    return res, time.perf_counter() - t0


def process_segment(seg_path: str, t0: float, t1: float) -> dict:
    """Process a single separated segment."""
    logger.info(
        f"ğŸ”§ åŸ·è¡Œç·’ {threading.get_ident()} è™•ç† ({t0:.2f}-{t1:.2f}) â†’ {os.path.basename(seg_path)}"
    )
    
    start = time.perf_counter()

    with ThreadPoolExecutor(max_workers=2) as ex:
        spk_future = ex.submit(_timed_call, spk.process_audio_file, seg_path)
        asr_future = ex.submit(_timed_call, asr.transcribe, seg_path)

        (speaker_id, name, dist), spk_time = spk_future.result()
        (text, conf, words), asr_time = asr_future.result()

    logger.info(f"â± SpeakerID è€—æ™‚ {spk_time:.3f}s")
    logger.info(f"â± ASR è€—æ™‚ {asr_time:.3f}s")

    # èª¿æ•´æ¯å€‹è©çš„æ™‚é–“æˆ³ï¼Œä½¿å…¶å°é½ŠåŸå§‹æ··éŸ³æª”è»¸
    adjusted_words = []
    for w in words:
        w['start'] = w['start'] + t0
        w['end']   = w['end'] + t0
        adjusted_words.append(w)

    total = time.perf_counter() - start
    logger.info(f"â± segment ç¸½è€—æ™‚ {total:.3f}s")

    return {
        "start": round(t0, 2),
        "end": round(t1, 2),
        "end": round(t1, 2),
        "speaker": name,
        "distance": round(float(dist), 3),
        "text": text,
        "confidence": round(conf, 2),
        "words": adjusted_words,
        "spk_time": spk_time,
        "asr_time": asr_time,
    }



def make_pretty(seg: dict) -> dict:
    """Convert a segment dict to human friendly format."""
    """Convert a segment dict to human friendly format."""
    return {
        "time": f"{seg['start']:.2f}s â†’ {seg['end']:.2f}s",
        "speaker": seg["speaker"],
        "similarity": f"{seg['distance']:.3f}",
        "confidence": f"{seg['confidence']*100:.1f}%",
        "text": seg["text"],
        "word_count": len(seg["words"]),
    }


def run_pipeline_file(raw_wav: str, max_workers: int = 3):
    """Run pipeline on an existing wav file."""
    total_start = time.perf_counter()


def run_pipeline_file(raw_wav: str, max_workers: int = 3):
    """Run pipeline on an existing wav file."""
    total_start = time.perf_counter()

    waveform, sr = torchaudio.load(raw_wav)
    # â† æŠŠ waveform å‚³åˆ° separator è¨­å®šçš„è£ç½® (cuda or cpu)
    waveform = waveform.to(sep.device)
    audio_len = waveform.shape[1] / sr
    out_dir = pathlib.Path("work_output") / dt.now().strftime("%Y%m%d_%H%M%S")
    out_dir.mkdir(parents=True, exist_ok=True)

    # 1) åˆ†é›¢
    sep_start = time.perf_counter()
    # 1) åˆ†é›¢
    sep_start = time.perf_counter()
    segments = sep.separate_and_save(waveform, str(out_dir), segment_index=0)
    if not segments:                           # â† æ–°å¢
        logger.error("ğŸš¨ èªè€…åˆ†é›¢å¤±æ•—ï¼šå›å‚³ç©ºå€¼ / None")
        raise RuntimeError("Speaker separation failed â€“ no segments returned")
    sep_end = time.perf_counter()
    logger.info(f"â± åˆ†é›¢è€—æ™‚ {sep_end - sep_start:.3f}s, å…± {len(segments)} æ®µ")

    # 2) å¤šåŸ·è¡Œç·’è™•ç†æ‰€æœ‰æ®µ
    # 2) å¤šåŸ·è¡Œç·’è™•ç†æ‰€æœ‰æ®µ
    logger.info(f"ğŸ”„ è™•ç† {len(segments)} æ®µ... (max_workers={max_workers})")
    with ThreadPoolExecutor(max_workers=max_workers) as ex:
        bundle = [r for r in ex.map(lambda s: process_segment(*s), segments) if r]

    spk_time = max((s.get("spk_time", 0.0) for s in bundle), default=0.0)
    asr_time = max((s.get("asr_time", 0.0) for s in bundle), default=0.0)
    pipeline_total = (sep_end - sep_start) + spk_time + asr_time

    stages_path = pathlib.Path("pipeline_stages.csv")
    if not stages_path.exists():
        stages_path.write_text("sep_max,sid_max,asr_max,pipeline_total\n", encoding="utf-8")
    with stages_path.open("a", encoding="utf-8") as f:
        f.write(f"{sep_end - sep_start:.3f},{spk_time:.3f},{asr_time:.3f},{pipeline_total:.3f}\n")

    # 3) è¼¸å‡ºçµæœ
    # 3) è¼¸å‡ºçµæœ
    bundle.sort(key=lambda x: x["start"])
    pretty_bundle = [make_pretty(s) for s in bundle]

    json_path = out_dir / "output.json"
    with open(json_path, "w", encoding="utf-8") as f:
        json.dump({"segments": bundle}, f, ensure_ascii=False, indent=2)

    total_end = time.perf_counter()
    total_time = total_end - total_start
    logger.info(f"âœ… Pipeline finished â†’ {json_path} (ç¸½è€—æ™‚ {total_time:.3f}s)")

    stats = {
        "length": audio_len,
        "total": total_time,
        "separate": sep_end - sep_start,
        "speaker": spk_time,
        "asr": asr_time,
        "pipeline_total": pipeline_total,
    }

    return bundle, pretty_bundle, stats


def run_pipeline_dir(dir_path: str, max_workers: int = 3) -> str:
    """ä¸€æ¬¡è™•ç†è³‡æ–™å¤¾å…§æ‰€æœ‰éŸ³æª”ï¼Œä¸¦å°‡ã€Œæª”æ¡ˆå±¤ç´šçµ±è¨ˆã€+ã€Œæ®µè½å±¤ç´šè©³æƒ…ã€å¯«å…¥åŒä¸€ä»½ TSVã€‚

    åªå‘¼å« `run_pipeline_file()` ä¸€æ¬¡å°±åŒæ™‚æ‹¿åˆ° `stats` èˆ‡ `segments`ï¼Œé¿å…é‡è¤‡é‹ç®—ã€‚
    å›å‚³å€¼ç‚ºç”Ÿæˆä¹‹ `summary.tsv` çš„è·¯å¾‘ï¼ˆå­—ä¸²ï¼‰ã€‚
    """

    # === æº–å‚™è¼¸å‡ºç›®éŒ„ ===
    timestamp = dt.now().strftime("%Y%m%d_%H%M%S")
    out_dir = pathlib.Path("work_output") / f"batch_{timestamp}"
    out_dir.mkdir(parents=True, exist_ok=True)
    summary_path = out_dir / "summary.tsv"

    # === æ”¶é›†æ‰€æœ‰æ”¯æ´æ ¼å¼ä¹‹éŸ³æª”ï¼ˆå«å­ç›®éŒ„ï¼‰ ===
    audio_files = [f for f in pathlib.Path(dir_path).rglob("*") if f.suffix.lower() in {".wav", ".mp3", ".flac", ".ogg"}]
    if not audio_files:
        logger.warning("âš ï¸  ç›®éŒ„å…§æœªæ‰¾åˆ°æ”¯æ´æ ¼å¼éŸ³æª”")
        return str(summary_path)

    # === ä¸€æ¬¡è™•ç†ä¸¦æš«å­˜çµæœ ===
    file_results = []  # [(æª”æ¡ˆç´¢å¼•, Path, stats, segments)]
    for idx, audio in enumerate(sorted(audio_files), start=1):
        logger.info(f"===== è™•ç†æª”æ¡ˆ {audio.name} ({idx}/{len(audio_files)}) =====")
        segments, pretty, stats = run_pipeline_file(str(audio), max_workers)
        file_results.append((idx, audio, stats, segments))

    # === å¯«å…¥ TSV ===
    with open(summary_path, "w", encoding="utf-8") as f:
        # -- æª”æ¡ˆå±¤ç´šçµ±è¨ˆ --
        f.write("ç·¨è™Ÿ\tæª”å\téŸ³æª”é•·åº¦(s)\tç¸½è€—æ™‚(s)\tåˆ†é›¢è€—æ™‚(s)\tSpeakerIDè€—æ™‚(s)\tASRè€—æ™‚(s)\n")
        for idx, audio, stats, _ in file_results:
            f.write(
                f"{idx}\t{audio.name}\t{stats['length']:.2f}\t{stats['total']:.2f}\t"
                f"{stats['separate']:.2f}\t{stats['speaker']:.2f}\t{stats['asr']:.2f}\n"
            )
        # ç©ºè¡Œåˆ†æ®µ
        f.write("\næª”æ¡ˆ\té–‹å§‹(s)\tçµæŸ(s)\tèªªè©±è€…\tdistance\tconfidence\tæ–‡å­—\n")
        # -- æ®µè½å±¤ç´šè©³æƒ… --
        for _, audio, _, segments in file_results:
            for seg in segments:
                text = str(seg.get("text", "")).replace("\t", " ")
                f.write(
                    f"{audio.name}\t{seg['start']:.3f}\t{seg['end']:.3f}\t"
                    f"{seg['speaker']}\t{seg['distance']:.4f}\t{seg['confidence']:.4f}\t{text}\n"
                )

    logger.info(f"âœ… Directory pipeline å®Œæˆ â†’ {summary_path}")
    return str(summary_path)


def run_pipeline_stream(
    chunk_secs: float = 6.0,
    rate: int = 16000,
    channels: int = 1,
    frames_per_buffer: int = 1024,
    max_workers: int = 2,
    record_secs: float | None = None,
    queue_out: "queue.Queue[dict] | None" = None,
    stop_event: threading.Event | None = None,
    in_bytes_queue: "queue.Queue[bytes] | None" = None,  # æ–°å¢ï¼šå‰ç«¯å‚³å…¥çš„åŸå§‹ bytes ä½‡åˆ—
):
    """é€£çºŒéŒ„éŸ³æˆ–æ¥æ”¶å‰ç«¯ bytesï¼Œæ¯ `chunk_secs` ç§’åˆ‡ç‰‡ï¼Œ
    å°ä¸‰ä½èªªè©±äººåš SpeakerID + ASRã€‚

    * è‹¥ ``in_bytes_queue`` ç‚º ``None`` â†’ ä½¿ç”¨æœ¬æ©Ÿéº¥å…‹é¢¨ã€‚
    * è‹¥æä¾› ``in_bytes_queue`` â†’ å¾è©²ä½‡åˆ—è®€å– 16â€‘bit PCM bytesï¼Œ
      é©åˆ WebSocket/APP ä¸Šå‚³ä¸²æµéŸ³è¨Šçš„æƒ…å¢ƒã€‚
    """

    total_start = time.perf_counter()
    out_root = Path("stream_output") / dt.now().strftime("%Y%m%d_%H%M%S")
    out_root.mkdir(parents=True, exist_ok=True)

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ åŸ·è¡Œç·’æ±  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    executor: ThreadPoolExecutor = ThreadPoolExecutor(max_workers=max_workers)
    futures: list[Future] = []

    # -------------------------------------------------
    # 1. å°å–®ä¸€ chunk é€²è¡Œåˆ†é›¢ã€SpeakerIDã€ASR
    # -------------------------------------------------
    def process_chunk(raw_bytes: bytes, idx: int):
        t0 = idx * chunk_secs
        t1 = t0 + chunk_secs

        # 1) è½‰ tensor [-1, 1]
        waveform = torch.frombuffer(raw_bytes, dtype=torch.int16).float() / 32768.0
        waveform = waveform.view(1, -1)

        # 2) å„²å­˜æ··åˆéŸ³æª”
        seg_dir = out_root / f"segment_{idx:03d}"
        seg_dir.mkdir(parents=True, exist_ok=True)
        mix_path = seg_dir / "mix.wav"
        torchaudio.save(mix_path.as_posix(), waveform, rate)

        # 3) èªè€…åˆ†é›¢
        sep.separate_and_save(waveform, seg_dir.as_posix(), segment_index=idx)
        speaker_paths = sorted(seg_dir.glob("speaker*.wav"))
        if not speaker_paths:
            logger.warning("segment %d ç„¡ speaker wav", idx)
            return None

        # 4) SpeakerID + ASR
        speaker_results: list[dict] = []
        for sp_idx, wav_path in enumerate(speaker_paths, 1):
            res = process_segment(str(wav_path), t0, t1)
            if not res["text"].strip() or res["confidence"] < 0.1:
                continue  # è·³éä½ä¿¡å¿ƒæˆ–ç©ºç™½
            res["speaker_index"] = sp_idx
            speaker_results.append(res)

        # å»é‡è¤‡ï¼šåŒ speaker ååªç•™ä¿¡å¿ƒæœ€é«˜
        unique: dict[str, dict] = {}
        for item in speaker_results:
            n = item["speaker"]
            if n not in unique or item["confidence"] > unique[n]["confidence"]:
                unique[n] = item
        speaker_results = list(unique.values())

        seg_dict = {
            "segment": idx,
            "start": round(t0, 2),
            "end": round(t1, 2),
            "mix": mix_path.as_posix(),
            "sources": [str(p) for p in speaker_paths],
            "speakers": speaker_results,
        }

        # æ¯æ®µå¯«ä¸€ä»½ JSON
        with open(seg_dir / "output.json", "w", encoding="utf-8") as f:
            json.dump(seg_dict, f, ensure_ascii=False, indent=2)

        # å³æ™‚æ¨é€çµ¦å¤–éƒ¨ä½‡åˆ—ï¼ˆWS/SSEï¼‰
        if queue_out is not None:
            queue_out.put(seg_dict)
        return seg_dict

    # -------------------------------------------------
    # 2. éŒ„éŸ³ / è®€å– bytes çš„åŸ·è¡Œç·’
    # -------------------------------------------------
    q: queue.Queue[tuple[bytes, int]] = queue.Queue(maxsize=max_workers * 2)
    stop_flag = threading.Event()

    def recorder_from_queue():
        """å¤–éƒ¨ bytes ä¾†æºï¼šå¾ in_bytes_queue è®€è³‡æ–™"""
        frames_needed = int(rate * chunk_secs) * 2  # bytes
        buf = bytearray()
        idx = 0
        start_time = time.time()
        while not stop_flag.is_set():
            if stop_event and stop_event.is_set():
                break
            try:
                pkt = in_bytes_queue.get(timeout=0.1)
            except queue.Empty:
                # æª¢æŸ¥æ˜¯å¦æ™‚é–“åˆ°
                if record_secs is not None and time.time() - start_time >= record_secs:
                    stop_flag.set(); break
                continue
            buf.extend(pkt)
            while len(buf) >= frames_needed:
                raw = bytes(buf[:frames_needed]); buf = buf[frames_needed:]
                q.put((raw, idx)); idx += 1

    def recorder_from_mic():
        pa = pyaudio.PyAudio()
        stream = pa.open(
            format=pyaudio.paInt16,
            channels=channels,
            rate=rate,
            input=True,
            frames_per_buffer=frames_per_buffer,
        )
        frames_needed = int(rate * chunk_secs)
        buf = bytearray(); idx = 0; start_time = time.time()
        try:
            while not stop_flag.is_set():
                if stop_event and stop_event.is_set():
                    break
                if record_secs is not None and time.time() - start_time >= record_secs:
                    stop_flag.set(); break
                buf.extend(stream.read(frames_per_buffer, exception_on_overflow=False))
                if len(buf) // 2 >= frames_needed:
                    raw = bytes(buf[: frames_needed * 2]); buf = buf[frames_needed * 2 :]
                    q.put((raw, idx)); idx += 1
        finally:
            stream.stop_stream(); stream.close(); pa.terminate()

    # å•Ÿå‹•æ­£ç¢ºçš„ recorder
    rec_thread = threading.Thread(
        target=recorder_from_queue if in_bytes_queue else recorder_from_mic,
        daemon=True,
    )
    rec_thread.start()

    logger.info("ğŸ™ é–‹å§‹éŒ„éŸ³/æ¥æ”¶ (%s) ...", "å¤–éƒ¨ bytes" if in_bytes_queue else ("Ctrlâ€‘C" if record_secs is None else f"{record_secs}s"))

    # -------------------------------------------------
    # 3. ä¸»è¿´åœˆï¼šæŠŠ chunk äº¤çµ¦åŸ·è¡Œç·’æ± 
    # -------------------------------------------------
    try:
        while True:
            if stop_event and stop_event.is_set():
                break
            try:
                raw, idx = q.get(timeout=0.1)
            except queue.Empty:
                if stop_flag.is_set():
                    break
                continue
            futures.append(executor.submit(process_chunk, raw, idx))
    except KeyboardInterrupt:
        logger.info("ğŸ›‘ Ctrlâ€‘C åµæ¸¬åˆ°ä½¿ç”¨è€…æ‰‹å‹•åœæ­¢")
        if stop_event:
            stop_event.set()   # â† è®“å¤–éƒ¨ WebSocket loop ä¹Ÿèƒ½æ”¶å°¾
        stop_flag.set()        # â† é€šçŸ¥ recorder thread çµæŸ
    finally:
        stop_flag.set(); rec_thread.join(timeout=1)
        executor.shutdown(wait=True)

    # -------------------------------------------------
    # 4. èšåˆçµæœï¼ˆå¯é¸ï¼‰
    # -------------------------------------------------
    bundle = [f.result() for f in futures if f.done() and f.result()]
    bundle.sort(key=lambda x: x["start"])

    pretty_bundle: list[dict] = []
    for seg in bundle:
        for sp in seg["speakers"]:
            pretty_bundle.append(make_pretty(sp))

    logger.info("ğŸš© stream çµæŸï¼Œå…± %d æ®µï¼Œè€—æ™‚ %.3fs â†’ %s", len(bundle), time.perf_counter() - total_start, out_root)
    return bundle, pretty_bundle



# Backwards compatible name
run_pipeline_FILE = run_pipeline_file
run_pipeline_STREAM = run_pipeline_stream


def main():
    parser = argparse.ArgumentParser(description="Speech pipeline")
    sub = parser.add_subparsers(dest="mode", required=True)

    p_file = sub.add_parser("file", help="process existing wav file")
    p_file.add_argument("path")
    p_file.add_argument("--workers", type=int, default=4)

    p_stream = sub.add_parser("stream", help="live stream from microphone")
    p_stream.add_argument("--chunk", type=float, default=6.0, help="seconds per chunk")
    p_stream.add_argument("--workers", type=int, default=2)
    p_stream.add_argument("--record_secs", type=float, default=18.0, help="total recording time in seconds (None for infinite)")

    args = parser.parse_args()

    if args.mode == "file":
        run_pipeline_file(args.path, args.workers)
    elif args.mode == "stream":
        run_pipeline_stream(chunk_secs=args.chunk, max_workers=args.workers)

if __name__ == "__main__":
    main()